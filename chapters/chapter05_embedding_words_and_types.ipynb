{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf324d1",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Embedding Words and Types<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Why-Learn-Embeddings?\" data-toc-modified-id=\"Why-Learn-Embeddings?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Why Learn Embeddings?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Efficiency-of-Embeddings\" data-toc-modified-id=\"Efficiency-of-Embeddings-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Efficiency of Embeddings</a></span></li><li><span><a href=\"#Approaches-to-Learning-Word-Embeddings\" data-toc-modified-id=\"Approaches-to-Learning-Word-Embeddings-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Approaches to Learning Word Embeddings</a></span></li><li><span><a href=\"#The-Practical-Use-of-Pretrained-Word-Embeddings\" data-toc-modified-id=\"The-Practical-Use-of-Pretrained-Word-Embeddings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>The Practical Use of Pretrained Word Embeddings</a></span></li></ul></li><li><span><a href=\"#Example:-Learning-the-Continous-Bag-of-Words-Embeddings\" data-toc-modified-id=\"Example:-Learning-the-Continous-Bag-of-Words-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example: Learning the Continous Bag of Words Embeddings</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965775e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b869fb5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Representataion Learning or Embedding* refer to learning the mapping from one discrete type to a point in the vector space. When the discrete types are words, the dense vector representation is called a _word embedding_. TF-IDF(Term Frequency-Inverse Document Frequency) is an example of _count based embedding_ method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c4615",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Why Learn Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403c9c3f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The count-based representations are also called _distributional representations_ because their significant content or meaning is represented by multiple dimensions in the vector. These representations are not learned from the data but heuristically constructed.\n",
    "\n",
    "**Benefits of Low Dimensional Learned Representations:**\n",
    "- Reducing the dimensionality is computationally efficient.\n",
    "- The count based representations result in high dimensional vectors that encode similar information along many dimensions and do not share statistical strength.\n",
    "- Very high dimensions in the input can result in real problems in machine learning and optimisation which is often called _curse of dimensionality_.\n",
    "- Representations learned from task specific data are optimal for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b026c81e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Efficiency of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f739e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we perform the matrix multiplication of one hot vector with weight matrix, the resulting vector is just selecting the row indicated by the non zero entry.\n",
    "\n",
    "![Figure 5.1](../images/figure_5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd12a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Approaches to Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebf4d6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Auxiliary Tasks used to train Word Embeddings:\n",
    "- Given a sequence of words, predict the next word. This is also called the _language modeling task_.\n",
    "- Given a sequence of words before and after, predict the missing word.\n",
    "- Given a word, predict words that occur within a window, independent of the position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daee52d5",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Practical Use of Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829d07f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Loading Embeddings\\n# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\\n%load_ext nb_black\\n\\nimport numpy as np\\nfrom annoy import AnnoyIndex\\n\\n\\nclass PreTrainedEmbeddings(object):\\n    def __init__(self, word_to_index, word_vectors):\\n        \\\"\\\"\\\"\\n        Args:\\n            word_to_index: mapping from word to integers.\\n            word_vectors: list of numpy array.\\n        \\\"\\\"\\\"\\n        self.word_to_index = word_to_index\\n        self.word_vectors = word_vectors\\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\\n        self.index = AnnoyIndex(len(word_vectors[0]), metric=\\\"euclidean\\\")\\n        for _, i in self.word_to_index.items():\\n            self.index.add_item(i, self.word_vectors[i])\\n        self.index.build(50)\\n\\n    @classmethod\\n    def from_embeddings_file(cls, embedding_file):\\n        \\\"\\\"\\\"\\n        Init from pretrained vector file.\\n\\n        Vector filw should be of the format:\\n            word0 x0_0 x0_1, x0_2 ... x0_N\\n            word1 x1_0 x1_1, x1_2 ... x1_N\\n\\n        Args:\\n            embedding_file: location of the file\\n        Returns:\\n            instance of PretrainedEmbeddings\\n        \\\"\\\"\\\"\\n        word_to_index, word_vectors = {}, []\\n        with open(embedding_file) as fp:\\n            for line in fp.readlines():\\n                line = line.split(\\\" \\\")\\n                word = line[0]\\n                vec = np.array([float(x) for x in line[1:]])\\n\\n                word_to_index[word] = len(word_to_index)\\n                word_vectors.append(vec)\\n        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\\n    \\n    def get_embedding(self, word):\\n        \\\"\\\"\\\"\\n        Args:\\n            word: Input word to get embedding for.\\n        Returns:\\n            an embedding for given word\\n        \\\"\\\"\\\"\\n        return self.word_vectors[self.word_to_index[word]]\\n    \\n    def get_closed_to_vector(self, vector, n=1):\\n        \\\"\\\"\\\"\\n        Given a vector, return its n nearest neighbors.\\n        \\n        Args:\\n            vector: should match the size of the vectors in the Annoy Index.\\n            n: the number of neighbors to return\\n        Returns:\\n            Unsorted list of words nearest to the given vector.\\n        \\\"\\\"\\\"\\n        nn_indices = self.index.get_nns_by_vector(vector, n)\\n        return [\\n            self.index_to_word[neighbor] for neighbor in nn_indices\\n        ]\\n    \\n    def compute_and_print_analogy(self, word1, word2, word3):\\n        \\\"\\\"\\\"\\n        Prints the solutions to analogies using word embeddings.\\n        \\n        Analogies are word1 to word2 as word3 is to __\\n        This methid will print: word1 : word2 :: word3 : word4\\n        \\n        Args:\\n            word1, word2, word3\\n        \\\"\\\"\\\"\\n        vec1 = self.get_embedding(word1)\\n        vec2 = self.get_embedding(word2)\\n        vec3 = self.get_embedding(word3)\\n        \\n        spatial_relationship = vec2 - vec1\\n        vec4 = vec3 + spatial_relationship\\n        \\n        closed_words = self.get_closed_to_vector(vec4, n=4)\\n        existing_words = set([word1, word2, word3])\\n        closed_words = [\\n            word for word in closed_words\\n            if word not in existing_words\\n        ]\\n        if len(closed_words) == 0:\\n            print(\\\"Could not find nearest neighbors for the vector!\\\")\\n            return\\n        for word4 in closed_words:\\n            print(\\n                f\\\"{word1}:{word2} :: {word3}:{word4}\\\"\\n            )\\n\\nembeddings = PreTrainedEmbeddings.from_embeddings_file(\\\"../data/glove.6B.100d.txt\\\")\";\n",
       "                var nbb_formatted_code = \"# Loading Embeddings\\n# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\\n%load_ext nb_black\\n\\nimport numpy as np\\nfrom annoy import AnnoyIndex\\n\\n\\nclass PreTrainedEmbeddings(object):\\n    def __init__(self, word_to_index, word_vectors):\\n        \\\"\\\"\\\"\\n        Args:\\n            word_to_index: mapping from word to integers.\\n            word_vectors: list of numpy array.\\n        \\\"\\\"\\\"\\n        self.word_to_index = word_to_index\\n        self.word_vectors = word_vectors\\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\\n        self.index = AnnoyIndex(len(word_vectors[0]), metric=\\\"euclidean\\\")\\n        for _, i in self.word_to_index.items():\\n            self.index.add_item(i, self.word_vectors[i])\\n        self.index.build(50)\\n\\n    @classmethod\\n    def from_embeddings_file(cls, embedding_file):\\n        \\\"\\\"\\\"\\n        Init from pretrained vector file.\\n\\n        Vector filw should be of the format:\\n            word0 x0_0 x0_1, x0_2 ... x0_N\\n            word1 x1_0 x1_1, x1_2 ... x1_N\\n\\n        Args:\\n            embedding_file: location of the file\\n        Returns:\\n            instance of PretrainedEmbeddings\\n        \\\"\\\"\\\"\\n        word_to_index, word_vectors = {}, []\\n        with open(embedding_file) as fp:\\n            for line in fp.readlines():\\n                line = line.split(\\\" \\\")\\n                word = line[0]\\n                vec = np.array([float(x) for x in line[1:]])\\n\\n                word_to_index[word] = len(word_to_index)\\n                word_vectors.append(vec)\\n        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\\n\\n    def get_embedding(self, word):\\n        \\\"\\\"\\\"\\n        Args:\\n            word: Input word to get embedding for.\\n        Returns:\\n            an embedding for given word\\n        \\\"\\\"\\\"\\n        return self.word_vectors[self.word_to_index[word]]\\n\\n    def get_closed_to_vector(self, vector, n=1):\\n        \\\"\\\"\\\"\\n        Given a vector, return its n nearest neighbors.\\n\\n        Args:\\n            vector: should match the size of the vectors in the Annoy Index.\\n            n: the number of neighbors to return\\n        Returns:\\n            Unsorted list of words nearest to the given vector.\\n        \\\"\\\"\\\"\\n        nn_indices = self.index.get_nns_by_vector(vector, n)\\n        return [self.index_to_word[neighbor] for neighbor in nn_indices]\\n\\n    def compute_and_print_analogy(self, word1, word2, word3):\\n        \\\"\\\"\\\"\\n        Prints the solutions to analogies using word embeddings.\\n\\n        Analogies are word1 to word2 as word3 is to __\\n        This methid will print: word1 : word2 :: word3 : word4\\n\\n        Args:\\n            word1, word2, word3\\n        \\\"\\\"\\\"\\n        vec1 = self.get_embedding(word1)\\n        vec2 = self.get_embedding(word2)\\n        vec3 = self.get_embedding(word3)\\n\\n        spatial_relationship = vec2 - vec1\\n        vec4 = vec3 + spatial_relationship\\n\\n        closed_words = self.get_closed_to_vector(vec4, n=4)\\n        existing_words = set([word1, word2, word3])\\n        closed_words = [word for word in closed_words if word not in existing_words]\\n        if len(closed_words) == 0:\\n            print(\\\"Could not find nearest neighbors for the vector!\\\")\\n            return\\n        for word4 in closed_words:\\n            print(f\\\"{word1}:{word2} :: {word3}:{word4}\\\")\\n\\n\\nembeddings = PreTrainedEmbeddings.from_embeddings_file(\\\"../data/glove.6B.100d.txt\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Embeddings\n",
    "# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\n",
    "%load_ext nb_black\n",
    "\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "class PreTrainedEmbeddings(object):\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index: mapping from word to integers.\n",
    "            word_vectors: list of numpy array.\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric=\"euclidean\")\n",
    "        for _, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "\n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"\n",
    "        Init from pretrained vector file.\n",
    "\n",
    "        Vector filw should be of the format:\n",
    "            word0 x0_0 x0_1, x0_2 ... x0_N\n",
    "            word1 x1_0 x1_1, x1_2 ... x1_N\n",
    "\n",
    "        Args:\n",
    "            embedding_file: location of the file\n",
    "        Returns:\n",
    "            instance of PretrainedEmbeddings\n",
    "        \"\"\"\n",
    "        word_to_index, word_vectors = {}, []\n",
    "        with open(embedding_file) as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: Input word to get embedding for.\n",
    "        Returns:\n",
    "            an embedding for given word\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "\n",
    "    def get_closed_to_vector(self, vector, n=1):\n",
    "        \"\"\"\n",
    "        Given a vector, return its n nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            vector: should match the size of the vectors in the Annoy Index.\n",
    "            n: the number of neighbors to return\n",
    "        Returns:\n",
    "            Unsorted list of words nearest to the given vector.\n",
    "        \"\"\"\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "\n",
    "    def compute_and_print_analogy(self, word1, word2, word3):\n",
    "        \"\"\"\n",
    "        Prints the solutions to analogies using word embeddings.\n",
    "\n",
    "        Analogies are word1 to word2 as word3 is to __\n",
    "        This methid will print: word1 : word2 :: word3 : word4\n",
    "\n",
    "        Args:\n",
    "            word1, word2, word3\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "\n",
    "        spatial_relationship = vec2 - vec1\n",
    "        vec4 = vec3 + spatial_relationship\n",
    "\n",
    "        closed_words = self.get_closed_to_vector(vec4, n=4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closed_words = [word for word in closed_words if word not in existing_words]\n",
    "        if len(closed_words) == 0:\n",
    "            print(\"Could not find nearest neighbors for the vector!\")\n",
    "            return\n",
    "        for word4 in closed_words:\n",
    "            print(f\"{word1}:{word2} :: {word3}:{word4}\")\n",
    "\n",
    "\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file(\"../data/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2064aff8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relationship between gendered nouns and pronouns\n",
      "man:he :: woman:she\n",
      "man:he :: woman:never\n",
      "\n",
      "Verb-noun relationships\n",
      "fly:plane :: sail:ship\n",
      "fly:plane :: sail:vessel\n",
      "\n",
      "Noun-noun relationships\n",
      "cat:kitten :: dog:puppy\n",
      "cat:kitten :: dog:puppies\n",
      "cat:kitten :: dog:toddler\n",
      "\n",
      "Hypernymy (broader category)\n",
      "blue:color :: dog:behavior\n",
      "blue:color :: dog:touch\n",
      "blue:color :: dog:viewer\n",
      "\n",
      "Meronymy (part-to-whole)\n",
      "toe:foot :: finger:ground\n",
      "toe:foot :: finger:pointing\n",
      "\n",
      "Troponymy (difference in manner)\n",
      "talk:communicate :: read:interpret\n",
      "talk:communicate :: read:typed\n",
      "talk:communicate :: read:correctly\n",
      "talk:communicate :: read:instructions\n",
      "\n",
      "Metonymy (convention / figures of speech)\n",
      "blue:democrat :: red:republican\n",
      "blue:democrat :: red:congressman\n",
      "blue:democrat :: red:senator\n",
      "\n",
      "Adjectival scales\n",
      "fast:fastest :: young:female\n",
      "fast:fastest :: young:fellow\n",
      "fast:fastest :: young:younger\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"# Relationships between word embeddings\\n\\n# Relationship 1: the relationship between gendered nouns and pronouns\\nprint(\\\"the relationship between gendered nouns and pronouns\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"he\\\", \\\"woman\\\")\\nprint()\\n\\n# Relationship 2: Verb-noun relationships\\nprint(\\\"Verb-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"fly\\\", \\\"plane\\\", \\\"sail\\\")\\nprint()\\n\\n#  Relationship 3: Noun-noun relationships\\nprint(\\\"Noun-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"cat\\\", \\\"kitten\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 4: Hypernymy (broader category)\\nprint(\\\"Hypernymy (broader category)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"color\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 5: Meronymy (part-to-whole)\\nprint(\\\"Meronymy (part-to-whole)\\\")\\nembeddings.compute_and_print_analogy(\\\"toe\\\", \\\"foot\\\", \\\"finger\\\")\\nprint()\\n\\n# Relationship 6: Troponymy (difference in manner)\\nprint(\\\"Troponymy (difference in manner)\\\")\\nembeddings.compute_and_print_analogy(\\\"talk\\\", \\\"communicate\\\", \\\"read\\\")\\nprint()\\n\\n# Relationship 7: Metonymy (convention / figures of speech)\\nprint(\\\"Metonymy (convention / figures of speech)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"democrat\\\", \\\"red\\\")\\nprint()\\n\\n# Relationship 8: Adjectival scales\\nprint(\\\"Adjectival scales\\\")\\nembeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"young\\\")\\nprint()\";\n",
       "                var nbb_formatted_code = \"# Relationships between word embeddings\\n\\n# Relationship 1: the relationship between gendered nouns and pronouns\\nprint(\\\"the relationship between gendered nouns and pronouns\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"he\\\", \\\"woman\\\")\\nprint()\\n\\n# Relationship 2: Verb-noun relationships\\nprint(\\\"Verb-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"fly\\\", \\\"plane\\\", \\\"sail\\\")\\nprint()\\n\\n#  Relationship 3: Noun-noun relationships\\nprint(\\\"Noun-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"cat\\\", \\\"kitten\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 4: Hypernymy (broader category)\\nprint(\\\"Hypernymy (broader category)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"color\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 5: Meronymy (part-to-whole)\\nprint(\\\"Meronymy (part-to-whole)\\\")\\nembeddings.compute_and_print_analogy(\\\"toe\\\", \\\"foot\\\", \\\"finger\\\")\\nprint()\\n\\n# Relationship 6: Troponymy (difference in manner)\\nprint(\\\"Troponymy (difference in manner)\\\")\\nembeddings.compute_and_print_analogy(\\\"talk\\\", \\\"communicate\\\", \\\"read\\\")\\nprint()\\n\\n# Relationship 7: Metonymy (convention / figures of speech)\\nprint(\\\"Metonymy (convention / figures of speech)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"democrat\\\", \\\"red\\\")\\nprint()\\n\\n# Relationship 8: Adjectival scales\\nprint(\\\"Adjectival scales\\\")\\nembeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"young\\\")\\nprint()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relationships between word embeddings\n",
    "\n",
    "# Relationship 1: the relationship between gendered nouns and pronouns\n",
    "print(\"the relationship between gendered nouns and pronouns\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"he\", \"woman\")\n",
    "print()\n",
    "\n",
    "# Relationship 2: Verb-noun relationships\n",
    "print(\"Verb-noun relationships\")\n",
    "embeddings.compute_and_print_analogy(\"fly\", \"plane\", \"sail\")\n",
    "print()\n",
    "\n",
    "#  Relationship 3: Noun-noun relationships\n",
    "print(\"Noun-noun relationships\")\n",
    "embeddings.compute_and_print_analogy(\"cat\", \"kitten\", \"dog\")\n",
    "print()\n",
    "\n",
    "# Relationship 4: Hypernymy (broader category)\n",
    "print(\"Hypernymy (broader category)\")\n",
    "embeddings.compute_and_print_analogy(\"blue\", \"color\", \"dog\")\n",
    "print()\n",
    "\n",
    "# Relationship 5: Meronymy (part-to-whole)\n",
    "print(\"Meronymy (part-to-whole)\")\n",
    "embeddings.compute_and_print_analogy(\"toe\", \"foot\", \"finger\")\n",
    "print()\n",
    "\n",
    "# Relationship 6: Troponymy (difference in manner)\n",
    "print(\"Troponymy (difference in manner)\")\n",
    "embeddings.compute_and_print_analogy(\"talk\", \"communicate\", \"read\")\n",
    "print()\n",
    "\n",
    "# Relationship 7: Metonymy (convention / figures of speech)\n",
    "print(\"Metonymy (convention / figures of speech)\")\n",
    "embeddings.compute_and_print_analogy(\"blue\", \"democrat\", \"red\")\n",
    "print()\n",
    "\n",
    "# Relationship 8: Adjectival scales\n",
    "print(\"Adjectival scales\")\n",
    "embeddings.compute_and_print_analogy(\"fast\", \"fastest\", \"young\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b7edcf7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast:fastest :: small:smallest\n",
      "fast:fastest :: small:large\n",
      "man:king :: woman:queen\n",
      "man:king :: woman:monarch\n",
      "man:king :: woman:throne\n",
      "man:doctor :: woman:nurse\n",
      "man:doctor :: woman:physician\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 17;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"small\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"king\\\", \\\"woman\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"doctor\\\", \\\"woman\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"small\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"king\\\", \\\"woman\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"doctor\\\", \\\"woman\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fast\", \"fastest\", \"small\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"king\", \"woman\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49701700",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sachin:cricket :: messi:rugby\n",
      "sachin:cricket :: messi:soccer\n",
      "sachin:cricket :: messi:football\n",
      "sachin:cricket :: messi:club\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"sachin\\\", \\\"cricket\\\", \\\"messi\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"sachin\\\", \\\"cricket\\\", \\\"messi\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"sachin\", \"cricket\", \"messi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "672aee04",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifty:sensex :: nasdaq:index\n",
      "nifty:sensex :: nasdaq:composite\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 29;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"nifty\\\", \\\"sensex\\\", \\\"nasdaq\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"nifty\\\", \\\"sensex\\\", \\\"nasdaq\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"nifty\", \"sensex\", \"nasdaq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4957e43f",
   "metadata": {},
   "source": [
    "## Example: Learning the Continous Bag of Words Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b83b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Embedding Words and Types",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
