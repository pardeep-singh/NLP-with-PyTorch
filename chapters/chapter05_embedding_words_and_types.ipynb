{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eafc47",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Embedding Words and Types<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Why-Learn-Embeddings?\" data-toc-modified-id=\"Why-Learn-Embeddings?-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Why Learn Embeddings?</a></span><ul class=\"toc-item\"><li><span><a href=\"#Efficiency-of-Embeddings\" data-toc-modified-id=\"Efficiency-of-Embeddings-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Efficiency of Embeddings</a></span></li><li><span><a href=\"#Approaches-to-Learning-Word-Embeddings\" data-toc-modified-id=\"Approaches-to-Learning-Word-Embeddings-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Approaches to Learning Word Embeddings</a></span></li><li><span><a href=\"#The-Practical-Use-of-Pretrained-Word-Embeddings\" data-toc-modified-id=\"The-Practical-Use-of-Pretrained-Word-Embeddings-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>The Practical Use of Pretrained Word Embeddings</a></span></li></ul></li><li><span><a href=\"#Example:-Learning-the-Continous-Bag-of-Words-Embeddings\" data-toc-modified-id=\"Example:-Learning-the-Continous-Bag-of-Words-Embeddings-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example: Learning the Continous Bag of Words Embeddings</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Vectorization-Classes\" data-toc-modified-id=\"Data-Vectorization-Classes-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Data Vectorization Classes</a></span></li><li><span><a href=\"#The-CBOW-Model\" data-toc-modified-id=\"The-CBOW-Model-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>The CBOW Model</a></span></li><li><span><a href=\"#Model-Training-&amp;-Evaluation\" data-toc-modified-id=\"Model-Training-&amp;-Evaluation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Model Training &amp; Evaluation</a></span></li><li><span><a href=\"#Trained-Embeddings\" data-toc-modified-id=\"Trained-Embeddings-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Trained Embeddings</a></span></li></ul></li><li><span><a href=\"#Example:-Transfer-Learning-Using-Pretrained-Embeddings-for-Document-Classification\" data-toc-modified-id=\"Example:-Transfer-Learning-Using-Pretrained-Embeddings-for-Document-Classification-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Example: Transfer Learning Using Pretrained Embeddings for Document Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Vectorization-classes\" data-toc-modified-id=\"Data-Vectorization-classes-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Data Vectorization classes</a></span></li><li><span><a href=\"#The-NewsClassifier\" data-toc-modified-id=\"The-NewsClassifier-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>The NewsClassifier</a></span></li><li><span><a href=\"#Utils\" data-toc-modified-id=\"Utils-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Utils</a></span></li><li><span><a href=\"#Model-Training-&amp;-Evaluation\" data-toc-modified-id=\"Model-Training-&amp;-Evaluation-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Model Training &amp; Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-4.4.1\"><span class=\"toc-item-num\">4.4.1&nbsp;&nbsp;</span>Inference</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383fc6af",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd200b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "*Representataion Learning or Embedding* refer to learning the mapping from one discrete type to a point in the vector space. When the discrete types are words, the dense vector representation is called a _word embedding_. TF-IDF(Term Frequency-Inverse Document Frequency) is an example of _count based embedding_ method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8510cdf",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Why Learn Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c182ccb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The count-based representations are also called _distributional representations_ because their significant content or meaning is represented by multiple dimensions in the vector. These representations are not learned from the data but heuristically constructed.\n",
    "\n",
    "**Benefits of Low Dimensional Learned Representations:**\n",
    "- Reducing the dimensionality is computationally efficient.\n",
    "- The count based representations result in high dimensional vectors that encode similar information along many dimensions and do not share statistical strength.\n",
    "- Very high dimensions in the input can result in real problems in machine learning and optimisation which is often called _curse of dimensionality_.\n",
    "- Representations learned from task specific data are optimal for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e3b7f7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Efficiency of Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c921dd65",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we perform the matrix multiplication of one hot vector with weight matrix, the resulting vector is just selecting the row indicated by the non zero entry.\n",
    "\n",
    "![Figure 5.1](../images/figure_5_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159aab9e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Approaches to Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f42aa1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Auxiliary Tasks used to train Word Embeddings:\n",
    "- Given a sequence of words, predict the next word. This is also called the _language modeling task_.\n",
    "- Given a sequence of words before and after, predict the missing word.\n",
    "- Given a word, predict words that occur within a window, independent of the position."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23978f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Practical Use of Pretrained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d07441",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"# Loading Embeddings\\n# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\\n%load_ext nb_black\\n\\nimport numpy as np\\nfrom annoy import AnnoyIndex\\n\\n\\nclass PreTrainedEmbeddings(object):\\n    def __init__(self, word_to_index, word_vectors):\\n        \\\"\\\"\\\"\\n        Args:\\n            word_to_index: mapping from word to integers.\\n            word_vectors: list of numpy array.\\n        \\\"\\\"\\\"\\n        self.word_to_index = word_to_index\\n        self.word_vectors = word_vectors\\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\\n        self.index = AnnoyIndex(len(word_vectors[0]), metric=\\\"euclidean\\\")\\n        for _, i in self.word_to_index.items():\\n            self.index.add_item(i, self.word_vectors[i])\\n        self.index.build(50)\\n\\n    @classmethod\\n    def from_embeddings_file(cls, embedding_file):\\n        \\\"\\\"\\\"\\n        Init from pretrained vector file.\\n\\n        Vector filw should be of the format:\\n            word0 x0_0 x0_1, x0_2 ... x0_N\\n            word1 x1_0 x1_1, x1_2 ... x1_N\\n\\n        Args:\\n            embedding_file: location of the file\\n        Returns:\\n            instance of PretrainedEmbeddings\\n        \\\"\\\"\\\"\\n        word_to_index, word_vectors = {}, []\\n        with open(embedding_file) as fp:\\n            for line in fp.readlines():\\n                line = line.split(\\\" \\\")\\n                word = line[0]\\n                vec = np.array([float(x) for x in line[1:]])\\n\\n                word_to_index[word] = len(word_to_index)\\n                word_vectors.append(vec)\\n        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\\n\\n    def get_embedding(self, word):\\n        \\\"\\\"\\\"\\n        Args:\\n            word: Input word to get embedding for.\\n        Returns:\\n            an embedding for given word\\n        \\\"\\\"\\\"\\n        return self.word_vectors[self.word_to_index[word]]\\n\\n    def get_closed_to_vector(self, vector, n=1):\\n        \\\"\\\"\\\"\\n        Given a vector, return its n nearest neighbors.\\n\\n        Args:\\n            vector: should match the size of the vectors in the Annoy Index.\\n            n: the number of neighbors to return\\n        Returns:\\n            Unsorted list of words nearest to the given vector.\\n        \\\"\\\"\\\"\\n        nn_indices = self.index.get_nns_by_vector(vector, n)\\n        return [self.index_to_word[neighbor] for neighbor in nn_indices]\\n\\n    def compute_and_print_analogy(self, word1, word2, word3):\\n        \\\"\\\"\\\"\\n        Prints the solutions to analogies using word embeddings.\\n\\n        Analogies are word1 to word2 as word3 is to __\\n        This methid will print: word1 : word2 :: word3 : word4\\n\\n        Args:\\n            word1, word2, word3\\n        \\\"\\\"\\\"\\n        vec1 = self.get_embedding(word1)\\n        vec2 = self.get_embedding(word2)\\n        vec3 = self.get_embedding(word3)\\n\\n        spatial_relationship = vec2 - vec1\\n        vec4 = vec3 + spatial_relationship\\n\\n        closed_words = self.get_closed_to_vector(vec4, n=4)\\n        existing_words = set([word1, word2, word3])\\n        closed_words = [word for word in closed_words if word not in existing_words]\\n        if len(closed_words) == 0:\\n            print(\\\"Could not find nearest neighbors for the vector!\\\")\\n            return\\n        for word4 in closed_words:\\n            print(f\\\"{word1}:{word2} :: {word3}:{word4}\\\")\\n\\n\\nembeddings = PreTrainedEmbeddings.from_embeddings_file(\\\"../data/glove.6B.100d.txt\\\")\";\n",
       "                var nbb_formatted_code = \"# Loading Embeddings\\n# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\\n%load_ext nb_black\\n\\nimport numpy as np\\nfrom annoy import AnnoyIndex\\n\\n\\nclass PreTrainedEmbeddings(object):\\n    def __init__(self, word_to_index, word_vectors):\\n        \\\"\\\"\\\"\\n        Args:\\n            word_to_index: mapping from word to integers.\\n            word_vectors: list of numpy array.\\n        \\\"\\\"\\\"\\n        self.word_to_index = word_to_index\\n        self.word_vectors = word_vectors\\n        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\\n        self.index = AnnoyIndex(len(word_vectors[0]), metric=\\\"euclidean\\\")\\n        for _, i in self.word_to_index.items():\\n            self.index.add_item(i, self.word_vectors[i])\\n        self.index.build(50)\\n\\n    @classmethod\\n    def from_embeddings_file(cls, embedding_file):\\n        \\\"\\\"\\\"\\n        Init from pretrained vector file.\\n\\n        Vector filw should be of the format:\\n            word0 x0_0 x0_1, x0_2 ... x0_N\\n            word1 x1_0 x1_1, x1_2 ... x1_N\\n\\n        Args:\\n            embedding_file: location of the file\\n        Returns:\\n            instance of PretrainedEmbeddings\\n        \\\"\\\"\\\"\\n        word_to_index, word_vectors = {}, []\\n        with open(embedding_file) as fp:\\n            for line in fp.readlines():\\n                line = line.split(\\\" \\\")\\n                word = line[0]\\n                vec = np.array([float(x) for x in line[1:]])\\n\\n                word_to_index[word] = len(word_to_index)\\n                word_vectors.append(vec)\\n        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\\n\\n    def get_embedding(self, word):\\n        \\\"\\\"\\\"\\n        Args:\\n            word: Input word to get embedding for.\\n        Returns:\\n            an embedding for given word\\n        \\\"\\\"\\\"\\n        return self.word_vectors[self.word_to_index[word]]\\n\\n    def get_closed_to_vector(self, vector, n=1):\\n        \\\"\\\"\\\"\\n        Given a vector, return its n nearest neighbors.\\n\\n        Args:\\n            vector: should match the size of the vectors in the Annoy Index.\\n            n: the number of neighbors to return\\n        Returns:\\n            Unsorted list of words nearest to the given vector.\\n        \\\"\\\"\\\"\\n        nn_indices = self.index.get_nns_by_vector(vector, n)\\n        return [self.index_to_word[neighbor] for neighbor in nn_indices]\\n\\n    def compute_and_print_analogy(self, word1, word2, word3):\\n        \\\"\\\"\\\"\\n        Prints the solutions to analogies using word embeddings.\\n\\n        Analogies are word1 to word2 as word3 is to __\\n        This methid will print: word1 : word2 :: word3 : word4\\n\\n        Args:\\n            word1, word2, word3\\n        \\\"\\\"\\\"\\n        vec1 = self.get_embedding(word1)\\n        vec2 = self.get_embedding(word2)\\n        vec3 = self.get_embedding(word3)\\n\\n        spatial_relationship = vec2 - vec1\\n        vec4 = vec3 + spatial_relationship\\n\\n        closed_words = self.get_closed_to_vector(vec4, n=4)\\n        existing_words = set([word1, word2, word3])\\n        closed_words = [word for word in closed_words if word not in existing_words]\\n        if len(closed_words) == 0:\\n            print(\\\"Could not find nearest neighbors for the vector!\\\")\\n            return\\n        for word4 in closed_words:\\n            print(f\\\"{word1}:{word2} :: {word3}:{word4}\\\")\\n\\n\\nembeddings = PreTrainedEmbeddings.from_embeddings_file(\\\"../data/glove.6B.100d.txt\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading Embeddings\n",
    "# Download Embeddings file from https://www.kaggle.com/danielwillgeorge/glove6b100dtxt?select=glove.6B.100d.txt\n",
    "%load_ext nb_black\n",
    "\n",
    "import numpy as np\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "\n",
    "class PreTrainedEmbeddings(object):\n",
    "    def __init__(self, word_to_index, word_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word_to_index: mapping from word to integers.\n",
    "            word_vectors: list of numpy array.\n",
    "        \"\"\"\n",
    "        self.word_to_index = word_to_index\n",
    "        self.word_vectors = word_vectors\n",
    "        self.index_to_word = {v: k for k, v in self.word_to_index.items()}\n",
    "        self.index = AnnoyIndex(len(word_vectors[0]), metric=\"euclidean\")\n",
    "        for _, i in self.word_to_index.items():\n",
    "            self.index.add_item(i, self.word_vectors[i])\n",
    "        self.index.build(50)\n",
    "\n",
    "    @classmethod\n",
    "    def from_embeddings_file(cls, embedding_file):\n",
    "        \"\"\"\n",
    "        Init from pretrained vector file.\n",
    "\n",
    "        Vector filw should be of the format:\n",
    "            word0 x0_0 x0_1, x0_2 ... x0_N\n",
    "            word1 x1_0 x1_1, x1_2 ... x1_N\n",
    "\n",
    "        Args:\n",
    "            embedding_file: location of the file\n",
    "        Returns:\n",
    "            instance of PretrainedEmbeddings\n",
    "        \"\"\"\n",
    "        word_to_index, word_vectors = {}, []\n",
    "        with open(embedding_file) as fp:\n",
    "            for line in fp.readlines():\n",
    "                line = line.split(\" \")\n",
    "                word = line[0]\n",
    "                vec = np.array([float(x) for x in line[1:]])\n",
    "\n",
    "                word_to_index[word] = len(word_to_index)\n",
    "                word_vectors.append(vec)\n",
    "        return cls(word_to_index=word_to_index, word_vectors=word_vectors)\n",
    "\n",
    "    def get_embedding(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word: Input word to get embedding for.\n",
    "        Returns:\n",
    "            an embedding for given word\n",
    "        \"\"\"\n",
    "        return self.word_vectors[self.word_to_index[word]]\n",
    "\n",
    "    def get_closed_to_vector(self, vector, n=1):\n",
    "        \"\"\"\n",
    "        Given a vector, return its n nearest neighbors.\n",
    "\n",
    "        Args:\n",
    "            vector: should match the size of the vectors in the Annoy Index.\n",
    "            n: the number of neighbors to return\n",
    "        Returns:\n",
    "            Unsorted list of words nearest to the given vector.\n",
    "        \"\"\"\n",
    "        nn_indices = self.index.get_nns_by_vector(vector, n)\n",
    "        return [self.index_to_word[neighbor] for neighbor in nn_indices]\n",
    "\n",
    "    def compute_and_print_analogy(self, word1, word2, word3):\n",
    "        \"\"\"\n",
    "        Prints the solutions to analogies using word embeddings.\n",
    "\n",
    "        Analogies are word1 to word2 as word3 is to __\n",
    "        This methid will print: word1 : word2 :: word3 : word4\n",
    "\n",
    "        Args:\n",
    "            word1, word2, word3\n",
    "        \"\"\"\n",
    "        vec1 = self.get_embedding(word1)\n",
    "        vec2 = self.get_embedding(word2)\n",
    "        vec3 = self.get_embedding(word3)\n",
    "\n",
    "        spatial_relationship = vec2 - vec1\n",
    "        vec4 = vec3 + spatial_relationship\n",
    "\n",
    "        closed_words = self.get_closed_to_vector(vec4, n=4)\n",
    "        existing_words = set([word1, word2, word3])\n",
    "        closed_words = [word for word in closed_words if word not in existing_words]\n",
    "        if len(closed_words) == 0:\n",
    "            print(\"Could not find nearest neighbors for the vector!\")\n",
    "            return\n",
    "        for word4 in closed_words:\n",
    "            print(f\"{word1}:{word2} :: {word3}:{word4}\")\n",
    "\n",
    "\n",
    "embeddings = PreTrainedEmbeddings.from_embeddings_file(\"../data/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4559715a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the relationship between gendered nouns and pronouns\n",
      "man:he :: woman:she\n",
      "man:he :: woman:never\n",
      "\n",
      "Verb-noun relationships\n",
      "fly:plane :: sail:ship\n",
      "fly:plane :: sail:vessel\n",
      "\n",
      "Noun-noun relationships\n",
      "cat:kitten :: dog:puppy\n",
      "cat:kitten :: dog:puppies\n",
      "cat:kitten :: dog:toddler\n",
      "\n",
      "Hypernymy (broader category)\n",
      "blue:color :: dog:behavior\n",
      "blue:color :: dog:touch\n",
      "blue:color :: dog:viewer\n",
      "\n",
      "Meronymy (part-to-whole)\n",
      "toe:foot :: finger:ground\n",
      "toe:foot :: finger:pointing\n",
      "\n",
      "Troponymy (difference in manner)\n",
      "talk:communicate :: read:interpret\n",
      "talk:communicate :: read:typed\n",
      "talk:communicate :: read:correctly\n",
      "talk:communicate :: read:instructions\n",
      "\n",
      "Metonymy (convention / figures of speech)\n",
      "blue:democrat :: red:republican\n",
      "blue:democrat :: red:congressman\n",
      "blue:democrat :: red:senator\n",
      "\n",
      "Adjectival scales\n",
      "fast:fastest :: young:female\n",
      "fast:fastest :: young:fellow\n",
      "fast:fastest :: young:younger\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"# Relationships between word embeddings\\n\\n# Relationship 1: the relationship between gendered nouns and pronouns\\nprint(\\\"the relationship between gendered nouns and pronouns\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"he\\\", \\\"woman\\\")\\nprint()\\n\\n# Relationship 2: Verb-noun relationships\\nprint(\\\"Verb-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"fly\\\", \\\"plane\\\", \\\"sail\\\")\\nprint()\\n\\n#  Relationship 3: Noun-noun relationships\\nprint(\\\"Noun-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"cat\\\", \\\"kitten\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 4: Hypernymy (broader category)\\nprint(\\\"Hypernymy (broader category)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"color\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 5: Meronymy (part-to-whole)\\nprint(\\\"Meronymy (part-to-whole)\\\")\\nembeddings.compute_and_print_analogy(\\\"toe\\\", \\\"foot\\\", \\\"finger\\\")\\nprint()\\n\\n# Relationship 6: Troponymy (difference in manner)\\nprint(\\\"Troponymy (difference in manner)\\\")\\nembeddings.compute_and_print_analogy(\\\"talk\\\", \\\"communicate\\\", \\\"read\\\")\\nprint()\\n\\n# Relationship 7: Metonymy (convention / figures of speech)\\nprint(\\\"Metonymy (convention / figures of speech)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"democrat\\\", \\\"red\\\")\\nprint()\\n\\n# Relationship 8: Adjectival scales\\nprint(\\\"Adjectival scales\\\")\\nembeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"young\\\")\\nprint()\";\n",
       "                var nbb_formatted_code = \"# Relationships between word embeddings\\n\\n# Relationship 1: the relationship between gendered nouns and pronouns\\nprint(\\\"the relationship between gendered nouns and pronouns\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"he\\\", \\\"woman\\\")\\nprint()\\n\\n# Relationship 2: Verb-noun relationships\\nprint(\\\"Verb-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"fly\\\", \\\"plane\\\", \\\"sail\\\")\\nprint()\\n\\n#  Relationship 3: Noun-noun relationships\\nprint(\\\"Noun-noun relationships\\\")\\nembeddings.compute_and_print_analogy(\\\"cat\\\", \\\"kitten\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 4: Hypernymy (broader category)\\nprint(\\\"Hypernymy (broader category)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"color\\\", \\\"dog\\\")\\nprint()\\n\\n# Relationship 5: Meronymy (part-to-whole)\\nprint(\\\"Meronymy (part-to-whole)\\\")\\nembeddings.compute_and_print_analogy(\\\"toe\\\", \\\"foot\\\", \\\"finger\\\")\\nprint()\\n\\n# Relationship 6: Troponymy (difference in manner)\\nprint(\\\"Troponymy (difference in manner)\\\")\\nembeddings.compute_and_print_analogy(\\\"talk\\\", \\\"communicate\\\", \\\"read\\\")\\nprint()\\n\\n# Relationship 7: Metonymy (convention / figures of speech)\\nprint(\\\"Metonymy (convention / figures of speech)\\\")\\nembeddings.compute_and_print_analogy(\\\"blue\\\", \\\"democrat\\\", \\\"red\\\")\\nprint()\\n\\n# Relationship 8: Adjectival scales\\nprint(\\\"Adjectival scales\\\")\\nembeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"young\\\")\\nprint()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Relationships between word embeddings\n",
    "\n",
    "# Relationship 1: the relationship between gendered nouns and pronouns\n",
    "print(\"the relationship between gendered nouns and pronouns\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"he\", \"woman\")\n",
    "print()\n",
    "\n",
    "# Relationship 2: Verb-noun relationships\n",
    "print(\"Verb-noun relationships\")\n",
    "embeddings.compute_and_print_analogy(\"fly\", \"plane\", \"sail\")\n",
    "print()\n",
    "\n",
    "#  Relationship 3: Noun-noun relationships\n",
    "print(\"Noun-noun relationships\")\n",
    "embeddings.compute_and_print_analogy(\"cat\", \"kitten\", \"dog\")\n",
    "print()\n",
    "\n",
    "# Relationship 4: Hypernymy (broader category)\n",
    "print(\"Hypernymy (broader category)\")\n",
    "embeddings.compute_and_print_analogy(\"blue\", \"color\", \"dog\")\n",
    "print()\n",
    "\n",
    "# Relationship 5: Meronymy (part-to-whole)\n",
    "print(\"Meronymy (part-to-whole)\")\n",
    "embeddings.compute_and_print_analogy(\"toe\", \"foot\", \"finger\")\n",
    "print()\n",
    "\n",
    "# Relationship 6: Troponymy (difference in manner)\n",
    "print(\"Troponymy (difference in manner)\")\n",
    "embeddings.compute_and_print_analogy(\"talk\", \"communicate\", \"read\")\n",
    "print()\n",
    "\n",
    "# Relationship 7: Metonymy (convention / figures of speech)\n",
    "print(\"Metonymy (convention / figures of speech)\")\n",
    "embeddings.compute_and_print_analogy(\"blue\", \"democrat\", \"red\")\n",
    "print()\n",
    "\n",
    "# Relationship 8: Adjectival scales\n",
    "print(\"Adjectival scales\")\n",
    "embeddings.compute_and_print_analogy(\"fast\", \"fastest\", \"young\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2d8fedc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast:fastest :: small:smallest\n",
      "fast:fastest :: small:large\n",
      "man:king :: woman:queen\n",
      "man:king :: woman:monarch\n",
      "man:king :: woman:throne\n",
      "man:doctor :: woman:nurse\n",
      "man:doctor :: woman:physician\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"small\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"king\\\", \\\"woman\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"doctor\\\", \\\"woman\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"fast\\\", \\\"fastest\\\", \\\"small\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"king\\\", \\\"woman\\\")\\nembeddings.compute_and_print_analogy(\\\"man\\\", \\\"doctor\\\", \\\"woman\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"fast\", \"fastest\", \"small\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"king\", \"woman\")\n",
    "embeddings.compute_and_print_analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b38e4006",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sachin:cricket :: messi:rugby\n",
      "sachin:cricket :: messi:soccer\n",
      "sachin:cricket :: messi:football\n",
      "sachin:cricket :: messi:club\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"sachin\\\", \\\"cricket\\\", \\\"messi\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"sachin\\\", \\\"cricket\\\", \\\"messi\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"sachin\", \"cricket\", \"messi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24036595",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nifty:sensex :: nasdaq:index\n",
      "nifty:sensex :: nasdaq:composite\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"embeddings.compute_and_print_analogy(\\\"nifty\\\", \\\"sensex\\\", \\\"nasdaq\\\")\";\n",
       "                var nbb_formatted_code = \"embeddings.compute_and_print_analogy(\\\"nifty\\\", \\\"sensex\\\", \\\"nasdaq\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings.compute_and_print_analogy(\"nifty\", \"sensex\", \"nasdaq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd5a89",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Example: Learning the Continous Bag of Words Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0181b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The CBOW model is a multi‐ class classification task represented by scanning over texts of words, creating a context window of words, removing the center word from the context window, and classifying the context window to the missing word. It is actually like a fill-in-the-blank task. There is a sentence with a missing word, and the model’s job is to figure out what that word should be.\n",
    "\n",
    "![Figure 5.2](../images/figure_5_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa02da",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Vectorization Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5b129914",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "edee0873",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Class to process text and extract vocabulary for mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx: a pre-existing map of tokens to indices.\n",
    "            mask_token: the MASK token to add into the Vocab.\n",
    "            add_unk: a flag that indicates whether to add the UNK token.\n",
    "            unk_token: the UNK token to add into the vocab.\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary that can be serialized.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"token_to_idx\": self._token_to_idx,\n",
    "            \"add_unk\": self._add_unk,\n",
    "            \"unk_token\": self._unk_token,\n",
    "            \"mask_token\": self._mask_token,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Instantiates the vocab from a serialized dictionary.\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token: the item to add into the vocab.\n",
    "        Returns:\n",
    "            index: the integer corresponding to the token.\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"\n",
    "        Add a list of tokens into the Vocabulary.\n",
    "\n",
    "        Args:\n",
    "            tokens: a list of string tokens.\n",
    "        Returns:\n",
    "            indices: a list of indices corresponding to the tokens.\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index associated with the token or the UNK\n",
    "        index if token isnt present.\n",
    "\n",
    "        Args:\n",
    "            token: the token to lookup.\n",
    "        Returns:\n",
    "            index: the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >= 0 (having been added into the vocab).\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the token associated with the index.\n",
    "\n",
    "        Args:\n",
    "            index: the index to look up.\n",
    "        Returns:\n",
    "            token: the token corresponding to the index.\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocab.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"the index {index} is not in the vocab\")\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e97e4304",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CBOWVectorizer(object):\n",
    "    \"\"\"\n",
    "    The vectorizer which coordinates the vocabularies and puts them to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab: maps words to integers.\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context: the string of words separated by a space.\n",
    "            vector_length: an argument for forcing the length of index vector.\n",
    "        \"\"\"\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(\" \")]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[: len(indices)] = indices\n",
    "        out_vector[len(indices) :] = self.cbow_vocab.mask_index\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from the dataset df.\n",
    "\n",
    "        Args:\n",
    "            cbow_df: the target dataset.\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer.\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(\" \"):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = Vocabulary.from_serializable(contents[\"cbow_vocab\"])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"cbow_vocab\": self.cbow_vocab.to_serializable()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6834575b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df: the dataset.\n",
    "            vectorizer: vectorizer instantiated from dataset.\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "        self.train_df = self.cbow_df[self.cbow_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split == \"val\"]\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, self.train_size),\n",
    "            \"val\": (self.val_df, self.val_size),\n",
    "            \"test\": (self.test_df, self.test_size),\n",
    "        }\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"\n",
    "        Load dataset and make a new vectorizer.\n",
    "\n",
    "        Args:\n",
    "            cbow_csb: location of the dataset.\n",
    "        Returns:\n",
    "            an instance of CBOWDataset.\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split == \"train\"]\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csb, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use.\n",
    "\n",
    "        Args:\n",
    "            cbow_csb: location of the dataset.\n",
    "            vectorizer_filepath: location of the saved vectorizer.\n",
    "        Returns:\n",
    "            an instance of CBOWDataset.\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        A static method for loading the vectorizer from file.\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath: the location of the serialized vectorizer.\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer.\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Saves the vectorizer to disk using json.\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath: the location to save the vectorizer.\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Returns the vectorizer.\n",
    "        \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Selects the splits in the dataset using a column in the dataframe.\n",
    "        \"\"\"\n",
    "        self._train_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entryp point method for PyTorch dataset.\n",
    "\n",
    "        Args:\n",
    "            index: the index to the data point.\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features(x_data) and label(y_target).\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        context_vector = self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "        return {\"x_data\": context_vector, \"y_target\": target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf6a77",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ef081a64",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CBOWClassifier(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocabulary_size: number of vocab items.\n",
    "            embedding_size: size of the embeddings.\n",
    "            padding_idx: default 0, Embedding will not use this index.\n",
    "        \"\"\"\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocabulary_size,\n",
    "            embedding_dim=embedding_size,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size, out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        The forward pass of the classifier.\n",
    "\n",
    "        Args:\n",
    "            x_in: an input data tensor. x_in.shape should be (batch, input_dim).\n",
    "            apply_softmax: a flag for the softmax activation.\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim).\n",
    "        \"\"\"\n",
    "        x_embedded_sum = self.embedding(x_in).sum(dim=1)\n",
    "        x_embedded_sum = F.dropout(x_embedded_sum, 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7e16cd",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "79e9de1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/chapter05/cbow/vectorizer.json\n",
      "\tmodels/chapter05/cbow/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"../data/books/frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/chapter05/cbow\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=50,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "01d131ea",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset & creating vectorizer\n",
      "CBOWClassifier(\n",
      "  (embedding): Embedding(6138, 50, padding_idx=0)\n",
      "  (fc1): Linear(in_features=50, out_features=6138, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and loading vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(\n",
    "        args.cbow_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & creating vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = CBOWClassifier(\n",
    "    vocabulary_size=len(vectorizer.cbow_vocab), embedding_size=args.embedding_size\n",
    ")\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a5cba44",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dc749967574b1e8523524f3b4c8a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ca1546232e4a15b0f7185bc283160c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/1984 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70fecb7a5dd4045bed379d1b34563aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/425 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=63489 ============\n",
      "============ Split=val, Size=13605 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=8.791101831822646, Training Accuracy=1.5703755040322613\n",
      "Validation Loss=8.070989081438857, Validation Accuracy=4.176470588235291.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=5.938392519229839, Training Accuracy=13.904989919354827\n",
      "Validation Loss=6.7094865484798625, Validation Accuracy=13.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=5.410481493799914, Training Accuracy=15.265877016129041\n",
      "Validation Loss=6.579481416590075, Validation Accuracy=13.98529411764705.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=5.16410692624987, Training Accuracy=15.908518145161317\n",
      "Validation Loss=6.535959159626684, Validation Accuracy=14.55882352941176.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=5.126896913974522, Training Accuracy=16.0187752016129\n",
      "Validation Loss=6.535583877563475, Validation Accuracy=14.735294117647067.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=13604 ============\n",
      "-------- Test Accuracy=13.272058823529406, Test Loss=7.6933134325812835.--------\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5455f1e6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Trained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "628a4814",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty Print Embedding Results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print(f\"[{item[1]}] = {item[0]}\")\n",
    "        \n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest words to your word.\n",
    "    \"\"\"\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n+2]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0a217dc8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a word:monster\n",
      "[7.512077808380127] = cares\n",
      "[7.6816558837890625] = griefs\n",
      "[7.735276222229004] = saw\n",
      "[7.779043674468994] = confused\n",
      "[7.783658027648926] = truly\n",
      "[7.7867045402526855] = relief\n"
     ]
    }
   ],
   "source": [
    "word = input(\"Enter a word:\")\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "pretty_print(\n",
    "    get_closest(word, word_to_idx, embeddings, n=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6abf37be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======frankenstein=======\n",
      "[7.184050559997559] = irradiated\n",
      "[7.610386371612549] = men\n",
      "[7.64546537399292] = enslaved\n",
      "[7.70945405960083] = mode\n",
      "[7.720515251159668] = professor\n",
      "[7.734167098999023] = wound\n",
      "=======monster=======\n",
      "[7.512077808380127] = cares\n",
      "[7.6816558837890625] = griefs\n",
      "[7.735276222229004] = saw\n",
      "[7.779043674468994] = confused\n",
      "[7.783658027648926] = truly\n",
      "[7.7867045402526855] = relief\n",
      "=======science=======\n",
      "[6.982034206390381] = mutual\n",
      "[6.998227596282959] = impression\n",
      "[7.0495219230651855] = mist\n",
      "[7.153514385223389] = swelling\n",
      "[7.237594127655029] = darkened\n",
      "[7.29238748550415] = nearly\n",
      "=======sickness=======\n",
      "[6.25612211227417] = while\n",
      "[6.5490875244140625] = awoke\n",
      "[6.605112075805664] = foundations\n",
      "[6.687403202056885] = consoles\n",
      "[6.693512916564941] = depend\n",
      "[6.726706504821777] = literally\n",
      "=======lonely=======\n",
      "[6.723246097564697] = excessive\n",
      "[6.872636795043945] = ought\n",
      "[6.897297382354736] = moonlight\n",
      "[7.065033912658691] = bed\n",
      "[7.114339828491211] = three\n",
      "[7.160595893859863] = superhuman\n",
      "=======happy=======\n",
      "[6.369864463806152] = bottom\n",
      "[6.404392242431641] = penetrated\n",
      "[6.4275641441345215] = wand\n",
      "[6.476714134216309] = altered\n",
      "[6.526817798614502] = danger\n",
      "[6.536870002746582] = chivalry\n"
     ]
    }
   ],
   "source": [
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words: \n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ec7f4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Example: Transfer Learning Using Pretrained Embeddings for Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c34cd7",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Data Vectorization classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6b6a14db",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "79aff316",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Class to process text and extract vocabulary for mapping.\n",
    "    \"\"\"\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices.\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {\n",
    "            idx: token\n",
    "            for token, idx in self._token_to_idx.items()\n",
    "        }\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary that can be serialized.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'token_to_idx': self._token_to_idx\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Instantiates the Vocabulary from a Srialized Dictionary.\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "    \n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update mapping dicts based on the token.\n",
    "        \n",
    "        Args:\n",
    "            token: the item to add into the Vocab.\n",
    "        Returns:\n",
    "            index: the integer corresponding to the token.\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "    \n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"\n",
    "        Add a list of tokens into the Vocab.\n",
    "        \n",
    "        Args:\n",
    "            tokens: a list of string tokens.\n",
    "        Returns:\n",
    "            indices: a list of indices corresponding to the tokens.\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index associated with the token.\n",
    "        \n",
    "        Args:\n",
    "            token: the token to look up.\n",
    "        Returns:\n",
    "            index: the index corresponding to the token.\n",
    "        \"\"\"\n",
    "        return self._token_to_idx[token]\n",
    "    \n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the token associated with the index.\n",
    "        \n",
    "        Args:\n",
    "            index: the index to lookup.\n",
    "        Returns:\n",
    "            token: the token corresponding to the index.\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the vocab.\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"The index {index} is not in the Vocab.\")\n",
    "        return self._idx_to_token[index]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbbc88",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Figure 5.3](../images/figure_5_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f497cd8b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(\n",
    "        self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "        mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "        end_seq_token=\"<END>\"\n",
    "    ):\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "        \n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update(\n",
    "            {\n",
    "                'unk_token': self._unk_token,\n",
    "                'mask_token': self._mask_token,\n",
    "                'begin_seq_token': self._begin_seq_token,\n",
    "                'end_seq_token': self._end_seq_token\n",
    "            }\n",
    "        )\n",
    "        return contents\n",
    "    \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a32e5535",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NewsVectorizer(object):\n",
    "    def __init__(self, title_vocab, category_vocab):\n",
    "        self.title_vocab = title_vocab\n",
    "        self.category_vocab = category_vocab\n",
    "        \n",
    "    def vectorize(self, title, vector_length=-1):\n",
    "        indices = [self.title_vocab.begin_seq_index]\n",
    "        indices.extend(\n",
    "            self.title_vocab.lookup_token(token)\n",
    "            for token in title.split(\" \")\n",
    "        )\n",
    "        indices.append(self.title_vocab.end_seq_index)\n",
    "        \n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "            \n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.title_vocab.mask_index\n",
    "        return out_vector\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, news_df, cutoff=25):\n",
    "        category_vocab = Vocabulary()\n",
    "        for category in sorted(set(news_df.category)):\n",
    "            category_vocab.add_token(category)\n",
    "        \n",
    "        word_counts = Counter()\n",
    "        for title in news_df.title:\n",
    "            for token in title.split(\" \"):\n",
    "                if token not in string.punctuation:\n",
    "                    word_counts[token] += 1\n",
    "        \n",
    "        title_vocab = SequenceVocabulary()\n",
    "        for word, word_count in word_counts.items():\n",
    "            if word_count >= cutoff:\n",
    "                title_vocab.add_token(word)\n",
    "        return cls(title_vocab, category_vocab)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        title_vocab = SequenceVocabulary().from_serializable(\n",
    "            contents['title_vocab']\n",
    "        )\n",
    "        category_vocab = Vocabulary.from_serializable(\n",
    "            contents['category_vocab']\n",
    "        )\n",
    "        return cls(title_vocab=title_vocab, category_vocab=category_vocab)\n",
    "    \n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            'title_vocab': self.title_vocab.to_serializable(),\n",
    "            'category_vocab': self.category_vocab.to_serializable()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4c26a133",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, news_df, vectorizer):\n",
    "        self.news_df = news_df\n",
    "        self._vectorizer = vectorizer\n",
    "        \n",
    "        measure_len = lambda content: len(content.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, news_df.title)) + 2\n",
    "\n",
    "        self.train_df = self.news_df[self.news_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "        \n",
    "        self.val_df = self.news_df[self.news_df.split == 'val']\n",
    "        self.val_size = len(self.val_df)\n",
    "        \n",
    "        self.test_df = self.news_df[self.news_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "        \n",
    "        self._lookup_dict = {\n",
    "            'train': (self.train_df, self.train_size),\n",
    "            'val': (self.val_df, self.val_size),\n",
    "            'test': (self.test_df, self.test_size)\n",
    "        }\n",
    "        self.set_split('train')\n",
    "        \n",
    "        class_counts = news_df.category.value_counts().to_dict()\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.category_vocab.lookup_token(item[0])\n",
    "        \n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequences = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequences, dtype=torch.float32)\n",
    "        \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, news_csv):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        train_news_df = news_df[news_df.split == 'train']\n",
    "        return cls(news_df, \n",
    "                   NewsVectorizer.from_dataframe(train_news_df))\n",
    "    \n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, news_csv, vectorizer_filepath):\n",
    "        news_df = pd.read_csv(news_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(news_csv, vectorizer)\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NewsVectorizer.from_serializable(json.load(fp))\n",
    "    \n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, 'w') as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "            \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "    \n",
    "    def set_split(self, split='train'):\n",
    "        self._train_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        title_vector = self._vectorizer.vectorize(\n",
    "            row.title, self._max_seq_length\n",
    "        )\n",
    "        category_index = self._vectorizer.category_vocab.lookup_token(\n",
    "            row.category\n",
    "        )\n",
    "        return {\n",
    "            'x_data': title_vector,\n",
    "            'y_target': category_index\n",
    "        }\n",
    "    \n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10de8e74",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The NewsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f0a70d10",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NewsClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, embedding_size, num_embeddings, num_channels,\n",
    "        hidden_dim, num_classes, dropout_p,\n",
    "        pretrained_embeddings=None, padding_idx=0\n",
    "    ):\n",
    "        super(NewsClassifier, self).__init__()\n",
    "        if pretrained_embeddings is None:\n",
    "            self.emb = nn.Embedding(\n",
    "                embedding_dim=embedding_size,\n",
    "                num_embeddings=num_embeddings,\n",
    "                padding_idx=padding_idx\n",
    "            )\n",
    "        else:\n",
    "            pretrained_embeddings = torch.from_numpy(pretrained_embeddings).float()\n",
    "            self.emb = nn.Embedding(\n",
    "                embedding_dim=embedding_size,\n",
    "                num_embeddings=num_embeddings,\n",
    "                padding_idx=padding_idx,\n",
    "                _weight=pretrained_embeddings\n",
    "            )\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_size,out_channels=num_channels,\n",
    "                kernel_size=3\n",
    "            ),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3, stride=2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv1d(in_channels=num_channels, out_channels=num_channels,\n",
    "                     kernel_size=3),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self._dropout_p = dropout_p\n",
    "        self.fc1 = nn.Linear(num_channels, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        # Why permute here?\n",
    "        # Embed and permute so features are channels\n",
    "        x_embedded = self.emb(x_in).permute(0, 2, 1)\n",
    "        features = self.convnet(x_embedded)\n",
    "        \n",
    "        # Average and remove the extra dimension\n",
    "        remaining_size = features.size(dim=2)\n",
    "        features = F.avg_pool1d(features, remaining_size).squeeze(dim=2)\n",
    "        features = F.dropout(features, p=self._dropout_p)\n",
    "        \n",
    "        # MLP Classifier\n",
    "        intermediate_vector = F.relu(\n",
    "            F.dropout(\n",
    "                self.fc1(features),\n",
    "                p=self._dropout_p))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55c4d7d",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3b8f994",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_glove_from_file(glove_filepath):\n",
    "    word_to_index = {}\n",
    "    embeddings = []\n",
    "    with open(glove_filepath, 'r') as fp:\n",
    "        for index, line in enumerate(fp):\n",
    "            line = line.split(\" \")\n",
    "            word_to_index[line[0]] = index\n",
    "            embedding_i = np.array(\n",
    "                [float(val) for val in line[1:]]\n",
    "            )\n",
    "            embeddings.append(embedding_i)\n",
    "    return word_to_index, np.stack(embeddings)\n",
    "\n",
    "def make_embedding_matrix(glove_filepath, words):\n",
    "    word_to_idx, glove_embeddings = load_glove_from_file(glove_filepath)\n",
    "    embedding_size = glove_embeddings.shape[1]\n",
    "    final_embeddings = np.zeros((len(words), embedding_size))\n",
    "    for i, word in enumerate(words):\n",
    "        if word in word_to_idx:\n",
    "            final_embeddings[i, :] = glove_embeddings[word_to_idx[word]]\n",
    "        else:\n",
    "            embedding_i = torch.ones(1, embedding_size)\n",
    "            torch.nn.init.xavier_uniform_(embedding_i)\n",
    "            final_embeddings[i, :] = embedding_i\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc099894",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "75f91784",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/chapter05/document_classification/vectorizer.json\n",
      "\tmodels/chapter05/document_classification/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    news_csv=\"../data/ag_news/news_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/chapter05/document_classification\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath='../data/glove.6B.100d.txt', \n",
    "    use_glove=False,\n",
    "    embedding_size=100, \n",
    "    hidden_dim=100, \n",
    "    num_channels=100, \n",
    "    # Training hyper parameter\n",
    "    seed=1337, \n",
    "    learning_rate=0.001, \n",
    "    dropout_p=0.1, \n",
    "    batch_size=128, \n",
    "    num_epochs=100, \n",
    "    early_stopping_criteria=5, \n",
    "    # Runtime option\n",
    "    cuda=True, \n",
    "    catch_keyboard_interrupt=True, \n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ") \n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "    \n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d295220d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n"
     ]
    }
   ],
   "source": [
    "args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = NewsDataset.load_dataset_and_load_vectorizer(args.news_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.news_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.title_vocab._token_to_idx.keys()\n",
    "    embeddings = make_embedding_matrix(glove_filepath=args.glove_filepath, \n",
    "                                       words=words)\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = NewsClassifier(embedding_size=args.embedding_size, \n",
    "                            num_embeddings=len(vectorizer.title_vocab),\n",
    "                            num_channels=args.num_channels,\n",
    "                            hidden_dim=args.hidden_dim, \n",
    "                            num_classes=len(vectorizer.category_vocab), \n",
    "                            dropout_p=args.dropout_p,\n",
    "                            pretrained_embeddings=embeddings,\n",
    "                            padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3f7d3c68",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e334fae76334a308b21870df77cbb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4e27fec6f44292ad2517e0c7f74a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/656 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb510db0e8d4ae994883a96d3617ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=84000 ============\n",
      "============ Split=val, Size=18000 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.7482051128839566, Training Accuracy=70.51733993902434\n",
      "Validation Loss=0.5905174578939169, Validation Accuracy=78.16406250000003.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.35484601164282104, Training Accuracy=86.50438262195128\n",
      "Validation Loss=0.5897864873920168, Validation Accuracy=79.5591517857143.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.3304986624592324, Training Accuracy=87.51548208841474\n",
      "Validation Loss=0.625057249196938, Validation Accuracy=79.1685267857143.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.32986575882972735, Training Accuracy=87.51786394817078\n",
      "Validation Loss=0.6239482183541574, Validation Accuracy=79.16852678571423.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.3295513896453308, Training Accuracy=87.54287347560985\n",
      "Validation Loss=0.6199696140629906, Validation Accuracy=79.26339285714283.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.32965323692414833, Training Accuracy=87.49523628048786\n",
      "Validation Loss=0.622236075571605, Validation Accuracy=79.0122767857143.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.3299218930712926, Training Accuracy=87.5655011432926\n",
      "Validation Loss=0.623907563303198, Validation Accuracy=79.1908482142857.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.33046179766789435, Training Accuracy=87.41068025914645\n",
      "Validation Loss=0.6228940814733509, Validation Accuracy=79.1685267857143.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.33060993985613696, Training Accuracy=87.46665396341474\n",
      "Validation Loss=0.6238711838211334, Validation Accuracy=79.11830357142856.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.32979340649141764, Training Accuracy=87.47618140243905\n",
      "Validation Loss=0.6224112172211919, Validation Accuracy=79.38616071428571.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=18000 ============\n",
      "-------- Test Accuracy=79.52566964285711, Test Loss=0.6139586027179449.--------\n"
     ]
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(\n",
    "    classifer.parameters(),\n",
    "    lr=args.learning_rate\n",
    ")\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340b931",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "07602966",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "787626b7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def predict_category(title, classifier, vectorizer, max_length):\n",
    "    title = preprocess_text(title)\n",
    "    vectorized_title = torch.tensor(vectorizer.vectorize(title, vector_length=max_length))\n",
    "    result = classifier(vectorized_title.unsqueeze(0),\n",
    "                       apply_softmax=True)\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    predicted_category = vectorizer.category_vocab.lookup_index(indices.item())\n",
    "    return {\n",
    "        'category': predicted_category,\n",
    "        'probability': probability_values.item()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c477354a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_samples():\n",
    "    samples = {}\n",
    "    for cat in dataset.val_df.category.unique():\n",
    "        samples[cat] = dataset.val_df.title[\n",
    "            dataset.val_df.category == cat\n",
    "        ].tolist()[:15]\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0699cb0a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Category: Business\n",
      "==============================\n",
      "Prediction: Business (p=0.87)\n",
      "\t + Sampe=AZ suspends marketing of cancer drug\n",
      "Prediction: Business (p=0.99)\n",
      "\t + Sampe=Business world has mixed reaction to Perez move\n",
      "Prediction: Sports (p=0.69)\n",
      "\t + Sampe=Betting Against Bombay\n",
      "Prediction: Sports (p=0.39)\n",
      "\t + Sampe=Malpractice Insurers Face a Tough Market\n",
      "Prediction: Sports (p=0.76)\n",
      "\t + Sampe=NVIDIA Is Vindicated\n",
      "Prediction: Sci/Tech (p=0.84)\n",
      "\t + Sampe=It Takes Time to Judge the True Impact of New Technology\n",
      "Prediction: Business (p=0.76)\n",
      "\t + Sampe=Union agrees to Karstadt job cuts\n",
      "Prediction: Sports (p=0.99)\n",
      "\t + Sampe=QRS Jilts JDA, Teams with Inovis\n",
      "Prediction: Sci/Tech (p=0.86)\n",
      "\t + Sampe=Night flights fuel dispute\n",
      "Prediction: Business (p=0.99)\n",
      "\t + Sampe=Intel Profit Up; Outlook Reassures Market\n",
      "Prediction: Business (p=0.69)\n",
      "\t + Sampe=Airbus Raises Delivery Target for 2004\n",
      "Prediction: Sports (p=0.34)\n",
      "\t + Sampe=Bribery Considered, Halliburton Notes Suggest\n",
      "Prediction: Business (p=0.99)\n",
      "\t + Sampe=Mutual Funds Weigh Post-Election Options (Investor's Business Daily)\n",
      "Prediction: Business (p=0.97)\n",
      "\t + Sampe=Unilever and Colgate Issue Profit Warnings for Rest of the Year\n",
      "Prediction: Sports (p=0.38)\n",
      "\t + Sampe=Doctor, heal thyself\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_samples = get_samples()\n",
    "\n",
    "classifier = classifier.to('cpu')\n",
    "for truth, sample_group in val_samples.items():\n",
    "    print(f\"True Category: {truth}\")\n",
    "    print(\"=\" * 30)\n",
    "    for sample in sample_group:\n",
    "        prediction = predict_category(\n",
    "            sample, classifier, vectorizer, dataset._max_seq_length + 1\n",
    "        )\n",
    "        print(f\"Prediction: {prediction['category']} \"\n",
    "              f\"(p={prediction['probability']:.2f})\")\n",
    "        print(f\"\\t + Sampe={sample}\")\n",
    "    print(\"-\" * 30 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Embedding Words and Types",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
