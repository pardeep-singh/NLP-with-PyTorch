{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef96406d",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Sequence Modeling for NLP<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Introduction-to-Recurrent-Neural-Networks\" data-toc-modified-id=\"Introduction-to-Recurrent-Neural-Networks-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Introduction to Recurrent Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implemnting-an-Elman-RNN\" data-toc-modified-id=\"Implemnting-an-Elman-RNN-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Implemnting an Elman RNN</a></span></li></ul></li><li><span><a href=\"#Example:-Classifying-Surname-Nationality-Using-a-Character-RNN\" data-toc-modified-id=\"Example:-Classifying-Surname-Nationality-Using-a-Character-RNN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Example: Classifying Surname Nationality Using a Character RNN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vocabulary,-Vectorizer,-Dataset\" data-toc-modified-id=\"Vocabulary,-Vectorizer,-Dataset-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Vocabulary, Vectorizer, Dataset</a></span></li><li><span><a href=\"#Modeling\" data-toc-modified-id=\"Modeling-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Modeling</a></span></li><li><span><a href=\"#Init-and-Model-Training-+-Evaluation\" data-toc-modified-id=\"Init-and-Model-Training-+-Evaluation-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Init and Model Training + Evaluation</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69895c",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb33b48",
   "metadata": {},
   "source": [
    "- A Sequence is an ordered collection of items ie Language sentences or words.\n",
    "- In DL, Modeling sequences involves maintaining hidden state information or a hidden state. As each item in the sequence is encountered, the hidden state is updated. This hidden state vector(sequence representation) can then be used for different tasks like classification or predicting sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2019baf4",
   "metadata": {},
   "source": [
    "## Introduction to Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c4895",
   "metadata": {},
   "source": [
    "- The Purpose of recurrent neural networds is to model sequences of tensors.\n",
    "- The basic form of RNN is called _Elman RNN_.\n",
    "- Goal of RNN is to learn a representation of a sequence.\n",
    "\n",
    "**RNN Steps**\n",
    "- Hidden state vector is maintained to capture the current state of the sequence.\n",
    "- The hidden state vector is computed from both a current input vector and the previous hidden state vector.\n",
    "- The input from the current time step and the hidden state vector from the previous time step are mapped to the hidden state vector of the current time step.\n",
    "- A new hidden vector is computed using a hidden-to-hideen weigth matric to map the previous hidden state vector and an input-to-hidden weight matric to map the input vector.\n",
    "- The hidden-to-hidden and input-to-hidden weights are shared across the different time steps. During training these weights will be adjusted so that RNN learns how to incorporate incoming inforation and maintain a state representation summarizing the input seen so far.\n",
    "- Using the same weights to transform inputs into outputs at every time step is another example of parameter sharing which is used by CNN. RNN shares parameters across time and CNN shares parameters across space.\n",
    "\n",
    "![Figure 6.1](../images/figure_6_1.png)\n",
    "![Figure 6.2](../images/figure_6_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b57099b",
   "metadata": {},
   "source": [
    "### Implemnting an Elman RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24033dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport torch\\nimport torch.nn as nn\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport torch\\nimport torch.nn as nn\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98b1a628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"class ElmanRNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, batch_first=False):\\n        super(ElmanRNN, self).__init__()\\n        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\\n        self.batch_first = batch_first\\n        self.hidden_size=hidden_size\\n        \\n    def _initialize_hidden(self, batch_size):\\n        return torch.zeros((batch_size, self.hidden_size))\\n    \\n    def forward(self, x_in, initial_hidden=None):\\n        if self.batch_first:\\n            batch_size, seq_size, feat_size = x_in.size()\\n            x_in = x_in.permute(1, 0, 2)\\n        else:\\n            seq_size, batch_size, feat_size = x_in.size()\\n        hiddens = []\\n        if initial_hidden is None:\\n            initial_hidden = self._initialize_hidden(batch_size)\\n            initial_hidden = initial_hidden.to(x_in.device)\\n        hidden_t = initial_hidden\\n        for t in range(seq_size):\\n            hidden_t = self.rnn_cell(x_in[t], hidden_t)\\n            hiddens.append(hidden_t)\\n        hiddens = torch.stack(hiddens)\\n        if self.batch_first:\\n            hiddens = hiddens.permute(1, 0, 2)\\n        return hiddens\";\n",
       "                var nbb_formatted_code = \"class ElmanRNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, batch_first=False):\\n        super(ElmanRNN, self).__init__()\\n        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\\n        self.batch_first = batch_first\\n        self.hidden_size = hidden_size\\n\\n    def _initialize_hidden(self, batch_size):\\n        return torch.zeros((batch_size, self.hidden_size))\\n\\n    def forward(self, x_in, initial_hidden=None):\\n        if self.batch_first:\\n            batch_size, seq_size, feat_size = x_in.size()\\n            x_in = x_in.permute(1, 0, 2)\\n        else:\\n            seq_size, batch_size, feat_size = x_in.size()\\n        hiddens = []\\n        if initial_hidden is None:\\n            initial_hidden = self._initialize_hidden(batch_size)\\n            initial_hidden = initial_hidden.to(x_in.device)\\n        hidden_t = initial_hidden\\n        for t in range(seq_size):\\n            hidden_t = self.rnn_cell(x_in[t], hidden_t)\\n            hiddens.append(hidden_t)\\n        hiddens = torch.stack(hiddens)\\n        if self.batch_first:\\n            hiddens = hiddens.permute(1, 0, 2)\\n        return hiddens\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initialize_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "        hiddens = []\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initialize_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "        hidden_t = initial_hidden\n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "        return hiddens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7e8e4",
   "metadata": {},
   "source": [
    "## Example: Classifying Surname Nationality Using a Character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d288e5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"from argparse import Namespace\\nimport os\\nimport json\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm import tqdm_notebook\\n\\nimport utils\";\n",
       "                var nbb_formatted_code = \"from argparse import Namespace\\nimport os\\nimport json\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm import tqdm_notebook\\n\\nimport utils\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fd9ea",
   "metadata": {},
   "source": [
    "### Vocabulary, Vectorizer, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745c0f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"class Vocabulary(object):\\n    def __init__(self, toekn_to_idx=None):\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idk: token for tken, idx in self._token_to_idx.items()}\\n\\n    def to_serializable(self):\\n        return {\\\"token_to_idx\\\": self._token_to_idx}\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        if token in self._token_to_idx:\\n            index = self._token_to_idx[token]\\n        else:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"The index {index} is not in the Vocab.\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\\n    \\nclass SequenceVocabulary(Vocabulary):\\n    def __init__(\\n        self, token_to_idx=None, unk_token=\\\"<UNK>\\\",\\n        mask_token=\\\"<MASK>\\\", begin_seq_token=\\\"<BEGIN>\\\",\\n        end_seq_token=\\\"<ENF>\\\"\\n    ):\\n        super(SequenceVocabulary, self).__init__(token_to_idx)\\n        self._mask_token = mask_token\\n        self._unk_token = unk_token\\n        self._begin_seq_token = begin_seq_token\\n        self._end_seq_token = end_seq_token\\n        \\n        self.mask_index = self.add_token(self._mask_token)\\n        self.unk_index = self.add_token(self._unk_token)\\n        self.begin_seq_index = self.add_token(self._begin_seq_token)\\n        self.end_seq_index = self.add_token(self._end_seq_token)\\n        \\n    def to_serializable(self):\\n        contents = super(SequenceVocabulary, self).to_serializable()\\n        contents.update(\\n            {\\n                'unk_token': self._unk_token,\\n                'mask_token': self._mask_token,\\n                'begin_seq_token': self._begin_seq_token,\\n                'end_seq_token': self._end_seq_token\\n            }\\n        )\\n        return contents\\n    \\n    def lookup_token(self, token):\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_token)\\n        else:\\n            return self._token_to_idx[token]\";\n",
       "                var nbb_formatted_code = \"class Vocabulary(object):\\n    def __init__(self, toekn_to_idx=None):\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idk: token for tken, idx in self._token_to_idx.items()}\\n\\n    def to_serializable(self):\\n        return {\\\"token_to_idx\\\": self._token_to_idx}\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        if token in self._token_to_idx:\\n            index = self._token_to_idx[token]\\n        else:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"The index {index} is not in the Vocab.\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\\n\\n\\nclass SequenceVocabulary(Vocabulary):\\n    def __init__(\\n        self,\\n        token_to_idx=None,\\n        unk_token=\\\"<UNK>\\\",\\n        mask_token=\\\"<MASK>\\\",\\n        begin_seq_token=\\\"<BEGIN>\\\",\\n        end_seq_token=\\\"<ENF>\\\",\\n    ):\\n        super(SequenceVocabulary, self).__init__(token_to_idx)\\n        self._mask_token = mask_token\\n        self._unk_token = unk_token\\n        self._begin_seq_token = begin_seq_token\\n        self._end_seq_token = end_seq_token\\n\\n        self.mask_index = self.add_token(self._mask_token)\\n        self.unk_index = self.add_token(self._unk_token)\\n        self.begin_seq_index = self.add_token(self._begin_seq_token)\\n        self.end_seq_index = self.add_token(self._end_seq_token)\\n\\n    def to_serializable(self):\\n        contents = super(SequenceVocabulary, self).to_serializable()\\n        contents.update(\\n            {\\n                \\\"unk_token\\\": self._unk_token,\\n                \\\"mask_token\\\": self._mask_token,\\n                \\\"begin_seq_token\\\": self._begin_seq_token,\\n                \\\"end_seq_token\\\": self._end_seq_token,\\n            }\\n        )\\n        return contents\\n\\n    def lookup_token(self, token):\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_token)\\n        else:\\n            return self._token_to_idx[token]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, toekn_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idk: token for tken, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"The index {index} is not in the Vocab.\")\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_to_idx=None,\n",
    "        unk_token=\"<UNK>\",\n",
    "        mask_token=\"<MASK>\",\n",
    "        begin_seq_token=\"<BEGIN>\",\n",
    "        end_seq_token=\"<ENF>\",\n",
    "    ):\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update(\n",
    "            {\n",
    "                \"unk_token\": self._unk_token,\n",
    "                \"mask_token\": self._mask_token,\n",
    "                \"begin_seq_token\": self._begin_seq_token,\n",
    "                \"end_seq_token\": self._end_seq_token,\n",
    "            }\n",
    "        )\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_token)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c828777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"class SurnameVectorizer(object):\\n    def __init__(self, char_vocab, nationality_vocab):\\n        self.char_vocab = char_vocab\\n        self.nationality_vocab = nationality_vocab\\n        \\n    def vectorize(self, surname, vector_length=-1):\\n        indices = [self.char_vocab.begin_seq_index]\\n        indices.extend(\\n            self.char_vocab.lookup_token(token)\\n            for token in surname\\n        )\\n        indices.append(self.char_vocab.end_seq_index)\\n        if vector_length < 0:\\n            vector_length = len(indices)\\n        out_vector = np.zeros(vector_length, dtype=np.int64)\\n        out_vector[:len(indices)] = indices\\n        out_vector[len(indices):] = self.char_vocab.mask_index\\n        return out_vector, len(indices)\\n    \\n    @classmethod\\n    def from_dataframe(cls, surname_df):\\n        char_vocab = SequenceVocabulary()\\n        nationality_vocab = Vocabulary()\\n        for index_row in surname_df.iterrows():\\n            for char in row.surname:\\n                char_vocab.add_token(char)\\n            nationality_vocab.add_token(row.nationality)\\n        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\\n    \\n    @classmethod\\n    def from_serializable(cls, contents):\\n        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\\n        nat_vocab = Vocabulary.from_serializable(contents['nationality_vocab'])\\n        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\\n\\n    def to_serializable(self):\\n        return {\\n            'char_vocab': self.char_vocab.to_serializable(),\\n            'nationality_vocab': self.nationality_vocab.to_serializable()\\n        }\";\n",
       "                var nbb_formatted_code = \"class SurnameVectorizer(object):\\n    def __init__(self, char_vocab, nationality_vocab):\\n        self.char_vocab = char_vocab\\n        self.nationality_vocab = nationality_vocab\\n\\n    def vectorize(self, surname, vector_length=-1):\\n        indices = [self.char_vocab.begin_seq_index]\\n        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\\n        indices.append(self.char_vocab.end_seq_index)\\n        if vector_length < 0:\\n            vector_length = len(indices)\\n        out_vector = np.zeros(vector_length, dtype=np.int64)\\n        out_vector[: len(indices)] = indices\\n        out_vector[len(indices) :] = self.char_vocab.mask_index\\n        return out_vector, len(indices)\\n\\n    @classmethod\\n    def from_dataframe(cls, surname_df):\\n        char_vocab = SequenceVocabulary()\\n        nationality_vocab = Vocabulary()\\n        for index_row in surname_df.iterrows():\\n            for char in row.surname:\\n                char_vocab.add_token(char)\\n            nationality_vocab.add_token(row.nationality)\\n        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        char_vocab = SequenceVocabulary.from_serializable(contents[\\\"char_vocab\\\"])\\n        nat_vocab = Vocabulary.from_serializable(contents[\\\"nationality_vocab\\\"])\\n        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\\n\\n    def to_serializable(self):\\n        return {\\n            \\\"char_vocab\\\": self.char_vocab.to_serializable(),\\n            \\\"nationality_vocab\\\": self.nationality_vocab.to_serializable(),\\n        }\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    def __init__(self, char_vocab, nationality_vocab):\n",
    "        self.char_vocab = char_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname, vector_length=-1):\n",
    "        indices = [self.char_vocab.begin_seq_index]\n",
    "        indices.extend(self.char_vocab.lookup_token(token) for token in surname)\n",
    "        indices.append(self.char_vocab.end_seq_index)\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[: len(indices)] = indices\n",
    "        out_vector[len(indices) :] = self.char_vocab.mask_index\n",
    "        return out_vector, len(indices)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        char_vocab = SequenceVocabulary()\n",
    "        nationality_vocab = Vocabulary()\n",
    "        for index_row in surname_df.iterrows():\n",
    "            for char in row.surname:\n",
    "                char_vocab.add_token(char)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        char_vocab = SequenceVocabulary.from_serializable(contents[\"char_vocab\"])\n",
    "        nat_vocab = Vocabulary.from_serializable(contents[\"nationality_vocab\"])\n",
    "        return cls(char_vocab=char_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            \"char_vocab\": self.char_vocab.to_serializable(),\n",
    "            \"nationality_vocab\": self.nationality_vocab.to_serializable(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75f0c647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"class SurnameDataset(Dataset):\\n    def __init__(self, surname_df, vectorizer):\\n        self.surname_df = surname_df\\n        self._vectorizer = vectorizer\\n        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\\n        \\n        self.train_df = self.surname_df[\\n            self.surname_df.split == 'train']\\n        self.train_size = len(self.train_df)\\n        \\n        self.val_df = self.surname_df[\\n            self.surname_df.split == 'val']\\n        self.val_size = len(self.val_df)\\n\\n        self.test_df = self.surname_df[\\n            self.surname_df.split == 'test']\\n        self.test_size = len(self.test_df)\\n        \\n        self._lookup_dict = {\\n            'train': (self.train_df, self.train_size),\\n            'val': (self.val_df, self.val_size),\\n            'test': (self.test_df, self.test_size)\\n        }\\n        self.set_split('train')\\n        class_counts = self.train_df.nationality.value_counts().to_dict()\\n        def sort_key(item):\\n            return self._vectorizer.nationality_vocab.lookup_token(item[0])\\n        \\n        sorted_counts = sorted(class_counts.items(), key=sort_key)\\n        frequencies = [\\n            count for _, count in sorted_counts\\n        ]\\n        self.class_weigths = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\\n    \\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, surname_csv):\\n        surname_df = pd.read_csv(surname_csv)\\n        train_surname_df = surname_df[surname_df.split == 'train']\\n        return cls(\\n            surname_df, SurnameVectorizer.from_dataframe(train_surname_df)\\n        )\\n    \\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        with open(vectorizer_filepath) as fp:\\n            return SurnameVectorizer.from_serializable(json.load(fp))\\n        \\n    def save_vectorizer(self, vectorizer_filepath):\\n        with open(vectorizer_filepath, 'w') as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n            \\n    def get_vectorizer(self):\\n        return self._vectorizer\\n    \\n    def set_split(self, split='train'):\\n        self._target_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n        \\n\\n    def __len__(self):\\n        return self._target_size\\n    \\n    def __getitem__(self, index):\\n        row = self._target_df.iloc[index]\\n        surname_vector, vec_length = self._vectorizer.vectorize(\\n            row.surname, self._max_seq_length\\n        )\\n        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\\n            row.nationality)\\n        return {\\n            'x_data': surname_vector,\\n            'y_target': nationality_index,\\n            'x_length': vec_length\\n        }\\n    \\n    def get_num_batches(self, batch_size):\\n        return len(self) // batch_size\\n    \\n            \";\n",
       "                var nbb_formatted_code = \"class SurnameDataset(Dataset):\\n    def __init__(self, surname_df, vectorizer):\\n        self.surname_df = surname_df\\n        self._vectorizer = vectorizer\\n        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\\n\\n        self.train_df = self.surname_df[self.surname_df.split == \\\"train\\\"]\\n        self.train_size = len(self.train_df)\\n\\n        self.val_df = self.surname_df[self.surname_df.split == \\\"val\\\"]\\n        self.val_size = len(self.val_df)\\n\\n        self.test_df = self.surname_df[self.surname_df.split == \\\"test\\\"]\\n        self.test_size = len(self.test_df)\\n\\n        self._lookup_dict = {\\n            \\\"train\\\": (self.train_df, self.train_size),\\n            \\\"val\\\": (self.val_df, self.val_size),\\n            \\\"test\\\": (self.test_df, self.test_size),\\n        }\\n        self.set_split(\\\"train\\\")\\n        class_counts = self.train_df.nationality.value_counts().to_dict()\\n\\n        def sort_key(item):\\n            return self._vectorizer.nationality_vocab.lookup_token(item[0])\\n\\n        sorted_counts = sorted(class_counts.items(), key=sort_key)\\n        frequencies = [count for _, count in sorted_counts]\\n        self.class_weigths = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\\n\\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, surname_csv):\\n        surname_df = pd.read_csv(surname_csv)\\n        train_surname_df = surname_df[surname_df.split == \\\"train\\\"]\\n        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\\n\\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        with open(vectorizer_filepath) as fp:\\n            return SurnameVectorizer.from_serializable(json.load(fp))\\n\\n    def save_vectorizer(self, vectorizer_filepath):\\n        with open(vectorizer_filepath, \\\"w\\\") as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n\\n    def get_vectorizer(self):\\n        return self._vectorizer\\n\\n    def set_split(self, split=\\\"train\\\"):\\n        self._target_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n\\n    def __len__(self):\\n        return self._target_size\\n\\n    def __getitem__(self, index):\\n        row = self._target_df.iloc[index]\\n        surname_vector, vec_length = self._vectorizer.vectorize(\\n            row.surname, self._max_seq_length\\n        )\\n        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\\n            row.nationality\\n        )\\n        return {\\n            \\\"x_data\\\": surname_vector,\\n            \\\"y_target\\\": nationality_index,\\n            \\\"x_length\\\": vec_length,\\n        }\\n\\n    def get_num_batches(self, batch_size):\\n        return len(self) // batch_size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split == \"val\"]\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, self.train_size),\n",
    "            \"val\": (self.val_df, self.val_size),\n",
    "            \"test\": (self.test_df, self.test_size),\n",
    "        }\n",
    "        self.set_split(\"train\")\n",
    "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
    "\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weigths = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == \"train\"]\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        surname_vector, vec_length = self._vectorizer.vectorize(\n",
    "            row.surname, self._max_seq_length\n",
    "        )\n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\n",
    "            row.nationality\n",
    "        )\n",
    "        return {\n",
    "            \"x_data\": surname_vector,\n",
    "            \"y_target\": nationality_index,\n",
    "            \"x_length\": vec_length,\n",
    "        }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d8ae6c",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffeb7f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"def column_gather(y_out, x_lengths):\\n    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\\n    out = []\\n    for batch_index, column_index in enumerate(x_lengths):\\n        out.append(y_out[batch_index, column_index])\\n    return torch.stack(out)\\n\\n\\nclass ElmanRNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, batch_first=False):\\n        super(ElmanRNN, self).__init__()\\n        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\\n        self.batch_first = batch_first\\n        self.hidden_size = hidden_size\\n\\n    def _initial_hidden(self, batch_size):\\n        return torch.zeros((batch_size, self.hidden_size))\\n\\n    def forward(self, x_in, initial_hidden=None):\\n        if self.batch_first:\\n            batch_size, seq_size, feat_size = x_in.size()\\n            x_in = x_in.permute(1, 0, 2)\\n        else:\\n            seq_size, batch_size, feat_size = x_in.size()\\n        hiddens = []\\n        if initial_hidden is None:\\n            initial_hidden = self._initial_hidden(batch_size)\\n            initial_hidden = initial_hidden.to(x_in.device)\\n        hidden_t = initial_hidden\\n        for t in range(seq_size):\\n            hidden_t = self.rnn_cell(x_in[t], hidden_t)\\n            hiddens.append(hidden_t)\\n        hiddens = torch.stack(hiddens)\\n        if self.batch_first:\\n            hiddens = hiddens.permute(1, 0, 2)\\n        return hiddens\\n\\n\\nclass SurnameClassifier(nn.Module):\\n    def __init__(\\n        self,\\n        embedding_size,\\n        num_embeddings,\\n        num_classes,\\n        rnn_hidden_size,\\n        batch_first=True,\\n        padding_idx=0,\\n    ):\\n        super(SurnameClassifier, self).__init__()\\n        self.emb = nn.Embedding(\\n            num_embeddings=num_embeddings,\\n            embedding_im=embedding_size,\\n            padding_idx=padding_idx,\\n        )\\n        self.rnn = ElmanRNN(input_size=embedding_size,\\n                           hidden_size=rnn_hidden_size,\\n                           batch_first=batch_first)\\n        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\\n                            out_features=rnn_hidden_size)\\n        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\\n                            out_features=num_classes)\\n    \\n    def forward(self, x_in, x_lengths=None, apply_softmax=False):\\n        x_embedded = self.emb(x_in)\\n        y_out = self.rnn(x_embedded)\\n        if x_lenghts is not None:\\n            y_out = column_gather(y_out, x_lengths)\\n        else:\\n            y_out = y_out[:, -1, :]\\n        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\\n        y_out = self.fc2(F.dropout(y_out, 0.5))\\n        if apply_softmax:\\n            y_out = F.softmax(y_out, dim=1)\\n        return y_out\";\n",
       "                var nbb_formatted_code = \"def column_gather(y_out, x_lengths):\\n    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\\n    out = []\\n    for batch_index, column_index in enumerate(x_lengths):\\n        out.append(y_out[batch_index, column_index])\\n    return torch.stack(out)\\n\\n\\nclass ElmanRNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, batch_first=False):\\n        super(ElmanRNN, self).__init__()\\n        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\\n        self.batch_first = batch_first\\n        self.hidden_size = hidden_size\\n\\n    def _initial_hidden(self, batch_size):\\n        return torch.zeros((batch_size, self.hidden_size))\\n\\n    def forward(self, x_in, initial_hidden=None):\\n        if self.batch_first:\\n            batch_size, seq_size, feat_size = x_in.size()\\n            x_in = x_in.permute(1, 0, 2)\\n        else:\\n            seq_size, batch_size, feat_size = x_in.size()\\n        hiddens = []\\n        if initial_hidden is None:\\n            initial_hidden = self._initial_hidden(batch_size)\\n            initial_hidden = initial_hidden.to(x_in.device)\\n        hidden_t = initial_hidden\\n        for t in range(seq_size):\\n            hidden_t = self.rnn_cell(x_in[t], hidden_t)\\n            hiddens.append(hidden_t)\\n        hiddens = torch.stack(hiddens)\\n        if self.batch_first:\\n            hiddens = hiddens.permute(1, 0, 2)\\n        return hiddens\\n\\n\\nclass SurnameClassifier(nn.Module):\\n    def __init__(\\n        self,\\n        embedding_size,\\n        num_embeddings,\\n        num_classes,\\n        rnn_hidden_size,\\n        batch_first=True,\\n        padding_idx=0,\\n    ):\\n        super(SurnameClassifier, self).__init__()\\n        self.emb = nn.Embedding(\\n            num_embeddings=num_embeddings,\\n            embedding_im=embedding_size,\\n            padding_idx=padding_idx,\\n        )\\n        self.rnn = ElmanRNN(\\n            input_size=embedding_size,\\n            hidden_size=rnn_hidden_size,\\n            batch_first=batch_first,\\n        )\\n        self.fc1 = nn.Linear(in_features=rnn_hidden_size, out_features=rnn_hidden_size)\\n        self.fc2 = nn.Linear(in_features=rnn_hidden_size, out_features=num_classes)\\n\\n    def forward(self, x_in, x_lengths=None, apply_softmax=False):\\n        x_embedded = self.emb(x_in)\\n        y_out = self.rnn(x_embedded)\\n        if x_lenghts is not None:\\n            y_out = column_gather(y_out, x_lengths)\\n        else:\\n            y_out = y_out[:, -1, :]\\n        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\\n        y_out = self.fc2(F.dropout(y_out, 0.5))\\n        if apply_softmax:\\n            y_out = F.softmax(y_out, dim=1)\\n        return y_out\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def column_gather(y_out, x_lengths):\n",
    "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
    "    out = []\n",
    "    for batch_index, column_index in enumerate(x_lengths):\n",
    "        out.append(y_out[batch_index, column_index])\n",
    "    return torch.stack(out)\n",
    "\n",
    "\n",
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(ElmanRNN, self).__init__()\n",
    "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
    "        self.batch_first = batch_first\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def _initial_hidden(self, batch_size):\n",
    "        return torch.zeros((batch_size, self.hidden_size))\n",
    "\n",
    "    def forward(self, x_in, initial_hidden=None):\n",
    "        if self.batch_first:\n",
    "            batch_size, seq_size, feat_size = x_in.size()\n",
    "            x_in = x_in.permute(1, 0, 2)\n",
    "        else:\n",
    "            seq_size, batch_size, feat_size = x_in.size()\n",
    "        hiddens = []\n",
    "        if initial_hidden is None:\n",
    "            initial_hidden = self._initial_hidden(batch_size)\n",
    "            initial_hidden = initial_hidden.to(x_in.device)\n",
    "        hidden_t = initial_hidden\n",
    "        for t in range(seq_size):\n",
    "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
    "            hiddens.append(hidden_t)\n",
    "        hiddens = torch.stack(hiddens)\n",
    "        if self.batch_first:\n",
    "            hiddens = hiddens.permute(1, 0, 2)\n",
    "        return hiddens\n",
    "\n",
    "\n",
    "class SurnameClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_size,\n",
    "        num_embeddings,\n",
    "        num_classes,\n",
    "        rnn_hidden_size,\n",
    "        batch_first=True,\n",
    "        padding_idx=0,\n",
    "    ):\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.emb = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_im=embedding_size,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.rnn = ElmanRNN(\n",
    "            input_size=embedding_size,\n",
    "            hidden_size=rnn_hidden_size,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "        self.fc1 = nn.Linear(in_features=rnn_hidden_size, out_features=rnn_hidden_size)\n",
    "        self.fc2 = nn.Linear(in_features=rnn_hidden_size, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
    "        x_embedded = self.emb(x_in)\n",
    "        y_out = self.rnn(x_embedded)\n",
    "        if x_lenghts is not None:\n",
    "            y_out = column_gather(y_out, x_lengths)\n",
    "        else:\n",
    "            y_out = y_out[:, -1, :]\n",
    "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
    "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fac370c",
   "metadata": {},
   "source": [
    "### Init and Model Training + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "768a9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 14;\n",
       "                var nbb_unformatted_code = \"\\nargs = Namespace(\\n    # Data and path information\\n    surname_csv=\\\"../data/surnames/surnames_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/chapter06/surname_classification\\\",\\n    # Model hyper parameter\\n    char_embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    num_epochs=100,\\n    learning_rate=1e-3,\\n    batch_size=64,\\n    seed=1337,\\n    early_stopping_criteria=5,\\n    # Runtime hyper parameter\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n    \\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n    \\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir,\\n                                        args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir,\\n                                         args.model_state_file)\\n    \\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and path information\\n    surname_csv=\\\"../data/surnames/surnames_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/chapter06/surname_classification\\\",\\n    # Model hyper parameter\\n    char_embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    num_epochs=100,\\n    learning_rate=1e-3,\\n    batch_size=64,\\n    seed=1337,\\n    early_stopping_criteria=5,\\n    # Runtime hyper parameter\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    surname_csv=\"../data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/chapter06/surname_classification\",\n",
    "    # Model hyper parameter\n",
    "    char_embedding_size=100,\n",
    "    rnn_hidden_size=64,\n",
    "    # Training hyper parameter\n",
    "    num_epochs=100,\n",
    "    learning_rate=1e-3,\n",
    "    batch_size=64,\n",
    "    seed=1337,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime hyper parameter\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6dbe8a82",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'token_to_idx' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7db816af2a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = SurnameDataset.load_dataset_and_make_vectorizer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msurname_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-c788859392a1>\u001b[0m in \u001b[0;36mload_dataset_and_make_vectorizer\u001b[0;34m(cls, surname_csv)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_surname_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msurname_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msurname_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         return cls(\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0msurname_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSurnameVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_surname_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ed07f41ee8f2>\u001b[0m in \u001b[0;36mfrom_dataframe\u001b[0;34m(cls, surname_df)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurname_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mchar_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequenceVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mnationality_vocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_row\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msurname_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ad14811a1871>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, token_to_idx, unk_token, mask_token, begin_seq_token, end_seq_token)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mend_seq_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"<ENF>\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     ):\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceVocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_to_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mask_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unk_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munk_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-ad14811a1871>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, toekn_to_idx)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoekn_to_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtoken_to_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m             \u001b[0mtoken_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_token_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_to_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'token_to_idx' referenced before assignment"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"dataset = SurnameDataset.load_dataset_and_make_vectorizer(\\n    args.surname_csv\\n)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\nclassifier = SurnameClassifier(\\n    embedding_size=args.char_embedding_size,\\n    num_embeddings=len(vectorizer.char_vocab),\\n    num_classes=len(vectorizer.nationality_vocab),\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.char_vocab.mask_index\\n)\\n\\nclassifer = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode='min', factor=0.5, patience=1)\\n\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\nclassifier = SurnameClassifier(\\n    embedding_size=args.char_embedding_size,\\n    num_embeddings=len(vectorizer.char_vocab),\\n    num_classes=len(vectorizer.nationality_vocab),\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.char_vocab.mask_index,\\n)\\n\\nclassifer = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\n\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = SurnameClassifier(\n",
    "    embedding_size=args.char_embedding_size,\n",
    "    num_embeddings=len(vectorizer.char_vocab),\n",
    "    num_classes=len(vectorizer.nationality_vocab),\n",
    "    rnn_hidden_size=args.rnn_hidden_size,\n",
    "    padding_idx=vectorizer.char_vocab.mask_index,\n",
    ")\n",
    "\n",
    "classifer = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603d547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Sequence Modeling for NLP",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "427.715px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
