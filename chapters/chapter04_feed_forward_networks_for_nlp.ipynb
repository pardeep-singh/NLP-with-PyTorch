{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c68a01b9",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965068f",
   "metadata": {},
   "source": [
    "Historic Downfalls of the perceptron was that it cannot learn nontrivial patterns present in data. For example, in XOR situation in which decision boundry cannot be single straight line, Perceptron fails to learn this decision boundry.\n",
    "\n",
    "![Figure 4.1](../images/figure_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1550bba3",
   "metadata": {},
   "source": [
    "**Feed-Forward** network is any neural network in which data flows in one direction(ie from input to output). By definition, perceptron is also a _feed-forward_ modelm but usually the term is reserved for more complicated models with multiple units.\n",
    "\n",
    "Two types of _Feed Forward Neural Networks_:\n",
    "\n",
    "- **Multilayer Perceptron(MLP)**\n",
    "    - MLP structurally extends the simpler perceptron by grouping many perceptrons in a single layer and stacking multiple layers together.\n",
    "- **Convolutional Neural Network(CNN)**\n",
    "    - CNNs are able to learn localized patterns in the inputs using windowing propertry which is inspired by windowed filters in the processing of digital signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b21899b",
   "metadata": {},
   "source": [
    "## The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863f98ad",
   "metadata": {},
   "source": [
    "The Perceptron takes the data vector as input and computes a single output value. In an MLP, many perceptrons are grouped so that output of a single layer is a new vector instead of a single output value. Additionally MLP combines multiple layers with nonlinearity in between each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10fa6f7",
   "metadata": {},
   "source": [
    "The simplest MLP is composed of 3 stages of representation and two linear layers. The first stage is the _input vector_. Given this input vector, the _First Linear Layer_ computes a _hidden vector_ which is the _second stage of representation_. Using the hidden vector, the _Second Linear Layer_ computes an _output vector_.\n",
    "\n",
    "![Figure 4.2](../images/figure_4_2.png)\n",
    "\n",
    "The power of MLPs comes from adding the second Linear Layer and allowing the model to learn an intermediate representation that is _linearly separable_ - a property of representations in which a single straight line can be used to distinguish the data points by whcih side of the line they fall on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458ba6e",
   "metadata": {},
   "source": [
    "### A Simple Example: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e7aa7",
   "metadata": {},
   "source": [
    "In Figure 4.3 We can see that perceptron has difficulty in learning a decision boundry that can separate the stars and circles however the MLP learns a decision boundry that classifies the stars and the circles more accurately.\n",
    "\n",
    "![Figure 4.3](../images/figure_4_3.png)\n",
    "\n",
    "It may appear that MLP has two decision boundries but is just one decision boundry because it has been constructed using the intermediate representation that has morphed the space to allow one hyperplane to appear in bith of these positions. This can be visualised in Figure 4.4 and 4.5\n",
    "\n",
    "![Figure 4.4](../images/figure_4_4.png)\n",
    "\n",
    "![Figure 4.5](../images/figure_4_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168d2d0f",
   "metadata": {},
   "source": [
    "### Implementing MLPs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acac1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multilayer perceptron using PyTorch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): the size of the input vectors\n",
    "            hidden_dim (int): the output size of the first Linear layer\n",
    "            output_dim (int): the output size of the second Linear layer\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        The forward pass of the MLP\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor x_in.shape\n",
    "                should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "        \n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e841d2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size, input_dim, hidden_dim, output_dim = 2, 3, 100, 4\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7dba8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 3])\n",
      "Values: tensor([[0.0073, 0.3744, 0.2184],\n",
      "        [0.1998, 0.2203, 0.0871]])\n",
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 4])\n",
      "Values: tensor([[-0.0095,  0.0164, -0.0362,  0.0886],\n",
      "        [ 0.0059,  0.0262, -0.0443,  0.1194]], grad_fn=<AddmmBackward>)\n",
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 4])\n",
      "Values: tensor([[0.2437, 0.2501, 0.2373, 0.2689],\n",
      "        [0.2444, 0.2494, 0.2324, 0.2738]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def describe(x):\n",
    "    print(f\"Type: {x.type()}\")\n",
    "    print(f\"Shape Size: {x.shape}\")\n",
    "    print(f\"Values: {x}\")\n",
    "\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abfced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
