{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd133f8",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4de56",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Historic Downfalls of the perceptron was that it cannot learn nontrivial patterns present in data. For example, in XOR situation in which decision boundry cannot be single straight line, Perceptron fails to learn this decision boundry.\n",
    "\n",
    "![Figure 4.1](../images/figure_4_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69debc89",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Feed-Forward** network is any neural network in which data flows in one direction(ie from input to output). By definition, perceptron is also a _feed-forward_ modelm but usually the term is reserved for more complicated models with multiple units.\n",
    "\n",
    "Two types of _Feed Forward Neural Networks_:\n",
    "\n",
    "- **Multilayer Perceptron(MLP)**\n",
    "    - MLP structurally extends the simpler perceptron by grouping many perceptrons in a single layer and stacking multiple layers together.\n",
    "- **Convolutional Neural Network(CNN)**\n",
    "    - CNNs are able to learn localized patterns in the inputs using windowing propertry which is inspired by windowed filters in the processing of digital signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef78912",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1396c29",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Perceptron takes the data vector as input and computes a single output value. In an MLP, many perceptrons are grouped so that output of a single layer is a new vector instead of a single output value. Additionally MLP combines multiple layers with nonlinearity in between each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c6bee7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The simplest MLP is composed of 3 stages of representation and two linear layers. The first stage is the _input vector_. Given this input vector, the _First Linear Layer_ computes a _hidden vector_ which is the _second stage of representation_. Using the hidden vector, the _Second Linear Layer_ computes an _output vector_.\n",
    "\n",
    "![Figure 4.2](../images/figure_4_2.png)\n",
    "\n",
    "The power of MLPs comes from adding the second Linear Layer and allowing the model to learn an intermediate representation that is _linearly separable_ - a property of representations in which a single straight line can be used to distinguish the data points by whcih side of the line they fall on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27327d30",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### A Simple Example: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865e3e1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Figure 4.3 We can see that perceptron has difficulty in learning a decision boundry that can separate the stars and circles however the MLP learns a decision boundry that classifies the stars and the circles more accurately.\n",
    "\n",
    "![Figure 4.3](../images/figure_4_3.png)\n",
    "\n",
    "It may appear that MLP has two decision boundries but is just one decision boundry because it has been constructed using the intermediate representation that has morphed the space to allow one hyperplane to appear in bith of these positions. This can be visualised in Figure 4.4 and 4.5\n",
    "\n",
    "![Figure 4.4](../images/figure_4_4.png)\n",
    "\n",
    "![Figure 4.5](../images/figure_4_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a996e8d4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Implementing MLPs in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a2ff8ca8",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 86;\n",
       "                var nbb_unformatted_code = \"# Multilayer perceptron using PyTorch\\n%load_ext nb_black\\n\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\nclass MultilayerPerceptron(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        \\\"\\\"\\\"\\n        Args:\\n            input_dim (int): the size of the input vectors\\n            hidden_dim (int): the output size of the first Linear layer\\n            output_dim (int): the output size of the second Linear layer\\n        \\\"\\\"\\\"\\n        super(MultilayerPerceptron, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x_in, apply_softmax=False):\\n        \\\"\\\"\\\"\\n        The forward pass of the MLP\\n\\n        Args:\\n            x_in (torch.Tensor): an input data tensor x_in.shape\\n                should be (batch, input_dim)\\n            apply_softmax (bool): a flag for the softmax activation\\n                should be false if used with the cross-entropy losses\\n        Returns:\\n            the resulting tensor. tensor.shape should be (batch, output_dim)\\n        \\\"\\\"\\\"\\n        intermediate = F.relu(self.fc1(x_in))\\n        output = self.fc2(intermediate)\\n\\n        if apply_softmax:\\n            output = F.softmax(output, dim=1)\\n        return output\";\n",
       "                var nbb_formatted_code = \"# Multilayer perceptron using PyTorch\\n%load_ext nb_black\\n\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\n\\nclass MultilayerPerceptron(nn.Module):\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        \\\"\\\"\\\"\\n        Args:\\n            input_dim (int): the size of the input vectors\\n            hidden_dim (int): the output size of the first Linear layer\\n            output_dim (int): the output size of the second Linear layer\\n        \\\"\\\"\\\"\\n        super(MultilayerPerceptron, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x_in, apply_softmax=False):\\n        \\\"\\\"\\\"\\n        The forward pass of the MLP\\n\\n        Args:\\n            x_in (torch.Tensor): an input data tensor x_in.shape\\n                should be (batch, input_dim)\\n            apply_softmax (bool): a flag for the softmax activation\\n                should be false if used with the cross-entropy losses\\n        Returns:\\n            the resulting tensor. tensor.shape should be (batch, output_dim)\\n        \\\"\\\"\\\"\\n        intermediate = F.relu(self.fc1(x_in))\\n        output = self.fc2(intermediate)\\n\\n        if apply_softmax:\\n            output = F.softmax(output, dim=1)\\n        return output\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Multilayer perceptron using PyTorch\n",
    "%load_ext nb_black\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): the size of the input vectors\n",
    "            hidden_dim (int): the output size of the first Linear layer\n",
    "            output_dim (int): the output size of the second Linear layer\n",
    "        \"\"\"\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        The forward pass of the MLP\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor x_in.shape\n",
    "                should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the cross-entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate = F.relu(self.fc1(x_in))\n",
    "        output = self.fc2(intermediate)\n",
    "\n",
    "        if apply_softmax:\n",
    "            output = F.softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cdb41219",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=3, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 87;\n",
       "                var nbb_unformatted_code = \"batch_size, input_dim, hidden_dim, output_dim = 2, 3, 100, 4\\n\\nmlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\\nprint(mlp)\";\n",
       "                var nbb_formatted_code = \"batch_size, input_dim, hidden_dim, output_dim = 2, 3, 100, 4\\n\\nmlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\\nprint(mlp)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size, input_dim, hidden_dim, output_dim = 2, 3, 100, 4\n",
    "\n",
    "mlp = MultilayerPerceptron(input_dim, hidden_dim, output_dim)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "792739d5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 3])\n",
      "Values: tensor([[0.7319, 0.7877, 0.7464],\n",
      "        [0.0557, 0.9196, 0.7085]])\n",
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 4])\n",
      "Values: tensor([[-0.0456, -0.4641,  0.2286, -0.1316],\n",
      "        [-0.1150, -0.3650,  0.3608,  0.2050]], grad_fn=<AddmmBackward>)\n",
      "Type: torch.FloatTensor\n",
      "Shape Size: torch.Size([2, 4])\n",
      "Values: tensor([[0.2570, 0.1691, 0.3381, 0.2358],\n",
      "        [0.2098, 0.1634, 0.3377, 0.2890]], grad_fn=<SoftmaxBackward>)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 88;\n",
       "                var nbb_unformatted_code = \"import torch\\n\\n\\ndef describe(x):\\n    print(f\\\"Type: {x.type()}\\\")\\n    print(f\\\"Shape Size: {x.shape}\\\")\\n    print(f\\\"Values: {x}\\\")\\n\\n\\nx_input = torch.rand(batch_size, input_dim)\\ndescribe(x_input)\\n\\ny_output = mlp(x_input, apply_softmax=False)\\ndescribe(y_output)\\n\\ny_output = mlp(x_input, apply_softmax=True)\\ndescribe(y_output)\";\n",
       "                var nbb_formatted_code = \"import torch\\n\\n\\ndef describe(x):\\n    print(f\\\"Type: {x.type()}\\\")\\n    print(f\\\"Shape Size: {x.shape}\\\")\\n    print(f\\\"Values: {x}\\\")\\n\\n\\nx_input = torch.rand(batch_size, input_dim)\\ndescribe(x_input)\\n\\ny_output = mlp(x_input, apply_softmax=False)\\ndescribe(y_output)\\n\\ny_output = mlp(x_input, apply_softmax=True)\\ndescribe(y_output)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def describe(x):\n",
    "    print(f\"Type: {x.type()}\")\n",
    "    print(f\"Shape Size: {x.shape}\")\n",
    "    print(f\"Values: {x}\")\n",
    "\n",
    "\n",
    "x_input = torch.rand(batch_size, input_dim)\n",
    "describe(x_input)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=False)\n",
    "describe(y_output)\n",
    "\n",
    "y_output = mlp(x_input, apply_softmax=True)\n",
    "describe(y_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70ea12",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Example: Surname classification with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8ea97e47",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 89;\n",
       "                var nbb_unformatted_code = \"from argparse import Namespace\\nfrom collections import Counter\\nimport json\\nimport os\\nimport string\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm.notebook import tqdm\";\n",
       "                var nbb_formatted_code = \"from argparse import Namespace\\nfrom collections import Counter\\nimport json\\nimport os\\nimport string\\n\\nimport numpy as np\\nimport pandas as pd\\n\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\nimport torch.optim as optim\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm.notebook import tqdm\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb29f731",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ab7c228",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 90;\n",
       "                var nbb_unformatted_code = \"class Vocabulary(object):\\n    \\\"\\\"\\\"\\n    Class to process text and extract vocabulary for mapping.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\\\"<UNK>\\\"):\\n        \\\"\\\"\\\"\\n        Args:\\n            token_to_idx (dict): a pre-existing map of tokens to indices\\n            add_unk (bool): a flag that indicates whether to add the UNK token\\n            unk_token (str): the UNK token to add into the Vocabulary\\n        \\\"\\\"\\\"\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\\n        self._add_unk = add_unk\\n        self._unk_token = unk_token\\n        self.unk_index = -1\\n        if add_unk:\\n            self.unk_index = self.add_token(unk_token)\\n\\n    def to_serializable(self):\\n        \\\"\\\"\\\"\\n        Returns a dictionary that can be serialized.\\n        \\\"\\\"\\\"\\n        return {\\n            \\\"token_to_idx\\\": self._token_to_idx,\\n            \\\"add_unk\\\": self._add_unk,\\n            \\\"unk_token\\\": self._unk_token,\\n        }\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        \\\"\\\"\\\"\\n        Instantiates the Vocabulary from a serialized dictionary.\\n        \\\"\\\"\\\"\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        \\\"\\\"\\\"\\n        Update mapping dicts based on the token.\\n\\n        Args:\\n            token (str): the item to add into the Vocabulary.\\n        Returns:\\n            index (int): the integer corresponding to the token.\\n        \\\"\\\"\\\"\\n        try:\\n            index = self._token_to_idx[token]\\n        except KeyError:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        \\\"\\\"\\\"\\n        Add a list of tokens into the Vocabulary\\n\\n        Args:\\n            tokens (list): a list of string tokens\\n        Returns:\\n            indices (list): a list of indices corresponding to the tokens.\\n        \\\"\\\"\\\"\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        \\\"\\\"\\\"\\n        Retrieve the index for given token. Returns UNK index\\n        if token is not found.\\n\\n        Args:\\n            token (str): the token to look up.\\n        Returns:\\n            index (int): the index corresponding to the token\\n        \\\"\\\"\\\"\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_index)\\n        else:\\n            return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        \\\"\\\"\\\"\\n        Return the token associated with the index\\n\\n        Args:\\n            index (int): the index to lookup\\n        Returns:\\n            token (str): the token corresponding to the index\\n        Raises:\\n            KeyError: if the index is not in the Vocabulary\\n        \\\"\\\"\\\"\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"the index {index} is not in the Vocabulary\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\";\n",
       "                var nbb_formatted_code = \"class Vocabulary(object):\\n    \\\"\\\"\\\"\\n    Class to process text and extract vocabulary for mapping.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\\\"<UNK>\\\"):\\n        \\\"\\\"\\\"\\n        Args:\\n            token_to_idx (dict): a pre-existing map of tokens to indices\\n            add_unk (bool): a flag that indicates whether to add the UNK token\\n            unk_token (str): the UNK token to add into the Vocabulary\\n        \\\"\\\"\\\"\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\\n        self._add_unk = add_unk\\n        self._unk_token = unk_token\\n        self.unk_index = -1\\n        if add_unk:\\n            self.unk_index = self.add_token(unk_token)\\n\\n    def to_serializable(self):\\n        \\\"\\\"\\\"\\n        Returns a dictionary that can be serialized.\\n        \\\"\\\"\\\"\\n        return {\\n            \\\"token_to_idx\\\": self._token_to_idx,\\n            \\\"add_unk\\\": self._add_unk,\\n            \\\"unk_token\\\": self._unk_token,\\n        }\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        \\\"\\\"\\\"\\n        Instantiates the Vocabulary from a serialized dictionary.\\n        \\\"\\\"\\\"\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        \\\"\\\"\\\"\\n        Update mapping dicts based on the token.\\n\\n        Args:\\n            token (str): the item to add into the Vocabulary.\\n        Returns:\\n            index (int): the integer corresponding to the token.\\n        \\\"\\\"\\\"\\n        try:\\n            index = self._token_to_idx[token]\\n        except KeyError:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        \\\"\\\"\\\"\\n        Add a list of tokens into the Vocabulary\\n\\n        Args:\\n            tokens (list): a list of string tokens\\n        Returns:\\n            indices (list): a list of indices corresponding to the tokens.\\n        \\\"\\\"\\\"\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        \\\"\\\"\\\"\\n        Retrieve the index for given token. Returns UNK index\\n        if token is not found.\\n\\n        Args:\\n            token (str): the token to look up.\\n        Returns:\\n            index (int): the index corresponding to the token\\n        \\\"\\\"\\\"\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_index)\\n        else:\\n            return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        \\\"\\\"\\\"\\n        Return the token associated with the index\\n\\n        Args:\\n            index (int): the index to lookup\\n        Returns:\\n            token (str): the token corresponding to the index\\n        Raises:\\n            KeyError: if the index is not in the Vocabulary\\n        \\\"\\\"\\\"\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"the index {index} is not in the Vocabulary\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\"\n",
    "    Class to process text and extract vocabulary for mapping.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "        \"\"\"\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idx: token for token, idx in self._token_to_idx.items()}\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary that can be serialized.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"token_to_idx\": self._token_to_idx,\n",
    "            \"add_unk\": self._add_unk,\n",
    "            \"unk_token\": self._unk_token,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\"\n",
    "        Instantiates the Vocabulary from a serialized dictionary.\n",
    "        \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"\n",
    "        Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary.\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            index = self._token_to_idx[token]\n",
    "        except KeyError:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"\n",
    "        Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens.\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"\n",
    "        Retrieve the index for given token. Returns UNK index\n",
    "        if token is not found.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up.\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"\n",
    "        Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to lookup\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"the index {index} is not in the Vocabulary\")\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1b82ed",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5370628e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 91;\n",
       "                var nbb_unformatted_code = \"class SurnameVectorizer(object):\\n    \\\"\\\"\\\"\\n    The Vectorizer which coordinates the Vocabularies and puts them to use.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, surname_vocab, nationality_vocab):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname_vocab (Vocabulary): maps characters to integers\\n            nationality_vocab (Vocabulary): maps nationalities to integers\\n        \\\"\\\"\\\"\\n        self.surname_vocab = surname_vocab\\n        self.nationality_vocab = nationality_vocab\\n\\n    def vectorize(self, surname):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname (str): the surname\\n        Returns:\\n            one_hot (np.ndarray): a collapsed one-hot encoding\\n        \\\"\\\"\\\"\\n        vocab = self.surname_vocab\\n        one_hot = np.zeros(len(vocab), dtype=np.float32)\\n        for token in surname:\\n            one_hot[vocab.lookup_token(token)] = 1\\n        return one_hot\\n\\n    @classmethod\\n    def from_dataframe(cls, surname_df):\\n        \\\"\\\"\\\"\\n        Instantiate the vectorizer from the dataset dataframe.\\n\\n        Args:\\n            surname_df (pandas.DataFrame): the surnames dataset\\n        Returns:\\n            an instance of the SurnameVectorizer\\n        \\\"\\\"\\\"\\n        surname_vocab = Vocabulary(unk_token=\\\"@\\\")\\n        nationality_vocab = Vocabulary(add_unk=False)\\n\\n        for index, row in surname_df.iterrows():\\n            for letter in row.surname:\\n                surname_vocab.add_token(letter)\\n            nationality_vocab.add_token(row.nationality)\\n\\n        return cls(surname_vocab, nationality_vocab)\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        surname_vocab = Vocabulary.from_serializable(contents[\\\"surname_vocab\\\"])\\n        nationality_vocab = Vocabulary.from_serializable(contents[\\\"nationality_vocab\\\"])\\n        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\\n\\n    def to_serializable(self):\\n        return {\\n            \\\"surname_vocab\\\": self.surname_vocab.to_serializable(),\\n            \\\"nationality_vocab\\\": self.nationality_vocab.to_serializable(),\\n        }\";\n",
       "                var nbb_formatted_code = \"class SurnameVectorizer(object):\\n    \\\"\\\"\\\"\\n    The Vectorizer which coordinates the Vocabularies and puts them to use.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, surname_vocab, nationality_vocab):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname_vocab (Vocabulary): maps characters to integers\\n            nationality_vocab (Vocabulary): maps nationalities to integers\\n        \\\"\\\"\\\"\\n        self.surname_vocab = surname_vocab\\n        self.nationality_vocab = nationality_vocab\\n\\n    def vectorize(self, surname):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname (str): the surname\\n        Returns:\\n            one_hot (np.ndarray): a collapsed one-hot encoding\\n        \\\"\\\"\\\"\\n        vocab = self.surname_vocab\\n        one_hot = np.zeros(len(vocab), dtype=np.float32)\\n        for token in surname:\\n            one_hot[vocab.lookup_token(token)] = 1\\n        return one_hot\\n\\n    @classmethod\\n    def from_dataframe(cls, surname_df):\\n        \\\"\\\"\\\"\\n        Instantiate the vectorizer from the dataset dataframe.\\n\\n        Args:\\n            surname_df (pandas.DataFrame): the surnames dataset\\n        Returns:\\n            an instance of the SurnameVectorizer\\n        \\\"\\\"\\\"\\n        surname_vocab = Vocabulary(unk_token=\\\"@\\\")\\n        nationality_vocab = Vocabulary(add_unk=False)\\n\\n        for index, row in surname_df.iterrows():\\n            for letter in row.surname:\\n                surname_vocab.add_token(letter)\\n            nationality_vocab.add_token(row.nationality)\\n\\n        return cls(surname_vocab, nationality_vocab)\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        surname_vocab = Vocabulary.from_serializable(contents[\\\"surname_vocab\\\"])\\n        nationality_vocab = Vocabulary.from_serializable(contents[\\\"nationality_vocab\\\"])\\n        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\\n\\n    def to_serializable(self):\\n        return {\\n            \\\"surname_vocab\\\": self.surname_vocab.to_serializable(),\\n            \\\"nationality_vocab\\\": self.nationality_vocab.to_serializable(),\\n        }\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SurnameVectorizer(object):\n",
    "    \"\"\"\n",
    "    The Vectorizer which coordinates the Vocabularies and puts them to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, surname_vocab, nationality_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_vocab (Vocabulary): maps characters to integers\n",
    "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
    "        \"\"\"\n",
    "        self.surname_vocab = surname_vocab\n",
    "        self.nationality_vocab = nationality_vocab\n",
    "\n",
    "    def vectorize(self, surname):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname (str): the surname\n",
    "        Returns:\n",
    "            one_hot (np.ndarray): a collapsed one-hot encoding\n",
    "        \"\"\"\n",
    "        vocab = self.surname_vocab\n",
    "        one_hot = np.zeros(len(vocab), dtype=np.float32)\n",
    "        for token in surname:\n",
    "            one_hot[vocab.lookup_token(token)] = 1\n",
    "        return one_hot\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, surname_df):\n",
    "        \"\"\"\n",
    "        Instantiate the vectorizer from the dataset dataframe.\n",
    "\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the surnames dataset\n",
    "        Returns:\n",
    "            an instance of the SurnameVectorizer\n",
    "        \"\"\"\n",
    "        surname_vocab = Vocabulary(unk_token=\"@\")\n",
    "        nationality_vocab = Vocabulary(add_unk=False)\n",
    "\n",
    "        for index, row in surname_df.iterrows():\n",
    "            for letter in row.surname:\n",
    "                surname_vocab.add_token(letter)\n",
    "            nationality_vocab.add_token(row.nationality)\n",
    "\n",
    "        return cls(surname_vocab, nationality_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        surname_vocab = Vocabulary.from_serializable(contents[\"surname_vocab\"])\n",
    "        nationality_vocab = Vocabulary.from_serializable(contents[\"nationality_vocab\"])\n",
    "        return cls(surname_vocab=surname_vocab, nationality_vocab=nationality_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            \"surname_vocab\": self.surname_vocab.to_serializable(),\n",
    "            \"nationality_vocab\": self.nationality_vocab.to_serializable(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad6169",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c7bb2288",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 92;\n",
       "                var nbb_unformatted_code = \"class SurnameDataset(Dataset):\\n    def __init__(self, surname_df, vectorizer):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname_df (pandas.DataFrame): the dataset.\\n            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset.\\n        \\\"\\\"\\\"\\n        self.surname_df = surname_df\\n        self._vectorizer = vectorizer\\n        self.train_df = self.surname_df[self.surname_df.split == \\\"train\\\"]\\n        self.train_size = len(self.train_df)\\n        self.val_df = self.surname_df[self.surname_df.split == \\\"val\\\"]\\n        self.validation_size = len(self.val_df)\\n        self.test_df = self.surname_df[self.surname_df.split == \\\"test\\\"]\\n        self.test_size = len(self.test_df)\\n\\n        self._lookup_dict = {\\n            \\\"train\\\": (self.train_df, self.train_size),\\n            \\\"val\\\": (self.val_df, self.validation_size),\\n            \\\"test\\\": (self.test_df, self.test_size),\\n        }\\n        self.set_split(\\\"train\\\")\\n        class_counts = surname_df.nationality.value_counts().to_dict()\\n\\n        def sort_key(item):\\n            return self._vectorizer.nationality_vocab.lookup_token(item[0])\\n\\n        sorted_counts = sorted(class_counts.items(), key=sort_key)\\n        frequencies = [count for _, count in sorted_counts]\\n        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\\n\\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, surname_csv):\\n        \\\"\\\"\\\"\\n        Load dataset and make a new vectorizer from scratch\\n\\n        Args:\\n            surname_csv (str): location of the dataset\\n        Returns:\\n            an instance of SurnameDataset\\n        \\\"\\\"\\\"\\n        surname_df = pd.read_csv(surname_csv)\\n        train_surname_df = surname_df[surname_df.split == \\\"train\\\"]\\n        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\\n\\n    @classmethod\\n    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_file_path):\\n        \\\"\\\"\\\"\\n        Load dataset and the corresponding vectorizer. Used when vectorizer\\n        has been cached for re-use.\\n\\n        Args:\\n            surname_csv (str)\\\" location of the dataset\\\"\\n            vectorizer_filepath (str): location of the saved vectorizer\\n        Returns:\\n            an instance of SurnameDataset\\n        \\\"\\\"\\\"\\n        surname_df = pd.read_csv(surname_csv)\\n        vectorizer = cls.load_vectorizer_only(vectorizer_file_path)\\n        return cls(surname_df, vectorizer)\\n\\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        \\\"\\\"\\\"\\n        A static method for loading the vectorizer from file\\n\\n        Args:\\n            vectorizer_filepath (str): the location of the serialized vectorizer\\n        Returns:\\n            an instance of SurnameVectorizer\\n        \\\"\\\"\\\"\\n        with open(vectorizer_filepath) as fp:\\n            return SurnameVectorizer.from_serializable(json.load(fp))\\n\\n    def save_vectorizer(self, vectorizer_filepath):\\n        \\\"\\\"\\\"\\n        Saves the vectorizer to disk using JSON\\n\\n        Args:\\n            vectorizer_filepath (str): the location to save the vectorizer\\n        \\\"\\\"\\\"\\n        with open(vectorizer_filepath, \\\"w\\\") as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n\\n    def get_vectorizer(self):\\n        \\\"\\\"\\\"\\n        Returns the vectorizer\\n        \\\"\\\"\\\"\\n        return self._vectorizer\\n\\n    def set_split(self, split=\\\"train\\\"):\\n        \\\"\\\"\\\"\\n        Selects the splits in the datatset using a column in the dataframe.\\n        \\\"\\\"\\\"\\n        self._target_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n\\n    def __len__(self):\\n        return self._target_size\\n\\n    def __getitem__(self, index):\\n        \\\"\\\"\\\"\\n        The primary entry point method for PyTorch datasets\\n\\n        Args:\\n            index (int): the index to the data point\\n        Returns:\\n            a dictionary holding the data poiint's:\\n                feature (x_surname)\\n                label (y_nationality)\\n        \\\"\\\"\\\"\\n        row = self._target_df.iloc[index]\\n        surname_vector = self._vectorizer.vectorize(row.surname)\\n        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\\n            row.nationality\\n        )\\n        return {\\\"x_surname\\\": surname_vector, \\\"y_nationality\\\": nationality_index}\\n\\n    def get_num_batches(self, batch_size):\\n        \\\"\\\"\\\"\\n        Given a batch size, return the number of batches in the dataset.\\n\\n        Args:\\n            batch_size (int)\\n        Returns:\\n            number of batches in the dataset\\n        \\\"\\\"\\\"\\n\\n\\ndef generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\\\"cpu\\\"):\\n    \\\"\\\"\\\"\\n    A generator function which wraps the PyTorch DataLoader.\\n    It will ensure each tensor is on the write device location.\\n    \\\"\\\"\\\"\\n    dataloader = DataLoader(\\n        dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\\n    )\\n    for data_dict in dataloader:\\n        out_data_dict = {}\\n        for name, tensor in data_dict.items():\\n            out_data_dict[name] = data_dict[name].to(device)\\n        yield out_data_dict\";\n",
       "                var nbb_formatted_code = \"class SurnameDataset(Dataset):\\n    def __init__(self, surname_df, vectorizer):\\n        \\\"\\\"\\\"\\n        Args:\\n            surname_df (pandas.DataFrame): the dataset.\\n            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset.\\n        \\\"\\\"\\\"\\n        self.surname_df = surname_df\\n        self._vectorizer = vectorizer\\n        self.train_df = self.surname_df[self.surname_df.split == \\\"train\\\"]\\n        self.train_size = len(self.train_df)\\n        self.val_df = self.surname_df[self.surname_df.split == \\\"val\\\"]\\n        self.validation_size = len(self.val_df)\\n        self.test_df = self.surname_df[self.surname_df.split == \\\"test\\\"]\\n        self.test_size = len(self.test_df)\\n\\n        self._lookup_dict = {\\n            \\\"train\\\": (self.train_df, self.train_size),\\n            \\\"val\\\": (self.val_df, self.validation_size),\\n            \\\"test\\\": (self.test_df, self.test_size),\\n        }\\n        self.set_split(\\\"train\\\")\\n        class_counts = surname_df.nationality.value_counts().to_dict()\\n\\n        def sort_key(item):\\n            return self._vectorizer.nationality_vocab.lookup_token(item[0])\\n\\n        sorted_counts = sorted(class_counts.items(), key=sort_key)\\n        frequencies = [count for _, count in sorted_counts]\\n        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\\n\\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, surname_csv):\\n        \\\"\\\"\\\"\\n        Load dataset and make a new vectorizer from scratch\\n\\n        Args:\\n            surname_csv (str): location of the dataset\\n        Returns:\\n            an instance of SurnameDataset\\n        \\\"\\\"\\\"\\n        surname_df = pd.read_csv(surname_csv)\\n        train_surname_df = surname_df[surname_df.split == \\\"train\\\"]\\n        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\\n\\n    @classmethod\\n    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_file_path):\\n        \\\"\\\"\\\"\\n        Load dataset and the corresponding vectorizer. Used when vectorizer\\n        has been cached for re-use.\\n\\n        Args:\\n            surname_csv (str)\\\" location of the dataset\\\"\\n            vectorizer_filepath (str): location of the saved vectorizer\\n        Returns:\\n            an instance of SurnameDataset\\n        \\\"\\\"\\\"\\n        surname_df = pd.read_csv(surname_csv)\\n        vectorizer = cls.load_vectorizer_only(vectorizer_file_path)\\n        return cls(surname_df, vectorizer)\\n\\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        \\\"\\\"\\\"\\n        A static method for loading the vectorizer from file\\n\\n        Args:\\n            vectorizer_filepath (str): the location of the serialized vectorizer\\n        Returns:\\n            an instance of SurnameVectorizer\\n        \\\"\\\"\\\"\\n        with open(vectorizer_filepath) as fp:\\n            return SurnameVectorizer.from_serializable(json.load(fp))\\n\\n    def save_vectorizer(self, vectorizer_filepath):\\n        \\\"\\\"\\\"\\n        Saves the vectorizer to disk using JSON\\n\\n        Args:\\n            vectorizer_filepath (str): the location to save the vectorizer\\n        \\\"\\\"\\\"\\n        with open(vectorizer_filepath, \\\"w\\\") as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n\\n    def get_vectorizer(self):\\n        \\\"\\\"\\\"\\n        Returns the vectorizer\\n        \\\"\\\"\\\"\\n        return self._vectorizer\\n\\n    def set_split(self, split=\\\"train\\\"):\\n        \\\"\\\"\\\"\\n        Selects the splits in the datatset using a column in the dataframe.\\n        \\\"\\\"\\\"\\n        self._target_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n\\n    def __len__(self):\\n        return self._target_size\\n\\n    def __getitem__(self, index):\\n        \\\"\\\"\\\"\\n        The primary entry point method for PyTorch datasets\\n\\n        Args:\\n            index (int): the index to the data point\\n        Returns:\\n            a dictionary holding the data poiint's:\\n                feature (x_surname)\\n                label (y_nationality)\\n        \\\"\\\"\\\"\\n        row = self._target_df.iloc[index]\\n        surname_vector = self._vectorizer.vectorize(row.surname)\\n        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\\n            row.nationality\\n        )\\n        return {\\\"x_surname\\\": surname_vector, \\\"y_nationality\\\": nationality_index}\\n\\n    def get_num_batches(self, batch_size):\\n        \\\"\\\"\\\"\\n        Given a batch size, return the number of batches in the dataset.\\n\\n        Args:\\n            batch_size (int)\\n        Returns:\\n            number of batches in the dataset\\n        \\\"\\\"\\\"\\n\\n\\ndef generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\\\"cpu\\\"):\\n    \\\"\\\"\\\"\\n    A generator function which wraps the PyTorch DataLoader.\\n    It will ensure each tensor is on the write device location.\\n    \\\"\\\"\\\"\\n    dataloader = DataLoader(\\n        dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\\n    )\\n    for data_dict in dataloader:\\n        out_data_dict = {}\\n        for name, tensor in data_dict.items():\\n            out_data_dict[name] = data_dict[name].to(device)\\n        yield out_data_dict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SurnameDataset(Dataset):\n",
    "    def __init__(self, surname_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            surname_df (pandas.DataFrame): the dataset.\n",
    "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset.\n",
    "        \"\"\"\n",
    "        self.surname_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "        self.train_df = self.surname_df[self.surname_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "        self.val_df = self.surname_df[self.surname_df.split == \"val\"]\n",
    "        self.validation_size = len(self.val_df)\n",
    "        self.test_df = self.surname_df[self.surname_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, self.train_size),\n",
    "            \"val\": (self.val_df, self.validation_size),\n",
    "            \"test\": (self.test_df, self.test_size),\n",
    "        }\n",
    "        self.set_split(\"train\")\n",
    "        class_counts = surname_df.nationality.value_counts().to_dict()\n",
    "\n",
    "        def sort_key(item):\n",
    "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
    "\n",
    "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
    "        frequencies = [count for _, count in sorted_counts]\n",
    "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
    "        \"\"\"\n",
    "        Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        train_surname_df = surname_df[surname_df.split == \"train\"]\n",
    "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_file_path):\n",
    "        \"\"\"\n",
    "        Load dataset and the corresponding vectorizer. Used when vectorizer\n",
    "        has been cached for re-use.\n",
    "\n",
    "        Args:\n",
    "            surname_csv (str)\" location of the dataset\"\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameDataset\n",
    "        \"\"\"\n",
    "        surname_df = pd.read_csv(surname_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_file_path)\n",
    "        return cls(surname_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        A static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of SurnameVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"\n",
    "        Saves the vectorizer to disk using JSON\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\"\n",
    "        Returns the vectorizer\n",
    "        \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Selects the splits in the datatset using a column in the dataframe.\n",
    "        \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data poiint's:\n",
    "                feature (x_surname)\n",
    "                label (y_nationality)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "        surname_vector = self._vectorizer.vectorize(row.surname)\n",
    "        nationality_index = self._vectorizer.nationality_vocab.lookup_token(\n",
    "            row.nationality\n",
    "        )\n",
    "        return {\"x_surname\": surname_vector, \"y_nationality\": nationality_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Given a batch size, return the number of batches in the dataset.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader.\n",
    "    It will ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a28e3b1",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### The Model: SurnameClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "feca2d97",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 93;\n",
       "                var nbb_unformatted_code = \"class SurnameClassifier(nn.Module):\\n    \\\"\\\"\\\"\\n    A 2-layer Multilayer Perceptron for classifying surnames.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        \\\"\\\"\\\"\\n        Args:\\n            input_dim (int): the size of the input vectors\\n            hidden_dim (int): the output size of the first Linear Layer\\n            output_dim (int): the output size of the second Linear Layer\\n        \\\"\\\"\\\"\\n        super(SurnameClassifier, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x_in, apply_softmax=False):\\n        \\\"\\\"\\\"\\n        The forward pass of the classifier\\n\\n        Args:\\n            x_in (torch.Tensor): an input data tensor.\\n                X_in.shape should be (batch, input_dim)\\n            apply_softmax (bool): a flag for the softmax activation\\n                should be false if used with the Cross Entropy losses\\n        Returns:\\n            the resulting tensor. tensor.shape should be (batch, output_dim)\\n        \\\"\\\"\\\"\\n        intermediate_vector = F.relu(self.fc1(x_in))\\n        prediction_vector = self.fc2(intermediate_vector)\\n\\n        if apply_softmax:\\n            prediction_vector = F.softmax(prediction_vector, dim=1)\\n        return prediction_vector\";\n",
       "                var nbb_formatted_code = \"class SurnameClassifier(nn.Module):\\n    \\\"\\\"\\\"\\n    A 2-layer Multilayer Perceptron for classifying surnames.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, input_dim, hidden_dim, output_dim):\\n        \\\"\\\"\\\"\\n        Args:\\n            input_dim (int): the size of the input vectors\\n            hidden_dim (int): the output size of the first Linear Layer\\n            output_dim (int): the output size of the second Linear Layer\\n        \\\"\\\"\\\"\\n        super(SurnameClassifier, self).__init__()\\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\\n\\n    def forward(self, x_in, apply_softmax=False):\\n        \\\"\\\"\\\"\\n        The forward pass of the classifier\\n\\n        Args:\\n            x_in (torch.Tensor): an input data tensor.\\n                X_in.shape should be (batch, input_dim)\\n            apply_softmax (bool): a flag for the softmax activation\\n                should be false if used with the Cross Entropy losses\\n        Returns:\\n            the resulting tensor. tensor.shape should be (batch, output_dim)\\n        \\\"\\\"\\\"\\n        intermediate_vector = F.relu(self.fc1(x_in))\\n        prediction_vector = self.fc2(intermediate_vector)\\n\\n        if apply_softmax:\\n            prediction_vector = F.softmax(prediction_vector, dim=1)\\n        return prediction_vector\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SurnameClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A 2-layer Multilayer Perceptron for classifying surnames.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): the size of the input vectors\n",
    "            hidden_dim (int): the output size of the first Linear Layer\n",
    "            output_dim (int): the output size of the second Linear Layer\n",
    "        \"\"\"\n",
    "        super(SurnameClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"\n",
    "        The forward pass of the classifier\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                X_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        intermediate_vector = F.relu(self.fc1(x_in))\n",
    "        prediction_vector = self.fc2(intermediate_vector)\n",
    "\n",
    "        if apply_softmax:\n",
    "            prediction_vector = F.softmax(prediction_vector, dim=1)\n",
    "        return prediction_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ac5c2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "93d6f74e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 94;\n",
       "                var nbb_unformatted_code = \"def make_train_state(args):\\n    return {\\n        \\\"stop_early\\\": False,\\n        \\\"early_stopping_step\\\": 0,\\n        \\\"early_stopping_best_val\\\": 1e8,\\n        \\\"learning_rate\\\": args.learning_rate,\\n        \\\"epoch_index\\\": 0,\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": [],\\n        \\\"test_loss\\\": -1,\\n        \\\"test_acc\\\": -1,\\n        \\\"model_filename\\\": args.model_state_file,\\n    }\\n\\n\\ndef update_train_state(args, model, train_state):\\n    \\\"\\\"\\\"\\n    Handle the training state updates.\\n\\n    Components:\\n    - Early Stopping to prevent overfitting\\n    - Saves model if the model is better\\n\\n    Args:\\n        args: main arguments\\n        model: model to train\\n        train_state: a dict representing the training state values\\n    Returns:\\n        a new train_state\\n    \\\"\\\"\\\"\\n    if train_state[\\\"epoch_index\\\"] == 0:\\n        torch.save(model.state_dict(), train_state[\\\"model_filename\\\"])\\n        train_state[\\\"stop_early\\\"] = False\\n    elif train_state[\\\"epoch_index\\\"] >= 1:\\n        loss_tm1, loss_t = train_state[\\\"val_loss\\\"][-2:]\\n        if loss_t >= train_state[\\\"early_stopping_best_val\\\"]:\\n            train_state[\\\"early_stopping_step\\\"] += 1\\n        else:\\n            if loss_t < train_state[\\\"early_stopping_best_val\\\"]:\\n                torch.save(model.state_dict(), train_state[\\\"model_filename\\\"])\\n            train_state[\\\"early_stopping_step\\\"] = 0\\n        train_state[\\\"stop_early\\\"] = (\\n            train_state[\\\"early_stopping_step\\\"] >= args.early_stopping_criteria\\n        )\\n    return train_state\\n\\n\\ndef compute_accuracy(y_pred, y_target):\\n    _, y_pred_indices = y_pred.max(dim=1)\\n    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\\n    return n_correct / len(y_pred_indices) * 100\\n\\n\\ndef set_seed_everywhere(seed, cuda):\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if cuda:\\n        torch.cuda.manual_seed_all(seed)\\n\\n\\ndef handle_dirs(dirpath):\\n    if not os.path.exists(dirpath):\\n        os.makedirs(dirpath)\";\n",
       "                var nbb_formatted_code = \"def make_train_state(args):\\n    return {\\n        \\\"stop_early\\\": False,\\n        \\\"early_stopping_step\\\": 0,\\n        \\\"early_stopping_best_val\\\": 1e8,\\n        \\\"learning_rate\\\": args.learning_rate,\\n        \\\"epoch_index\\\": 0,\\n        \\\"train_loss\\\": [],\\n        \\\"train_acc\\\": [],\\n        \\\"val_loss\\\": [],\\n        \\\"val_acc\\\": [],\\n        \\\"test_loss\\\": -1,\\n        \\\"test_acc\\\": -1,\\n        \\\"model_filename\\\": args.model_state_file,\\n    }\\n\\n\\ndef update_train_state(args, model, train_state):\\n    \\\"\\\"\\\"\\n    Handle the training state updates.\\n\\n    Components:\\n    - Early Stopping to prevent overfitting\\n    - Saves model if the model is better\\n\\n    Args:\\n        args: main arguments\\n        model: model to train\\n        train_state: a dict representing the training state values\\n    Returns:\\n        a new train_state\\n    \\\"\\\"\\\"\\n    if train_state[\\\"epoch_index\\\"] == 0:\\n        torch.save(model.state_dict(), train_state[\\\"model_filename\\\"])\\n        train_state[\\\"stop_early\\\"] = False\\n    elif train_state[\\\"epoch_index\\\"] >= 1:\\n        loss_tm1, loss_t = train_state[\\\"val_loss\\\"][-2:]\\n        if loss_t >= train_state[\\\"early_stopping_best_val\\\"]:\\n            train_state[\\\"early_stopping_step\\\"] += 1\\n        else:\\n            if loss_t < train_state[\\\"early_stopping_best_val\\\"]:\\n                torch.save(model.state_dict(), train_state[\\\"model_filename\\\"])\\n            train_state[\\\"early_stopping_step\\\"] = 0\\n        train_state[\\\"stop_early\\\"] = (\\n            train_state[\\\"early_stopping_step\\\"] >= args.early_stopping_criteria\\n        )\\n    return train_state\\n\\n\\ndef compute_accuracy(y_pred, y_target):\\n    _, y_pred_indices = y_pred.max(dim=1)\\n    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\\n    return n_correct / len(y_pred_indices) * 100\\n\\n\\ndef set_seed_everywhere(seed, cuda):\\n    np.random.seed(seed)\\n    torch.manual_seed(seed)\\n    if cuda:\\n        torch.cuda.manual_seed_all(seed)\\n\\n\\ndef handle_dirs(dirpath):\\n    if not os.path.exists(dirpath):\\n        os.makedirs(dirpath)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def make_train_state(args):\n",
    "    return {\n",
    "        \"stop_early\": False,\n",
    "        \"early_stopping_step\": 0,\n",
    "        \"early_stopping_best_val\": 1e8,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"epoch_index\": 0,\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"test_loss\": -1,\n",
    "        \"test_acc\": -1,\n",
    "        \"model_filename\": args.model_state_file,\n",
    "    }\n",
    "\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"\n",
    "    Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "    - Early Stopping to prevent overfitting\n",
    "    - Saves model if the model is better\n",
    "\n",
    "    Args:\n",
    "        args: main arguments\n",
    "        model: model to train\n",
    "        train_state: a dict representing the training state values\n",
    "    Returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "    if train_state[\"epoch_index\"] == 0:\n",
    "        torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "        train_state[\"stop_early\"] = False\n",
    "    elif train_state[\"epoch_index\"] >= 1:\n",
    "        loss_tm1, loss_t = train_state[\"val_loss\"][-2:]\n",
    "        if loss_t >= train_state[\"early_stopping_best_val\"]:\n",
    "            train_state[\"early_stopping_step\"] += 1\n",
    "        else:\n",
    "            if loss_t < train_state[\"early_stopping_best_val\"]:\n",
    "                torch.save(model.state_dict(), train_state[\"model_filename\"])\n",
    "            train_state[\"early_stopping_step\"] = 0\n",
    "        train_state[\"stop_early\"] = (\n",
    "            train_state[\"early_stopping_step\"] >= args.early_stopping_criteria\n",
    "        )\n",
    "    return train_state\n",
    "\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c91d5c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a7cb9674",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded Filepaths: models/chapter04/surname_mlp/vectorizer.json, models/chapter04/surname_mlp/model.pth\n",
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 95;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and path information\\n    surname_csv=\\\"../data/surnames/surnames_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/chapter04/surname_mlp\\\",\\n    # Model hyper parameters\\n    hidden_dim=300,\\n    # Training  hyper parameters\\n    seed=1337,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    batch_size=64,\\n    # Runtime options\\n    cuda=False,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(f\\\"Expanded Filepaths: {args.vectorizer_file}, {args.model_state_file}\\\")\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(f\\\"Using CUDA: {args.cuda}\\\")\\nset_seed_everywhere(args.seed, args.cuda)\\nhandle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and path information\\n    surname_csv=\\\"../data/surnames/surnames_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/chapter04/surname_mlp\\\",\\n    # Model hyper parameters\\n    hidden_dim=300,\\n    # Training  hyper parameters\\n    seed=1337,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    batch_size=64,\\n    # Runtime options\\n    cuda=False,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(f\\\"Expanded Filepaths: {args.vectorizer_file}, {args.model_state_file}\\\")\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(f\\\"Using CUDA: {args.cuda}\\\")\\nset_seed_everywhere(args.seed, args.cuda)\\nhandle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and path information\n",
    "    surname_csv=\"../data/surnames/surnames_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/chapter04/surname_mlp\",\n",
    "    # Model hyper parameters\n",
    "    hidden_dim=300,\n",
    "    # Training  hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    # Runtime options\n",
    "    cuda=False,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(f\"Expanded Filepaths: {args.vectorizer_file}, {args.model_state_file}\")\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(f\"Using CUDA: {args.cuda}\")\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7edc3033",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Fresh Dataset!\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 96;\n",
       "                var nbb_unformatted_code = \"if args.reload_from_files:\\n    print(\\\"Reloading Dataset\\\")\\n    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\\n        args.surname_csv, args.vectorizer_file\\n    )\\nelse:\\n    print(\\\"Creating Fresh Dataset!\\\")\\n    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\\n    dataset.save_vectorizer(args.vectorizer_file)\\n\\nvectorizer = dataset.get_vectorizer()\\nclassifier = SurnameClassifier(\\n    input_dim=len(vectorizer.surname_vocab),\\n    hidden_dim=args.hidden_dim,\\n    output_dim=len(vectorizer.nationality_vocab),\\n)\";\n",
       "                var nbb_formatted_code = \"if args.reload_from_files:\\n    print(\\\"Reloading Dataset\\\")\\n    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\\n        args.surname_csv, args.vectorizer_file\\n    )\\nelse:\\n    print(\\\"Creating Fresh Dataset!\\\")\\n    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\\n    dataset.save_vectorizer(args.vectorizer_file)\\n\\nvectorizer = dataset.get_vectorizer()\\nclassifier = SurnameClassifier(\\n    input_dim=len(vectorizer.surname_vocab),\\n    hidden_dim=args.hidden_dim,\\n    output_dim=len(vectorizer.nationality_vocab),\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Reloading Dataset\")\n",
    "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(\n",
    "        args.surname_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    print(\"Creating Fresh Dataset!\")\n",
    "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "classifier = SurnameClassifier(\n",
    "    input_dim=len(vectorizer.surname_vocab),\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=len(vectorizer.nationality_vocab),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ff3628ac",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['@', 'T', 'o', 't', 'a', 'h', 'A', 'b', 'u', 'd', 'F', 'k', 'r', 'y', 'S', 'e', 'g', 'C', 'm', 'H', 'i', 'K', 'n', 'W', 's', 'f', 'G', 'M', 'l', 'B', 'z', 'N', 'I', 'w', 'D', 'Q', 'j', 'E', 'R', 'Z', 'c', 'Y', 'J', 'L', 'O', '-', 'P', 'X', 'p', ':', 'v', 'U', '1', 'V', 'x', 'q', 'é', 'É', \"'\", 'ß', 'ö', 'ä', 'ü', 'ú', 'à', 'ò', 'è', 'ó', 'Ś', 'ą', 'ń', 'á', 'ż', 'õ', 'í', 'ñ', 'Á']),\n",
       " dict_keys(['Arabic', 'Chinese', 'Czech', 'Dutch', 'English', 'French', 'German', 'Greek', 'Irish', 'Italian', 'Japanese', 'Korean', 'Polish', 'Portuguese', 'Russian', 'Scottish', 'Spanish', 'Vietnamese']))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 97;\n",
       "                var nbb_unformatted_code = \"vectorizer.surname_vocab._token_to_idx.keys(), vectorizer.nationality_vocab._token_to_idx.keys()\";\n",
       "                var nbb_formatted_code = \"vectorizer.surname_vocab._token_to_idx.keys(), vectorizer.nationality_vocab._token_to_idx.keys()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.surname_vocab._token_to_idx.keys(), vectorizer.nationality_vocab._token_to_idx.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d0e02",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "87528573",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4622fe5fbf2044839f4b5ec89c78d58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcad51b4c30943a998379f06b7a47993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5807d1a50144a5bc521485664fc04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 98;\n",
       "                var nbb_unformatted_code = \"classifier = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\n\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\n\\ntrain_state = make_train_state(args)\\n\\nepoch_bar = tqdm(desc=\\\"training routine\\\", total=args.num_epochs, position=0)\\n\\ndataset.set_split(\\\"train\\\")\\ntrain_bar = tqdm(\\n    desc=\\\"split=train\\\",\\n    total=dataset.get_num_batches(args.batch_size),\\n    position=1,\\n    leave=True,\\n)\\n\\ndataset.set_split(\\\"val\\\")\\nval_bar = tqdm(\\n    desc=\\\"split=val\\\",\\n    total=dataset.get_num_batches(args.batch_size),\\n    position=1,\\n    leave=True,\\n)\\n\\ntry:\\n    for epoch_index in range(args.num_epochs):\\n        train_state[\\\"epoch_index\\\"] = epoch_index\\n        dataset.set_split(\\\"train\\\")\\n        batch_generator = generate_batches(\\n            dataset, batch_size=args.batch_size, device=args.device\\n        )\\n        running_loss, running_acc = 0.0, 0.0\\n        classifier.train()\\n\\n        for batch_index, batch_dict in enumerate(batch_generator):\\n            # Step 1. Zero the gradients\\n            optimizer.zero_grad()\\n\\n            # Step 2: Compute the output\\n            y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n            # Step 3. Compute the loss\\n            loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            loss_t = loss.to(args.device).item()\\n            running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n            # Step 4: Use loss to produce gradients\\n            loss.backward()\\n\\n            # Step 5: Use optimizer to take gradient step\\n            optimizer.step()\\n\\n            # Compute Accuracy\\n            acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            running_acc += (acc_t - running_acc) / (batch_index + 1)\\n\\n            # Update bar\\n            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\\n            train_bar.update()\\n\\n        train_state[\\\"train_loss\\\"].append(running_loss)\\n        train_state[\\\"train_acc\\\"].append(running_acc)\\n\\n        # Iterate over val dataset\\n        dataset.set_split(\\\"val\\\")\\n        batch_generator = generate_batches(\\n            dataset, batch_size=args.batch_size, device=args.device\\n        )\\n        running_loss, running_acc = 0.0, 0.0\\n        classifier.eval()\\n\\n        for batch_index, batch_dict in enumerate(batch_generator):\\n\\n            # Compute the output\\n            y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n            # Compute the loss\\n            loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            loss_t = loss.to(args.device).item()\\n            running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n            # Compute the accuracy\\n            acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            running_acc += (acc_t - running_acc) / (batch_index + 1)\\n            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\\n            val_bar.update()\\n\\n        train_state[\\\"val_loss\\\"].append(running_loss)\\n        train_state[\\\"val_acc\\\"].append(running_acc)\\n\\n        train_state = update_train_state(\\n            args=args, model=classifier, train_state=train_state\\n        )\\n        scheduler.step(train_state[\\\"val_loss\\\"][-1])\\n\\n        if train_state[\\\"stop_early\\\"]:\\n            break\\n        train_bar.n, val_bar.n = 0, 0\\n        epoch_bar.update()\\nexcept KeyboardInterrupt:\\n    print(\\\"Exiting Loop\\\")\";\n",
       "                var nbb_formatted_code = \"classifier = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\n\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\n\\ntrain_state = make_train_state(args)\\n\\nepoch_bar = tqdm(desc=\\\"training routine\\\", total=args.num_epochs, position=0)\\n\\ndataset.set_split(\\\"train\\\")\\ntrain_bar = tqdm(\\n    desc=\\\"split=train\\\",\\n    total=dataset.get_num_batches(args.batch_size),\\n    position=1,\\n    leave=True,\\n)\\n\\ndataset.set_split(\\\"val\\\")\\nval_bar = tqdm(\\n    desc=\\\"split=val\\\",\\n    total=dataset.get_num_batches(args.batch_size),\\n    position=1,\\n    leave=True,\\n)\\n\\ntry:\\n    for epoch_index in range(args.num_epochs):\\n        train_state[\\\"epoch_index\\\"] = epoch_index\\n        dataset.set_split(\\\"train\\\")\\n        batch_generator = generate_batches(\\n            dataset, batch_size=args.batch_size, device=args.device\\n        )\\n        running_loss, running_acc = 0.0, 0.0\\n        classifier.train()\\n\\n        for batch_index, batch_dict in enumerate(batch_generator):\\n            # Step 1. Zero the gradients\\n            optimizer.zero_grad()\\n\\n            # Step 2: Compute the output\\n            y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n            # Step 3. Compute the loss\\n            loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            loss_t = loss.to(args.device).item()\\n            running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n            # Step 4: Use loss to produce gradients\\n            loss.backward()\\n\\n            # Step 5: Use optimizer to take gradient step\\n            optimizer.step()\\n\\n            # Compute Accuracy\\n            acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            running_acc += (acc_t - running_acc) / (batch_index + 1)\\n\\n            # Update bar\\n            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\\n            train_bar.update()\\n\\n        train_state[\\\"train_loss\\\"].append(running_loss)\\n        train_state[\\\"train_acc\\\"].append(running_acc)\\n\\n        # Iterate over val dataset\\n        dataset.set_split(\\\"val\\\")\\n        batch_generator = generate_batches(\\n            dataset, batch_size=args.batch_size, device=args.device\\n        )\\n        running_loss, running_acc = 0.0, 0.0\\n        classifier.eval()\\n\\n        for batch_index, batch_dict in enumerate(batch_generator):\\n\\n            # Compute the output\\n            y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n            # Compute the loss\\n            loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            loss_t = loss.to(args.device).item()\\n            running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n            # Compute the accuracy\\n            acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n            running_acc += (acc_t - running_acc) / (batch_index + 1)\\n            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\\n            val_bar.update()\\n\\n        train_state[\\\"val_loss\\\"].append(running_loss)\\n        train_state[\\\"val_acc\\\"].append(running_acc)\\n\\n        train_state = update_train_state(\\n            args=args, model=classifier, train_state=train_state\\n        )\\n        scheduler.step(train_state[\\\"val_loss\\\"][-1])\\n\\n        if train_state[\\\"stop_early\\\"]:\\n            break\\n        train_bar.n, val_bar.n = 0, 0\\n        epoch_bar.update()\\nexcept KeyboardInterrupt:\\n    print(\\\"Exiting Loop\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc=\"training routine\", total=args.num_epochs, position=0)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_bar = tqdm(\n",
    "    desc=\"split=train\",\n",
    "    total=dataset.get_num_batches(args.batch_size),\n",
    "    position=1,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "dataset.set_split(\"val\")\n",
    "val_bar = tqdm(\n",
    "    desc=\"split=val\",\n",
    "    total=dataset.get_num_batches(args.batch_size),\n",
    "    position=1,\n",
    "    leave=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state[\"epoch_index\"] = epoch_index\n",
    "        dataset.set_split(\"train\")\n",
    "        batch_generator = generate_batches(\n",
    "            dataset, batch_size=args.batch_size, device=args.device\n",
    "        )\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # Step 1. Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Step 2: Compute the output\n",
    "            y_pred = classifier(batch_dict[\"x_surname\"])\n",
    "\n",
    "            # Step 3. Compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.to(args.device).item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # Step 4: Use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 5: Use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Compute Accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # Update bar\n",
    "            train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state[\"train_loss\"].append(running_loss)\n",
    "        train_state[\"train_acc\"].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "        dataset.set_split(\"val\")\n",
    "        batch_generator = generate_batches(\n",
    "            dataset, batch_size=args.batch_size, device=args.device\n",
    "        )\n",
    "        running_loss, running_acc = 0.0, 0.0\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # Compute the output\n",
    "            y_pred = classifier(batch_dict[\"x_surname\"])\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict[\"y_nationality\"])\n",
    "            loss_t = loss.to(args.device).item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # Compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict[\"y_nationality\"])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state[\"val_loss\"].append(running_loss)\n",
    "        train_state[\"val_acc\"].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(\n",
    "            args=args, model=classifier, train_state=train_state\n",
    "        )\n",
    "        scheduler.step(train_state[\"val_loss\"][-1])\n",
    "\n",
    "        if train_state[\"stop_early\"]:\n",
    "            break\n",
    "        train_bar.n, val_bar.n = 0, 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6db24bc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 99;\n",
       "                var nbb_unformatted_code = \"classifier.load_state_dict(torch.load(train_state['model_filename']))\\n\\nclassifier = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\n\\ndataset.set_split('test')\\nbatch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\\n\\nrunning_loss, running_acc = 0., 0.\\n\\nclassifier.eval()\\n\\nfor batch_index, batch_dict in enumerate(batch_generator):\\n    y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n    # Compute the loss\\n    loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n    loss_t = loss.to(args.device).item()\\n    running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n    # Compute the accuracy\\n    acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n    running_acc += (acc_t - running_acc) / (batch_index + 1)\\n    \\ntrain_state['test_loss'] = running_loss\\ntrain_state['test_acc'] = running_acc\";\n",
       "                var nbb_formatted_code = \"classifier.load_state_dict(torch.load(train_state[\\\"model_filename\\\"]))\\n\\nclassifier = classifier.to(args.device)\\ndataset.class_weights = dataset.class_weights.to(args.device)\\nloss_func = nn.CrossEntropyLoss(dataset.class_weights)\\n\\ndataset.set_split(\\\"test\\\")\\nbatch_generator = generate_batches(\\n    dataset, batch_size=args.batch_size, device=args.device\\n)\\n\\nrunning_loss, running_acc = 0.0, 0.0\\n\\nclassifier.eval()\\n\\nfor batch_index, batch_dict in enumerate(batch_generator):\\n    y_pred = classifier(batch_dict[\\\"x_surname\\\"])\\n\\n    # Compute the loss\\n    loss = loss_func(y_pred, batch_dict[\\\"y_nationality\\\"])\\n    loss_t = loss.to(args.device).item()\\n    running_loss += (loss_t - running_loss) / (batch_index + 1)\\n\\n    # Compute the accuracy\\n    acc_t = compute_accuracy(y_pred, batch_dict[\\\"y_nationality\\\"])\\n    running_acc += (acc_t - running_acc) / (batch_index + 1)\\n\\ntrain_state[\\\"test_loss\\\"] = running_loss\\ntrain_state[\\\"test_acc\\\"] = running_acc\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load(train_state[\"model_filename\"]))\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "dataset.class_weights = dataset.class_weights.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
    "\n",
    "dataset.set_split(\"test\")\n",
    "batch_generator = generate_batches(\n",
    "    dataset, batch_size=args.batch_size, device=args.device\n",
    ")\n",
    "\n",
    "running_loss, running_acc = 0.0, 0.0\n",
    "\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    y_pred = classifier(batch_dict[\"x_surname\"])\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict[\"y_nationality\"])\n",
    "    loss_t = loss.to(args.device).item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict[\"y_nationality\"])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state[\"test_loss\"] = running_loss\n",
    "train_state[\"test_acc\"] = running_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0abe9ff2",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss:1.2401203254858655, Train Acc:52.77343750000001 \n",
      "\n",
      "Val Loss:1.8175728702545166, Val Acc:46.31249999999999 \n",
      "\n",
      "Test Loss:1.8105636978149413, Test Acc:46.68750000000001\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 108;\n",
       "                var nbb_unformatted_code = \"print(\\n    f\\\"\\\\nTrain Loss:{train_state['train_loss'][-1]},\\\"\\n    f\\\" Train Acc:{train_state['train_acc'][-1]}\\\",\\n    f\\\"\\\\n\\\\nVal Loss:{train_state['val_loss'][-1]},\\\"\\n    f\\\" Val Acc:{train_state['val_acc'][-1]}\\\",\\n    f\\\"\\\\n\\\\nTest Loss:{train_state['test_loss']},\\\"\\n    f\\\" Test Acc:{train_state['test_acc']}\\\",\\n)\";\n",
       "                var nbb_formatted_code = \"print(\\n    f\\\"\\\\nTrain Loss:{train_state['train_loss'][-1]},\\\"\\n    f\\\" Train Acc:{train_state['train_acc'][-1]}\\\",\\n    f\\\"\\\\n\\\\nVal Loss:{train_state['val_loss'][-1]},\\\"\\n    f\\\" Val Acc:{train_state['val_acc'][-1]}\\\",\\n    f\\\"\\\\n\\\\nTest Loss:{train_state['test_loss']},\\\" f\\\" Test Acc:{train_state['test_acc']}\\\",\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    f\"\\nTrain Loss:{train_state['train_loss'][-1]},\"\n",
    "    f\" Train Acc:{train_state['train_acc'][-1]}\",\n",
    "    f\"\\n\\nVal Loss:{train_state['val_loss'][-1]},\"\n",
    "    f\" Val Acc:{train_state['val_acc'][-1]}\",\n",
    "    f\"\\n\\nTest Loss:{train_state['test_loss']},\" f\" Test Acc:{train_state['test_acc']}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db5a96b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e1617b07",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 113;\n",
       "                var nbb_unformatted_code = \"def predict_nationality(surname, classifier, vectorizer):\\n    \\\"\\\"\\\"\\n    Predict the nationality from a new surname\\n\\n    Args:\\n        surname (str): the surname to classifier\\n        classifier (SurnameClassifier): an instance of the classifier\\n        vectorizer (SurnameVectorizer): the corresponding vectorizer\\n    Returns:\\n        a dictionary with the most likely nationality and its probability\\n    \\\"\\\"\\\"\\n    vectorized_surname = vectorizer.vectorize(surname)\\n    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\\n    result = classifier(vectorized_surname, apply_softmax=True)\\n\\n    probability_values, indices = result.max(dim=1)\\n    index = indices.item()\\n\\n    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\\n    probability_value = probability_values.item()\\n\\n    return {\\\"nationality\\\": predicted_nationality, \\\"probability\\\": probability_value}\";\n",
       "                var nbb_formatted_code = \"def predict_nationality(surname, classifier, vectorizer):\\n    \\\"\\\"\\\"\\n    Predict the nationality from a new surname\\n\\n    Args:\\n        surname (str): the surname to classifier\\n        classifier (SurnameClassifier): an instance of the classifier\\n        vectorizer (SurnameVectorizer): the corresponding vectorizer\\n    Returns:\\n        a dictionary with the most likely nationality and its probability\\n    \\\"\\\"\\\"\\n    vectorized_surname = vectorizer.vectorize(surname)\\n    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\\n    result = classifier(vectorized_surname, apply_softmax=True)\\n\\n    probability_values, indices = result.max(dim=1)\\n    index = indices.item()\\n\\n    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\\n    probability_value = probability_values.item()\\n\\n    return {\\\"nationality\\\": predicted_nationality, \\\"probability\\\": probability_value}\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_nationality(surname, classifier, vectorizer):\n",
    "    \"\"\"\n",
    "    Predict the nationality from a new surname\n",
    "\n",
    "    Args:\n",
    "        surname (str): the surname to classifier\n",
    "        classifier (SurnameClassifier): an instance of the classifier\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    Returns:\n",
    "        a dictionary with the most likely nationality and its probability\n",
    "    \"\"\"\n",
    "    vectorized_surname = vectorizer.vectorize(surname)\n",
    "    vectorized_surname = torch.tensor(vectorized_surname).view(1, -1)\n",
    "    result = classifier(vectorized_surname, apply_softmax=True)\n",
    "\n",
    "    probability_values, indices = result.max(dim=1)\n",
    "    index = indices.item()\n",
    "\n",
    "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    probability_value = probability_values.item()\n",
    "\n",
    "    return {\"nationality\": predicted_nationality, \"probability\": probability_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c8199446",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify:McMahan\n",
      "McMahan -> Irish (0.4362080991268158)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 115;\n",
       "                var nbb_unformatted_code = \"new_surname = input(\\\"Enter a surname to classify:\\\")\\nclassifier = classifier.to(args.device)\\nprediction = predict_nationality(new_surname, classifier, vectorizer)\\nprint(f\\\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\\\")\";\n",
       "                var nbb_formatted_code = \"new_surname = input(\\\"Enter a surname to classify:\\\")\\nclassifier = classifier.to(args.device)\\nprediction = predict_nationality(new_surname, classifier, vectorizer)\\nprint(f\\\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname to classify:\")\n",
    "classifier = classifier.to(args.device)\n",
    "prediction = predict_nationality(new_surname, classifier, vectorizer)\n",
    "print(f\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73813d4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Top K Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5c226b1b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Irish'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 116;\n",
       "                var nbb_unformatted_code = \"vectorizer.nationality_vocab.lookup_index(8)\";\n",
       "                var nbb_formatted_code = \"vectorizer.nationality_vocab.lookup_index(8)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectorizer.nationality_vocab.lookup_index(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03f070ab",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 117;\n",
       "                var nbb_unformatted_code = \"def predict_topk_nationality(name, classifier, vectorizer, k=5):\\n    vectorized_name = vectorizer.vectorize(name)\\n    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\\n    prediction_vector = classifier(vectorized_name, apply_softmax=True)\\n    probability_values, indices = torch.topk(prediction_vector, k=k)\\n\\n    probability_values = probability_values.detach().numpy()[0]\\n    indices = indices.detach().numpy()[0]\\n    \\n    results = []\\n    \\n    for prob_value, index in zip(probability_values, indices):\\n        nationality = vectorizer.nationality_vocab.lookup_index(index)\\n        results.append(\\n            {'nationality': nationality, 'probability': prob_value}\\n        )\\n    return results\";\n",
       "                var nbb_formatted_code = \"def predict_topk_nationality(name, classifier, vectorizer, k=5):\\n    vectorized_name = vectorizer.vectorize(name)\\n    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\\n    prediction_vector = classifier(vectorized_name, apply_softmax=True)\\n    probability_values, indices = torch.topk(prediction_vector, k=k)\\n\\n    probability_values = probability_values.detach().numpy()[0]\\n    indices = indices.detach().numpy()[0]\\n\\n    results = []\\n\\n    for prob_value, index in zip(probability_values, indices):\\n        nationality = vectorizer.nationality_vocab.lookup_index(index)\\n        results.append({\\\"nationality\\\": nationality, \\\"probability\\\": prob_value})\\n    return results\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_topk_nationality(name, classifier, vectorizer, k=5):\n",
    "    vectorized_name = vectorizer.vectorize(name)\n",
    "    vectorized_name = torch.tensor(vectorized_name).view(1, -1)\n",
    "    prediction_vector = classifier(vectorized_name, apply_softmax=True)\n",
    "    probability_values, indices = torch.topk(prediction_vector, k=k)\n",
    "\n",
    "    probability_values = probability_values.detach().numpy()[0]\n",
    "    indices = indices.detach().numpy()[0]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for prob_value, index in zip(probability_values, indices):\n",
    "        nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "        results.append({\"nationality\": nationality, \"probability\": prob_value})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80c3a0e7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a surname to classify:Shergill\n",
      "How many of the top predictions to see? 10\n",
      "Top 10 predictions\n",
      "=================================\n",
      "Shergill -> German (0.40713614225387573)\n",
      "Shergill -> Dutch (0.25716254115104675)\n",
      "Shergill -> Irish (0.08425957709550858)\n",
      "Shergill -> Czech (0.07259545475244522)\n",
      "Shergill -> English (0.05040072649717331)\n",
      "Shergill -> Scottish (0.025059323757886887)\n",
      "Shergill -> Arabic (0.023825105279684067)\n",
      "Shergill -> Russian (0.020161177963018417)\n",
      "Shergill -> French (0.019863411784172058)\n",
      "Shergill -> Polish (0.014845119789242744)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 120;\n",
       "                var nbb_unformatted_code = \"new_surname = input(\\\"Enter a surname to classify:\\\")\\nclassifier = classifier.to(args.device)\\n\\nk = int(input(\\\"How many of the top predictions to see? \\\"))\\n\\nif k > len(vectorizer.nationality_vocab):\\n    print(\\n        f\\\"This is more than {len(vectorizer.nationality_vocab)}, defaulting to max size\\\"\\n    )\\n    k = len(vectorizer.nationality_vocab)\\n\\npredictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\\n\\nprint(f\\\"Top {k} predictions\\\")\\nprint(\\\"=================================\\\")\\nfor prediction in predictions:\\n    print(f\\\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\\\")\";\n",
       "                var nbb_formatted_code = \"new_surname = input(\\\"Enter a surname to classify:\\\")\\nclassifier = classifier.to(args.device)\\n\\nk = int(input(\\\"How many of the top predictions to see? \\\"))\\n\\nif k > len(vectorizer.nationality_vocab):\\n    print(\\n        f\\\"This is more than {len(vectorizer.nationality_vocab)}, defaulting to max size\\\"\\n    )\\n    k = len(vectorizer.nationality_vocab)\\n\\npredictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\\n\\nprint(f\\\"Top {k} predictions\\\")\\nprint(\\\"=================================\\\")\\nfor prediction in predictions:\\n    print(f\\\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_surname = input(\"Enter a surname to classify:\")\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "k = int(input(\"How many of the top predictions to see? \"))\n",
    "\n",
    "if k > len(vectorizer.nationality_vocab):\n",
    "    print(\n",
    "        f\"This is more than {len(vectorizer.nationality_vocab)}, defaulting to max size\"\n",
    "    )\n",
    "    k = len(vectorizer.nationality_vocab)\n",
    "\n",
    "predictions = predict_topk_nationality(new_surname, classifier, vectorizer, k=k)\n",
    "\n",
    "print(f\"Top {k} predictions\")\n",
    "print(\"=================================\")\n",
    "for prediction in predictions:\n",
    "    print(f\"{new_surname} -> {prediction['nationality']} ({prediction['probability']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637c90e8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d24eb8a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578a2e2e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e6210b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
