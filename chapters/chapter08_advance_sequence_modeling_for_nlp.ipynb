{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31648ca5",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Advanced Sequence Modeling for Natural Language Processing<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Sequence-to-Sequence-Models,-Encoder-Decoder-Models-and-Conditioned-Generation\" data-toc-modified-id=\"Sequence-to-Sequence-Models,-Encoder-Decoder-Models-and-Conditioned-Generation-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Sequence-to-Sequence Models, Encoder-Decoder Models and Conditioned Generation</a></span></li><li><span><a href=\"#Capturing-More-from-a-Sequence:-Bidirectional-Recurrent-Models\" data-toc-modified-id=\"Capturing-More-from-a-Sequence:-Bidirectional-Recurrent-Models-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Capturing More from a Sequence: Bidirectional Recurrent Models</a></span></li><li><span><a href=\"#Capturing-More-from-a-Sequence:-Attention\" data-toc-modified-id=\"Capturing-More-from-a-Sequence:-Attention-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Capturing More from a Sequence: Attention</a></span><ul class=\"toc-item\"><li><span><a href=\"#Attention-in-Deep-Neural-Networks\" data-toc-modified-id=\"Attention-in-Deep-Neural-Networks-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Attention in Deep Neural Networks</a></span></li></ul></li><li><span><a href=\"#Evaluating-Sequence-Generation-Models\" data-toc-modified-id=\"Evaluating-Sequence-Generation-Models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Evaluating Sequence Generation Models</a></span></li><li><span><a href=\"#Example:-Neural-Machine-Translation\" data-toc-modified-id=\"Example:-Neural-Machine-Translation-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Example: Neural Machine Translation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Vocabulary,-Vectorizer-and-Dataset\" data-toc-modified-id=\"Vocabulary,-Vectorizer-and-Dataset-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Vocabulary, Vectorizer and Dataset</a></span></li><li><span><a href=\"#NMT-Model-with-No-Sampling\" data-toc-modified-id=\"NMT-Model-with-No-Sampling-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>NMT Model with No Sampling</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c008ba4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068d8768",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- _Sequence-to-sequence Modeling_ refers to taking sequence as input and producing another sequence as an output of possibly different length\n",
    "- Examples\n",
    "    - Predict response for a given email\n",
    "    - Translate text\n",
    "    - Summarize the given text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087be68d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Sequence-to-Sequence Models, Encoder-Decoder Models and Conditioned Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c2de11",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Sequence-to-Sequence(S2S)** models are a special case of general family of models alled _encoder-decoder models_\n",
    "- Encoder-Decoder model is a composition of two models -> an Encoder and a Decoder that are trained jointly.\n",
    "    - Encoder Model takes an input and produces an encoding or a representation $(\\phi)$ of the input which is usually a vector. The goal of the encoder is to capture important properties of the input with respect to the task at hand.\n",
    "    - The goal of the decoder is to take the encoded input and produce a desired output.\n",
    "- So S2S models can be defined as encoder-decoder models in which the encoder and decoder are sequence models and the inputs and outputs are both sequences possibly of different lengths.\n",
    "\n",
    "![Figure 8.1](../images/figure_8_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8b4011",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Encoder-Decoder Models as Special Case of Conditioned Generation Models**\n",
    "\n",
    "- In Conditioned generation, instead of the input respresentation $\\phi$, a general conditioning context $c$ influences a decoder to produce an output.\n",
    "- When the conditioning context $c$ comes from an encoder model, conditioned generation is same as an encoder-decoder model.\n",
    "- Not all conditioned generation models are encoder-decoder models, because it is possible for the conditioning context to be derived from a structured source.\n",
    "- For example, in weather report generation, the value of the temperature, humidity and wind speed and direction could condition a decoder to generate the textual weather report.\n",
    "\n",
    "![Figure 8.2](../images/figure_8_2.png)\n",
    "![Figure 8.3](../images/figure_8_3.png)\n",
    "![Figure 8.4](../images/figure_8_4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620c4e38",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Capturing More from a Sequence: Bidirectional Recurrent Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a290d1f",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The goal of bidirectional recurrent model is to combine the information from past and future to robustly represent the meaning of a word in a sequence.\n",
    "- Any model in the recurrent family, such as Elmann RNNs, LSTMs or GRUs could be used in such a bidirectional formulation.\n",
    "- Bidirectional models like unidirectional models can be used in both classification and sequence labeling settings.\n",
    "\n",
    "![Figure 8.5](../images/figure_8_5.png)\n",
    "![Figure 8.6](../images/figure_8_6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f8c959",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Capturing More from a Sequence: Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b6022",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Problems with S2S, encoder-decoder and conditioned generation models**\n",
    "\n",
    "- These models crams(encodes) the entire input sentence into a single vector $\\phi$ and uses that encoding to generate the output. This might work for very short sentences but will fail to capture the information in the entire input in case of ling sentences. This is a limitation of using just the final hidden state as the encoding.\n",
    "- Gradients vanishing problem can also happen during back prooagation throigh time which makes the training difficult.\n",
    "\n",
    "![Figure 8.7](../images/figure_8_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d975237",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- **Attention** is the phenomenon in which our minds focus on the relevant parts of the input while producing output.\n",
    "- **Attention mechanism** is the process in which sequence generation models incorporate attention to different parts of the input and not just the final summary of the input.\n",
    "- The first models to incorporate the notion of attention for NLP were machine translation models by Bahdanau(2015).\n",
    "\n",
    "![Figure 8.8](../images/figure_8_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ec8058",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Attention in Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ee4bf2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- In typical S2S Model, each time step produces a hidden state representation, denoted as $\\phi_w$, specific to that time step in the encoder.\n",
    "- To incorporate attention, we consider not only the final hidden state of the encoder but also the hidden states for each of the intermediate steps. These encoder hidden states are uninformatively called _values_.\n",
    "- Attention also depends on the previous hidden state of the decoder called the _query_. The query vector for time step $t=0$ is a fixed hyperparameter.\n",
    "- Attention is represented by a vector with the same dimension as the number of values it is attending to. This is called _attention vector_, or _attention weights_ or sometimes _alignment_.\n",
    "- The attention weights are combined with the encoder states(values) to generate a _context vector_ that sometimes also known as a _glimpse_. This context vector becomes the input for the decoder instead of the full sentence encoding.\n",
    "- The attention vector for the next time step is updated using a _compatibility function_. The exact nature of the compatibility function depends on the attention mechanism being used.\n",
    "\n",
    "![Figure 8.9](../images/figure_8_9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8be84e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Ways to Implement Attention**\n",
    "- Simplest and most common is **Content-aware Mechanism**.\n",
    "- Another popular attention is **Location-aware Attention** which depends only on query vector and the key.\n",
    "- Attention weights are typically floating-point values between 0 and 1. This is called **Soft Attention**.\n",
    "- It is also possible to learn a binary 0/1 vector for attention which is called **Hard Attention**.\n",
    "- When the encoder depends on the states for all the time step in the input, this is known as **Global Attention**.\n",
    "- In **Local Attention**, attention mechanism only depends on a window of the input around the current time step.\n",
    "- When multiple attention vector are used to track different regions of input such mechanism is known as **Multiheaded Attention** which is based on Vaswani(2017) work. This popularized the concept of **Self Attention** a mechanizm where the model learns which regions of the input influence one another.\n",
    "- When the input is multimodal like both image and speech, it is possible to design a **Multimodal Attention Mechanism**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c25495",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluating Sequence Generation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2ab59",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sequence Models are evaluated against an expected output called **Reference Output**.\n",
    "\n",
    "There are two kinds of evaluation for sequence generation models:\n",
    "\n",
    "- Human Evaluation\n",
    "- Automation Evaluation\n",
    "    - n-gram overlap based metrics -> BLEU, ROUGE, METEOR\n",
    "    - Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e446e5e",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Example: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39e943dc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport os\\nfrom argparse import Namespace\\nfrom collections import Counter\\nimport json\\nimport re\\nimport string\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn import functional as F\\nimport torch.optim as optim\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm import notebook\\n\\nimport utils\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport os\\nfrom argparse import Namespace\\nfrom collections import Counter\\nimport json\\nimport re\\nimport string\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\nimport torch.nn as nn\\nfrom torch.nn import functional as F\\nimport torch.optim as optim\\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\\nfrom torch.utils.data import Dataset, DataLoader\\nfrom tqdm import notebook\\n\\nimport utils\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import notebook\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b99a47",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Vocabulary, Vectorizer and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7709e98a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"class Vocabulary(object):\\n    def __init__(self, token_to_idx=None):\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idk: token for token, idx in self._token_to_idx.items()}\\n\\n    def to_serializable(self):\\n        return {\\\"token_to_idx\\\": self._token_to_idx}\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        if token in self._token_to_idx:\\n            index = self._token_to_idx[token]\\n        else:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"The index {index} is not in the Vocab.\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\";\n",
       "                var nbb_formatted_code = \"class Vocabulary(object):\\n    def __init__(self, token_to_idx=None):\\n        if token_to_idx is None:\\n            token_to_idx = {}\\n        self._token_to_idx = token_to_idx\\n        self._idx_to_token = {idk: token for token, idx in self._token_to_idx.items()}\\n\\n    def to_serializable(self):\\n        return {\\\"token_to_idx\\\": self._token_to_idx}\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        return cls(**contents)\\n\\n    def add_token(self, token):\\n        if token in self._token_to_idx:\\n            index = self._token_to_idx[token]\\n        else:\\n            index = len(self._token_to_idx)\\n            self._token_to_idx[token] = index\\n            self._idx_to_token[index] = token\\n        return index\\n\\n    def add_many(self, tokens):\\n        return [self.add_token(token) for token in tokens]\\n\\n    def lookup_token(self, token):\\n        return self._token_to_idx[token]\\n\\n    def lookup_index(self, index):\\n        if index not in self._idx_to_token:\\n            raise KeyError(f\\\"The index {index} is not in the Vocab.\\\")\\n        return self._idx_to_token[index]\\n\\n    def __str__(self):\\n        return f\\\"<Vocabulary(size={len(self)})>\\\"\\n\\n    def __len__(self):\\n        return len(self._token_to_idx)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "        self._idx_to_token = {idk: token for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\"token_to_idx\": self._token_to_idx}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(f\"The index {index} is not in the Vocab.\")\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"<Vocabulary(size={len(self)})>\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "002d267a",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"class SequenceVocabulary(Vocabulary):\\n    def __init__(\\n        self,\\n        token_to_idx=None,\\n        unk_token=\\\"<UNK>\\\",\\n        mask_token=\\\"<MASK>\\\",\\n        begin_seq_token=\\\"<BEGIN>\\\",\\n        end_seq_token=\\\"<END>\\\",\\n    ):\\n        super(SequenceVocabulary, self).__init__(token_to_idx)\\n        self._mask_token = mask_token\\n        self._unk_token = unk_token\\n        self._begin_seq_token = begin_seq_token\\n        self._end_seq_token = end_seq_token\\n\\n        self.mask_index = self.add_token(self._mask_token)\\n        self.unk_index = self.add_token(self._unk_token)\\n        self.begin_seq_index = self.add_token(self._begin_seq_token)\\n        self.end_seq_index = self.add_token(self._end_seq_token)\\n\\n    def to_serializable(self):\\n        contents = super(SequenceVocabulary, self).to_serializable()\\n        contents.update(\\n            {\\n                \\\"unk_token\\\": self._unk_token,\\n                \\\"mask_token\\\": self._mask_token,\\n                \\\"begin_seq_token\\\": self._begin_seq_token,\\n                \\\"end_seq_token\\\": self._end_seq_token,\\n            }\\n        )\\n        return contents\\n\\n    def lookup_token(self, token):\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_index)\\n        else:\\n            return self._token_to_idx[token]\";\n",
       "                var nbb_formatted_code = \"class SequenceVocabulary(Vocabulary):\\n    def __init__(\\n        self,\\n        token_to_idx=None,\\n        unk_token=\\\"<UNK>\\\",\\n        mask_token=\\\"<MASK>\\\",\\n        begin_seq_token=\\\"<BEGIN>\\\",\\n        end_seq_token=\\\"<END>\\\",\\n    ):\\n        super(SequenceVocabulary, self).__init__(token_to_idx)\\n        self._mask_token = mask_token\\n        self._unk_token = unk_token\\n        self._begin_seq_token = begin_seq_token\\n        self._end_seq_token = end_seq_token\\n\\n        self.mask_index = self.add_token(self._mask_token)\\n        self.unk_index = self.add_token(self._unk_token)\\n        self.begin_seq_index = self.add_token(self._begin_seq_token)\\n        self.end_seq_index = self.add_token(self._end_seq_token)\\n\\n    def to_serializable(self):\\n        contents = super(SequenceVocabulary, self).to_serializable()\\n        contents.update(\\n            {\\n                \\\"unk_token\\\": self._unk_token,\\n                \\\"mask_token\\\": self._mask_token,\\n                \\\"begin_seq_token\\\": self._begin_seq_token,\\n                \\\"end_seq_token\\\": self._end_seq_token,\\n            }\\n        )\\n        return contents\\n\\n    def lookup_token(self, token):\\n        if self.unk_index >= 0:\\n            return self._token_to_idx.get(token, self.unk_index)\\n        else:\\n            return self._token_to_idx[token]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(\n",
    "        self,\n",
    "        token_to_idx=None,\n",
    "        unk_token=\"<UNK>\",\n",
    "        mask_token=\"<MASK>\",\n",
    "        begin_seq_token=\"<BEGIN>\",\n",
    "        end_seq_token=\"<END>\",\n",
    "    ):\n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        contents = super(SequenceVocabulary, self).to_serializable()\n",
    "        contents.update(\n",
    "            {\n",
    "                \"unk_token\": self._unk_token,\n",
    "                \"mask_token\": self._mask_token,\n",
    "                \"begin_seq_token\": self._begin_seq_token,\n",
    "                \"end_seq_token\": self._end_seq_token,\n",
    "            }\n",
    "        )\n",
    "        return contents\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb5a39c5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"class NMTVectorizer(object):\\n    def __init__(self, source_vocab, target_vocab, max_source_length, max_target_length):\\n        self.source_vocab = source_vocab\\n        self.target_vocab = target_vocab\\n        \\n        self.max_source_length = max_source_length\\n        self.max_target_length = max_target_length\\n        \\n    def _vectorize(self, indices, vector_length=-1, mask_index=0):\\n        if vector_length < 0:\\n            vector_length = len(indices)\\n        vector = np.zeros(vector_length, dtype=np.int64)\\n        vector[:len(indices)] = indices\\n        vector[len(indices):] = mask_index\\n    \\n    def _get_source_indices(self, text):\\n        indices = [self.source_vocab.begin_seq_index]\\n        indices.extend(\\n            self.source_vocab.lookup_token(token)\\n            for token in text.split(\\\" \\\")\\n        )\\n        indices.append(\\n            self.source_vocab.end_seq_index\\n        )\\n        return indices\\n    \\n    def _get_target_indices(self, text):\\n        indices = [\\n            self.target_vocab.lookup_token(token)\\n            for token in text.split(\\\" \\\")\\n        ]\\n        x_indices = [\\n            self.target_vocab.begin_seq_index\\n        ] + indices\\n        y_indices = indices + [self.target_vocab.end_seq_index]\\n        return x_indices, y_indices\\n    \\n    def vectorizer(\\n        self, source_text, target_text, use_dataset_max_lengths=True\\n    ):\\n        source_vector_length, target_vector_length = -1, -1\\n        if use_dataset_max_lengths:\\n            source_vector_length = self.max_source_length + 2\\n            target_vector_length = self.max_target_length + 1\\n        \\n        source_indices = self._get_source_indices(source_text)\\n        source_vector = self._vectorize(\\n            source_indices,\\n            vector_length,\\n            self.source_vocab.mask_index\\n        )\\n\\n        target_x_indices, target_y_indices = self._get_target_indices(\\n            target_text\\n        )\\n        target_x_vector = self._vectorize(\\n            target_x_indices,\\n            target_vector_length,\\n            self.target_vocab.mask_index\\n        )\\n\\n        target_y_vector = self._vectorize(\\n            target_y_indices,\\n            target_vector_length,\\n            self.target_vocab.mask_index\\n        )\\n        return {\\n            \\\"source_vector\\\": source_vector,\\n            \\\"target_x_vector\\\": target_x_vector,\\n            \\\"target_y_vector\\\": target_y_vector,\\n            \\\"source_length\\\": len(source_indices)\\n        }\\n    \\n    @classmethod\\n    def from_dataframe(cls, bitext_df):\\n        source_vocab = SequenceVocabulary()\\n        target_vocab = SequenceVocabulary()\\n        \\n        max_source_length, max_target_length = 0, 0\\n        \\n        for _, row in bitext_df.iterrows():\\n            source_tokens = row[\\\"source_language\\\"].split(\\\" \\\")\\n            if len(source_tokens) > max_source_length:\\n                max_source_length = len(source_tokens)\\n            for token in source_tokens:\\n                source_vocab.add_token(token)\\n                \\n            target_tokens = row[\\\"target_language\\\"].split(\\\" \\\")\\n            if len(target_tokens) > max_target_length:\\n                max_target_length = len(target_tokens)\\n            for token in target_tokens:\\n                target_vocab.add_token(token)\\n        return cls(\\n            source_vocab, target_vocab, max_source_length, max_target_length\\n        )\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        source_vocab = SequenceVocabulary.from_serializable(\\n            contents[\\\"source_vocab\\\"]\\n        )\\n        target_vocab = SequenceVocabulary.from_serializable(\\n            contents[\\\"target_vocab\\\"]\\n        )\\n        return cls(\\n            source_vocab=source_vocab,\\n            target_vocab=target_vocab,\\n            max_source_length=contents[\\\"max_source_length\\\"],\\n            max_target_length=contents[\\\"max_target_length\\\"]\\n        )\\n    \\n    def to_serializable(self):\\n        return {\\n            \\\"source_vocab\\\": self.source_vocab.to_serializable(),\\n            \\\"target_vocab\\\": self.target_vocab.to_serializable(),\\n            \\\"max_source_length\\\": self.max_source_length,\\n            \\\"max_target_length\\\": self.max_target_length\\n        }\";\n",
       "                var nbb_formatted_code = \"class NMTVectorizer(object):\\n    def __init__(\\n        self, source_vocab, target_vocab, max_source_length, max_target_length\\n    ):\\n        self.source_vocab = source_vocab\\n        self.target_vocab = target_vocab\\n\\n        self.max_source_length = max_source_length\\n        self.max_target_length = max_target_length\\n\\n    def _vectorize(self, indices, vector_length=-1, mask_index=0):\\n        if vector_length < 0:\\n            vector_length = len(indices)\\n        vector = np.zeros(vector_length, dtype=np.int64)\\n        vector[: len(indices)] = indices\\n        vector[len(indices) :] = mask_index\\n\\n    def _get_source_indices(self, text):\\n        indices = [self.source_vocab.begin_seq_index]\\n        indices.extend(\\n            self.source_vocab.lookup_token(token) for token in text.split(\\\" \\\")\\n        )\\n        indices.append(self.source_vocab.end_seq_index)\\n        return indices\\n\\n    def _get_target_indices(self, text):\\n        indices = [self.target_vocab.lookup_token(token) for token in text.split(\\\" \\\")]\\n        x_indices = [self.target_vocab.begin_seq_index] + indices\\n        y_indices = indices + [self.target_vocab.end_seq_index]\\n        return x_indices, y_indices\\n\\n    def vectorizer(self, source_text, target_text, use_dataset_max_lengths=True):\\n        source_vector_length, target_vector_length = -1, -1\\n        if use_dataset_max_lengths:\\n            source_vector_length = self.max_source_length + 2\\n            target_vector_length = self.max_target_length + 1\\n\\n        source_indices = self._get_source_indices(source_text)\\n        source_vector = self._vectorize(\\n            source_indices, vector_length, self.source_vocab.mask_index\\n        )\\n\\n        target_x_indices, target_y_indices = self._get_target_indices(target_text)\\n        target_x_vector = self._vectorize(\\n            target_x_indices, target_vector_length, self.target_vocab.mask_index\\n        )\\n\\n        target_y_vector = self._vectorize(\\n            target_y_indices, target_vector_length, self.target_vocab.mask_index\\n        )\\n        return {\\n            \\\"source_vector\\\": source_vector,\\n            \\\"target_x_vector\\\": target_x_vector,\\n            \\\"target_y_vector\\\": target_y_vector,\\n            \\\"source_length\\\": len(source_indices),\\n        }\\n\\n    @classmethod\\n    def from_dataframe(cls, bitext_df):\\n        source_vocab = SequenceVocabulary()\\n        target_vocab = SequenceVocabulary()\\n\\n        max_source_length, max_target_length = 0, 0\\n\\n        for _, row in bitext_df.iterrows():\\n            source_tokens = row[\\\"source_language\\\"].split(\\\" \\\")\\n            if len(source_tokens) > max_source_length:\\n                max_source_length = len(source_tokens)\\n            for token in source_tokens:\\n                source_vocab.add_token(token)\\n\\n            target_tokens = row[\\\"target_language\\\"].split(\\\" \\\")\\n            if len(target_tokens) > max_target_length:\\n                max_target_length = len(target_tokens)\\n            for token in target_tokens:\\n                target_vocab.add_token(token)\\n        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\\n\\n    @classmethod\\n    def from_serializable(cls, contents):\\n        source_vocab = SequenceVocabulary.from_serializable(contents[\\\"source_vocab\\\"])\\n        target_vocab = SequenceVocabulary.from_serializable(contents[\\\"target_vocab\\\"])\\n        return cls(\\n            source_vocab=source_vocab,\\n            target_vocab=target_vocab,\\n            max_source_length=contents[\\\"max_source_length\\\"],\\n            max_target_length=contents[\\\"max_target_length\\\"],\\n        )\\n\\n    def to_serializable(self):\\n        return {\\n            \\\"source_vocab\\\": self.source_vocab.to_serializable(),\\n            \\\"target_vocab\\\": self.target_vocab.to_serializable(),\\n            \\\"max_source_length\\\": self.max_source_length,\\n            \\\"max_target_length\\\": self.max_target_length,\\n        }\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMTVectorizer(object):\n",
    "    def __init__(\n",
    "        self, source_vocab, target_vocab, max_source_length, max_target_length\n",
    "    ):\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def _vectorize(self, indices, vector_length=-1, mask_index=0):\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "        vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        vector[: len(indices)] = indices\n",
    "        vector[len(indices) :] = mask_index\n",
    "\n",
    "    def _get_source_indices(self, text):\n",
    "        indices = [self.source_vocab.begin_seq_index]\n",
    "        indices.extend(\n",
    "            self.source_vocab.lookup_token(token) for token in text.split(\" \")\n",
    "        )\n",
    "        indices.append(self.source_vocab.end_seq_index)\n",
    "        return indices\n",
    "\n",
    "    def _get_target_indices(self, text):\n",
    "        indices = [self.target_vocab.lookup_token(token) for token in text.split(\" \")]\n",
    "        x_indices = [self.target_vocab.begin_seq_index] + indices\n",
    "        y_indices = indices + [self.target_vocab.end_seq_index]\n",
    "        return x_indices, y_indices\n",
    "\n",
    "    def vectorizer(self, source_text, target_text, use_dataset_max_lengths=True):\n",
    "        source_vector_length, target_vector_length = -1, -1\n",
    "        if use_dataset_max_lengths:\n",
    "            source_vector_length = self.max_source_length + 2\n",
    "            target_vector_length = self.max_target_length + 1\n",
    "\n",
    "        source_indices = self._get_source_indices(source_text)\n",
    "        source_vector = self._vectorize(\n",
    "            source_indices, vector_length, self.source_vocab.mask_index\n",
    "        )\n",
    "\n",
    "        target_x_indices, target_y_indices = self._get_target_indices(target_text)\n",
    "        target_x_vector = self._vectorize(\n",
    "            target_x_indices, target_vector_length, self.target_vocab.mask_index\n",
    "        )\n",
    "\n",
    "        target_y_vector = self._vectorize(\n",
    "            target_y_indices, target_vector_length, self.target_vocab.mask_index\n",
    "        )\n",
    "        return {\n",
    "            \"source_vector\": source_vector,\n",
    "            \"target_x_vector\": target_x_vector,\n",
    "            \"target_y_vector\": target_y_vector,\n",
    "            \"source_length\": len(source_indices),\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "\n",
    "        max_source_length, max_target_length = 0, 0\n",
    "\n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = row[\"source_language\"].split(\" \")\n",
    "            if len(source_tokens) > max_source_length:\n",
    "                max_source_length = len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "\n",
    "            target_tokens = row[\"target_language\"].split(\" \")\n",
    "            if len(target_tokens) > max_target_length:\n",
    "                max_target_length = len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        source_vocab = SequenceVocabulary.from_serializable(contents[\"source_vocab\"])\n",
    "        target_vocab = SequenceVocabulary.from_serializable(contents[\"target_vocab\"])\n",
    "        return cls(\n",
    "            source_vocab=source_vocab,\n",
    "            target_vocab=target_vocab,\n",
    "            max_source_length=contents[\"max_source_length\"],\n",
    "            max_target_length=contents[\"max_target_length\"],\n",
    "        )\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {\n",
    "            \"source_vocab\": self.source_vocab.to_serializable(),\n",
    "            \"target_vocab\": self.target_vocab.to_serializable(),\n",
    "            \"max_source_length\": self.max_source_length,\n",
    "            \"max_target_length\": self.max_target_length,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff66aaf6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"class NMTDataset(Dataset):\\n    def __init__(self, text_df, vectorizer):\\n        self.text_df = surname_df\\n        self._vectorizer = vectorizer\\n\\n        self.train_df = self.surname_df[self.surname_df.split == \\\"train\\\"]\\n        self.train_size = len(self.train_df)\\n\\n        self.val_df = self.surname_df[self.surname_df.split == \\\"val\\\"]\\n        self.val_size = len(self.val_df)\\n\\n        self.test_df = self.surname_df[self.surname_df.split == \\\"test\\\"]\\n        self.test_size = len(self.test_df)\\n\\n        self._lookup_dict = {\\n            \\\"train\\\": (self.train_df, self.train_size),\\n            \\\"val\\\": (self.val_df, self.val_size),\\n            \\\"test\\\": (self.test_df, self.test_size),\\n        }\\n        self.set_split(\\\"train\\\")\\n\\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, dataset_csv):\\n        text_df = pd.read_csv(dataset_csv)\\n        train_subset = text_df[text_df.split == 'train']\\n        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\\n\\n    @classmethod\\n    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\\n        text_df = pd.read_csv(dataset_csv)\\n        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\\n        return cls(text_df, vectorizer)\\n\\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        with open(vectorizer_filepath) as fp:\\n            return NMTVectorizer.from_serializable(json.load(fp))\\n\\n    def save_vectorizer(self, vectorizer_filepath):\\n        with open(vectorizer_filepath, \\\"w\\\") as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n\\n    def get_vectorizer(self):\\n        return self._vectorizer\\n\\n    def set_split(self, split=\\\"train\\\"):\\n        self._train_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n\\n    def __len__(self):\\n        return self._target_size\\n\\n    def __getitem__(self, index):\\n        row = self._target_df.iloc[index]\\n        vector_dict = self._vectorizer.vectorize(\\n            row.source_language, row.target_language\\n        )\\n        return {\\n            \\\"x_source\\\": vector_dict[\\\"source_vector\\\"],\\n            \\\"x_target\\\": vector_dict[\\\"target_x_vector\\\"],\\n            \\\"y_target\\\": vector_dict[\\\"target_y_vector\\\"],\\n            \\\"x_source_length\\\": vector_dict[\\\"source_length\\\"]\\n        }\\n\\n    def get_num_batches(self, batch_size):\\n        return len(self) // batch_size\";\n",
       "                var nbb_formatted_code = \"class NMTDataset(Dataset):\\n    def __init__(self, text_df, vectorizer):\\n        self.text_df = surname_df\\n        self._vectorizer = vectorizer\\n\\n        self.train_df = self.surname_df[self.surname_df.split == \\\"train\\\"]\\n        self.train_size = len(self.train_df)\\n\\n        self.val_df = self.surname_df[self.surname_df.split == \\\"val\\\"]\\n        self.val_size = len(self.val_df)\\n\\n        self.test_df = self.surname_df[self.surname_df.split == \\\"test\\\"]\\n        self.test_size = len(self.test_df)\\n\\n        self._lookup_dict = {\\n            \\\"train\\\": (self.train_df, self.train_size),\\n            \\\"val\\\": (self.val_df, self.val_size),\\n            \\\"test\\\": (self.test_df, self.test_size),\\n        }\\n        self.set_split(\\\"train\\\")\\n\\n    @classmethod\\n    def load_dataset_and_make_vectorizer(cls, dataset_csv):\\n        text_df = pd.read_csv(dataset_csv)\\n        train_subset = text_df[text_df.split == \\\"train\\\"]\\n        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\\n\\n    @classmethod\\n    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\\n        text_df = pd.read_csv(dataset_csv)\\n        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\\n        return cls(text_df, vectorizer)\\n\\n    @staticmethod\\n    def load_vectorizer_only(vectorizer_filepath):\\n        with open(vectorizer_filepath) as fp:\\n            return NMTVectorizer.from_serializable(json.load(fp))\\n\\n    def save_vectorizer(self, vectorizer_filepath):\\n        with open(vectorizer_filepath, \\\"w\\\") as fp:\\n            json.dump(self._vectorizer.to_serializable(), fp)\\n\\n    def get_vectorizer(self):\\n        return self._vectorizer\\n\\n    def set_split(self, split=\\\"train\\\"):\\n        self._train_split = split\\n        self._target_df, self._target_size = self._lookup_dict[split]\\n\\n    def __len__(self):\\n        return self._target_size\\n\\n    def __getitem__(self, index):\\n        row = self._target_df.iloc[index]\\n        vector_dict = self._vectorizer.vectorize(\\n            row.source_language, row.target_language\\n        )\\n        return {\\n            \\\"x_source\\\": vector_dict[\\\"source_vector\\\"],\\n            \\\"x_target\\\": vector_dict[\\\"target_x_vector\\\"],\\n            \\\"y_target\\\": vector_dict[\\\"target_y_vector\\\"],\\n            \\\"x_source_length\\\": vector_dict[\\\"source_length\\\"],\\n        }\\n\\n    def get_num_batches(self, batch_size):\\n        return len(self) // batch_size\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        self.text_df = surname_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        self.train_df = self.surname_df[self.surname_df.split == \"train\"]\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.surname_df[self.surname_df.split == \"val\"]\n",
    "        self.val_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.surname_df[self.surname_df.split == \"test\"]\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {\n",
    "            \"train\": (self.train_df, self.train_size),\n",
    "            \"val\": (self.val_df, self.val_size),\n",
    "            \"test\": (self.test_df, self.test_size),\n",
    "        }\n",
    "        self.set_split(\"train\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        train_subset = text_df[text_df.split == \"train\"]\n",
    "        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, dataset_csv, vectorizer_filepath):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(text_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return NMTVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._train_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    "        vector_dict = self._vectorizer.vectorize(\n",
    "            row.source_language, row.target_language\n",
    "        )\n",
    "        return {\n",
    "            \"x_source\": vector_dict[\"source_vector\"],\n",
    "            \"x_target\": vector_dict[\"target_x_vector\"],\n",
    "            \"y_target\": vector_dict[\"target_y_vector\"],\n",
    "            \"x_source_length\": vector_dict[\"source_length\"],\n",
    "        }\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        return len(self) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7798d96",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"def generate_nmt_batches(\\n    dataset, batch_size, shuffle=True, drop_last=True, device=\\\"cpu\\\"\\n):\\n    dataloader = DataLoader(\\n        dataset=dataset,\\n        batch_size=batch_size,\\n        shuffle=shuffle,\\n        drop_last=drop_last\\n    )\\n    for data_dict in dataloader:\\n        lengths = data_dict['x_source_length'].numpy()\\n        sorted_length_indices = lengths.argsort()[::-1].tolist()\\n        \\n        out_data_dict = {}\\n        for name, tensor in data_dict.items():\\n            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\\n        yield out_data_dict\";\n",
       "                var nbb_formatted_code = \"def generate_nmt_batches(\\n    dataset, batch_size, shuffle=True, drop_last=True, device=\\\"cpu\\\"\\n):\\n    dataloader = DataLoader(\\n        dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\\n    )\\n    for data_dict in dataloader:\\n        lengths = data_dict[\\\"x_source_length\\\"].numpy()\\n        sorted_length_indices = lengths.argsort()[::-1].tolist()\\n\\n        out_data_dict = {}\\n        for name, tensor in data_dict.items():\\n            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\\n        yield out_data_dict\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def generate_nmt_batches(\n",
    "    dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"\n",
    "):\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    for data_dict in dataloader:\n",
    "        lengths = data_dict[\"x_source_length\"].numpy()\n",
    "        sorted_length_indices = lengths.argsort()[::-1].tolist()\n",
    "\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name][sorted_length_indices].to(device)\n",
    "        yield out_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59580f79",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### NMT Model with No Sampling\n",
    "\n",
    "Components:\n",
    "\n",
    "1. NMTEncoder\n",
    "    - accepts as input a source sequence to be embedded and fed through a bi-directional GRU\n",
    "2. NMTDecoder\n",
    "    - using the encoder state and attention, the decoder generates a new sequence\n",
    "    - the ground truth target sequence is used as input to the decoder at each time step\n",
    "    - an alternative formulation would allow some of the decoder's own choices to be used as input\n",
    "    - this is referred to as curriculum learning, learning to search\n",
    "        - TODO: Look up references for this. I believe Bengio has a paper from the image captioning competitions. Hal Daume has tons on this and is the main NLP guy for it.\n",
    "3. NMTModel\n",
    "    - Combines the encoder and decoder into a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9fb4230",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"def verbose_attention(encoder_state_vectors, query_vector):\\n    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\\n    vector_scores = torch.sum(\\n        encoder_state_vectors * query_vector.view(batch_size, 1, vector_size),\\n        dim=2\\n    )\\n    vector_probabilities = F.softmax(vector_scores, dim=1)\\n    weighted_vectors = encoder_state_vectors * vector_probabilities.view(batch_size, num_vectors, 1)\\n    context_vectors = torch.sum(weighted_vectors, dim=1)\\n    return context_vectors, vector_probabilities, vector_scores\\n\\ndef terse_attention(encoder_state_vectors, query_vector):\\n    vector_scores = torch.matmul(\\n        encoder_state_vectors, query_vector.unsqueeze(dim=2)\\n    ).squeeze()\\n    vector_probabilities = F.softmax(vector_scores, dim=1)\\n    context_vectors = torch.matmul(\\n        encoder_state_vectors.transpose(-2, -1),\\n        vector_probabilities.unsqueeze(dim=2)\\n    ).squeeze()\\n    return context_vectors, vector_probabilities\";\n",
       "                var nbb_formatted_code = \"def verbose_attention(encoder_state_vectors, query_vector):\\n    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\\n    vector_scores = torch.sum(\\n        encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2\\n    )\\n    vector_probabilities = F.softmax(vector_scores, dim=1)\\n    weighted_vectors = encoder_state_vectors * vector_probabilities.view(\\n        batch_size, num_vectors, 1\\n    )\\n    context_vectors = torch.sum(weighted_vectors, dim=1)\\n    return context_vectors, vector_probabilities, vector_scores\\n\\n\\ndef terse_attention(encoder_state_vectors, query_vector):\\n    vector_scores = torch.matmul(\\n        encoder_state_vectors, query_vector.unsqueeze(dim=2)\\n    ).squeeze()\\n    vector_probabilities = F.softmax(vector_scores, dim=1)\\n    context_vectors = torch.matmul(\\n        encoder_state_vectors.transpose(-2, -1), vector_probabilities.unsqueeze(dim=2)\\n    ).squeeze()\\n    return context_vectors, vector_probabilities\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def verbose_attention(encoder_state_vectors, query_vector):\n",
    "    batch_size, num_vectors, vector_size = encoder_state_vectors.size()\n",
    "    vector_scores = torch.sum(\n",
    "        encoder_state_vectors * query_vector.view(batch_size, 1, vector_size), dim=2\n",
    "    )\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
    "    weighted_vectors = encoder_state_vectors * vector_probabilities.view(\n",
    "        batch_size, num_vectors, 1\n",
    "    )\n",
    "    context_vectors = torch.sum(weighted_vectors, dim=1)\n",
    "    return context_vectors, vector_probabilities, vector_scores\n",
    "\n",
    "\n",
    "def terse_attention(encoder_state_vectors, query_vector):\n",
    "    vector_scores = torch.matmul(\n",
    "        encoder_state_vectors, query_vector.unsqueeze(dim=2)\n",
    "    ).squeeze()\n",
    "    vector_probabilities = F.softmax(vector_scores, dim=1)\n",
    "    context_vectors = torch.matmul(\n",
    "        encoder_state_vectors.transpose(-2, -1), vector_probabilities.unsqueeze(dim=2)\n",
    "    ).squeeze()\n",
    "    return context_vectors, vector_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "206a7669",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"class NMTEncoder(nn.Module):\\n    def __init__(\\n        self, num_embeddings, embedding_size, rnn_hidden_size\\n    ):\\n        super(NMTEncoder, self).__init__()\\n        self.source_embedding = nn.Embedding(\\n            num_embeddings, embedding_size, padding_idx=0\\n        )\\n        self.birnn = nn.GRU(\\n            embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True\\n        )\\n    \\n    def forward(self, x_source, x_lengths):\\n        x_embedded = self.source_embedding(x_source)\\n        \\n        # Create PackedSequence, x_packed.data.shape = (number_items, embedding_size)\\n        x_packed = pack_padded_sequence(\\n            x_embedded, x_lengths.detach().cpu().numpy(),\\n            batch_first=True\\n        )\\n        \\n        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\\n        x_birnn_out, x_birnn_h = self.birnn(x_packed)\\n        \\n        # Permute to (batch_size, num_rnn, feature_size)\\n        x_birnn_h = x_birnn_h.permute(1, 0, 2)\\n        \\n        # Flatten Features: reshape to (batch_size, num_rnn * feature_size)\\n        x_birrn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\\n        x_unpacked, _ = pad_packed_sequence(\\n            x_birnn_out, batch_first=True\\n        )\\n        return x_unpacked, x_birnn_h\";\n",
       "                var nbb_formatted_code = \"class NMTEncoder(nn.Module):\\n    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\\n        super(NMTEncoder, self).__init__()\\n        self.source_embedding = nn.Embedding(\\n            num_embeddings, embedding_size, padding_idx=0\\n        )\\n        self.birnn = nn.GRU(\\n            embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True\\n        )\\n\\n    def forward(self, x_source, x_lengths):\\n        x_embedded = self.source_embedding(x_source)\\n\\n        # Create PackedSequence, x_packed.data.shape = (number_items, embedding_size)\\n        x_packed = pack_padded_sequence(\\n            x_embedded, x_lengths.detach().cpu().numpy(), batch_first=True\\n        )\\n\\n        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\\n        x_birnn_out, x_birnn_h = self.birnn(x_packed)\\n\\n        # Permute to (batch_size, num_rnn, feature_size)\\n        x_birnn_h = x_birnn_h.permute(1, 0, 2)\\n\\n        # Flatten Features: reshape to (batch_size, num_rnn * feature_size)\\n        x_birrn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\\n        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\\n        return x_unpacked, x_birnn_h\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMTEncoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size):\n",
    "        super(NMTEncoder, self).__init__()\n",
    "        self.source_embedding = nn.Embedding(\n",
    "            num_embeddings, embedding_size, padding_idx=0\n",
    "        )\n",
    "        self.birnn = nn.GRU(\n",
    "            embedding_size, rnn_hidden_size, bidirectional=True, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x_source, x_lengths):\n",
    "        x_embedded = self.source_embedding(x_source)\n",
    "\n",
    "        # Create PackedSequence, x_packed.data.shape = (number_items, embedding_size)\n",
    "        x_packed = pack_padded_sequence(\n",
    "            x_embedded, x_lengths.detach().cpu().numpy(), batch_first=True\n",
    "        )\n",
    "\n",
    "        # x_birnn_h.shape = (num_rnn, batch_size, feature_size)\n",
    "        x_birnn_out, x_birnn_h = self.birnn(x_packed)\n",
    "\n",
    "        # Permute to (batch_size, num_rnn, feature_size)\n",
    "        x_birnn_h = x_birnn_h.permute(1, 0, 2)\n",
    "\n",
    "        # Flatten Features: reshape to (batch_size, num_rnn * feature_size)\n",
    "        x_birrn_h = x_birnn_h.contiguous().view(x_birnn_h.size(0), -1)\n",
    "        x_unpacked, _ = pad_packed_sequence(x_birnn_out, batch_first=True)\n",
    "        return x_unpacked, x_birnn_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95920076",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"class NMTDecoder(nn.Module):\\n    def __init__(\\n        self,\\n        num_embeddings,\\n        embedding_size,\\n        rnn_hidden_size,\\n        bos_index\\n    ):\\n        super(NMTDecoder, self).__init__()\\n        self._rnn_hidden_size = rnn_hidden_size\\n        self.target_embedding = nn.Embedding(\\n            num_embeddings=num_embeddings,\\n            embedding_dim=embedding_size,\\n            padding_idx=0\\n        )\\n        self.gru_cell = nn.GRUCell(\\n            embedding_size + rnn_hidden_size,\\n            rnn_hidden_size\\n        )\\n        self.hidden_map = nn.Linear(\\n            rnn_hidden_size, rnn_hidden_size\\n        )\\n        self.classifier = nn.Linear(\\n            rnn_hidden_size * 2,\\n            num_embeddings\\n        )\\n        self.bos_index = bos_index\\n        \\n    def _init_indices(self, batch_size):\\n        return torch.ones(\\n            batch_size, dtype=torch.int64\\n        ) * self.bos_index\\n    \\n    def _init_context_vectors(self, batch_size):\\n        return torch.zeros(\\n            batch_size, self._rnn_hidden_size\\n        )\\n    \\n    def forward(\\n        self, encoder_state, initial_hidden_state, target_sequence\\n    ):\\n        target_sequence = target_sequence.permute(1, 0)\\n        output_sequence_size = target_sequence.size(0)\\n        \\n        # Use the provided encoder hidden state as the initial hidden state\\n        h_t = self.hidden_map(initial_hidden_state)\\n        \\n        batch_size = encoder_state.size(0)\\n        \\n        # Init context vectors to zeros\\n        context_vectors = self._init_context_vectors(batch_size)\\n        # Init first y_t word as BOS\\n        y_t_index = self._init_indices(batch_size)\\n        \\n        h_t = h_t.to(encoder_state.device)\\n        y_t_index = y_t_index.to(encoder_state.device)\\n        context_vectors = context_vectors.to(encoder_state.device)\\n        \\n        output_vectors. self._cached_p_attn, self._cached_ht = [], [], []\\n        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\\n        \\n        for i in range(output_sequence_size):\\n            y_t_index = target_sequence[i]\\n            \\n            # Step 1. Embed word and concat with previous context\\n            y_input_vector = self.target_embedding(y_t_index)\\n            rnn_input = torch.cat(\\n                [\\n                    y_input_vector, context_vectors\\n                ],\\n                dim=1\\n            )\\n            \\n            # Step 2: Make a GRU Step, getting a new hidden vector\\n            h_t = self.gru_cell(rnn_input, h_t)\\n            self._cached_ht.append(\\n                h_t.cpu().detach().numpy()\\n            )\\n            \\n            # Step 3: Use the current hidden to attend to the encoder state\\n            context_vectors, p_attn, _ = verbose_attention(\\n                encoder_state_vectors=encoder_state,\\n                query_vec=h_t\\n            )\\n            # Auxillary: Cache the attention probabilities for visualization\\n            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\\n            \\n            # Step 4: Use the current hidden and context vectors to make a prediction to the next word\\n            prediction_vector = torch.cat(\\n                (context_vectors, h_t),\\n                dim=1\\n            )\\n            score_for_y_t_index = self.classifier(\\n                F.dropout(prediction_vector, 0.3)\\n            )\\n            # Auxillary: Collect the prediction scores\\n            output_vectors.append(score_for_y_t_index)\\n        output_vectors = torch.stack(output_vectors).premute(1, 0, 2)\\n        return output_vectors\";\n",
       "                var nbb_formatted_code = \"class NMTDecoder(nn.Module):\\n    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\\n        super(NMTDecoder, self).__init__()\\n        self._rnn_hidden_size = rnn_hidden_size\\n        self.target_embedding = nn.Embedding(\\n            num_embeddings=num_embeddings, embedding_dim=embedding_size, padding_idx=0\\n        )\\n        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\\n        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\\n        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\\n        self.bos_index = bos_index\\n\\n    def _init_indices(self, batch_size):\\n        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\\n\\n    def _init_context_vectors(self, batch_size):\\n        return torch.zeros(batch_size, self._rnn_hidden_size)\\n\\n    def forward(self, encoder_state, initial_hidden_state, target_sequence):\\n        target_sequence = target_sequence.permute(1, 0)\\n        output_sequence_size = target_sequence.size(0)\\n\\n        # Use the provided encoder hidden state as the initial hidden state\\n        h_t = self.hidden_map(initial_hidden_state)\\n\\n        batch_size = encoder_state.size(0)\\n\\n        # Init context vectors to zeros\\n        context_vectors = self._init_context_vectors(batch_size)\\n        # Init first y_t word as BOS\\n        y_t_index = self._init_indices(batch_size)\\n\\n        h_t = h_t.to(encoder_state.device)\\n        y_t_index = y_t_index.to(encoder_state.device)\\n        context_vectors = context_vectors.to(encoder_state.device)\\n\\n        output_vectors.self._cached_p_attn, self._cached_ht = [], [], []\\n        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\\n\\n        for i in range(output_sequence_size):\\n            y_t_index = target_sequence[i]\\n\\n            # Step 1. Embed word and concat with previous context\\n            y_input_vector = self.target_embedding(y_t_index)\\n            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\\n\\n            # Step 2: Make a GRU Step, getting a new hidden vector\\n            h_t = self.gru_cell(rnn_input, h_t)\\n            self._cached_ht.append(h_t.cpu().detach().numpy())\\n\\n            # Step 3: Use the current hidden to attend to the encoder state\\n            context_vectors, p_attn, _ = verbose_attention(\\n                encoder_state_vectors=encoder_state, query_vec=h_t\\n            )\\n            # Auxillary: Cache the attention probabilities for visualization\\n            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\\n\\n            # Step 4: Use the current hidden and context vectors to make a prediction to the next word\\n            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\\n            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\\n            # Auxillary: Collect the prediction scores\\n            output_vectors.append(score_for_y_t_index)\\n        output_vectors = torch.stack(output_vectors).premute(1, 0, 2)\\n        return output_vectors\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMTDecoder(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_size, rnn_hidden_size, bos_index):\n",
    "        super(NMTDecoder, self).__init__()\n",
    "        self._rnn_hidden_size = rnn_hidden_size\n",
    "        self.target_embedding = nn.Embedding(\n",
    "            num_embeddings=num_embeddings, embedding_dim=embedding_size, padding_idx=0\n",
    "        )\n",
    "        self.gru_cell = nn.GRUCell(embedding_size + rnn_hidden_size, rnn_hidden_size)\n",
    "        self.hidden_map = nn.Linear(rnn_hidden_size, rnn_hidden_size)\n",
    "        self.classifier = nn.Linear(rnn_hidden_size * 2, num_embeddings)\n",
    "        self.bos_index = bos_index\n",
    "\n",
    "    def _init_indices(self, batch_size):\n",
    "        return torch.ones(batch_size, dtype=torch.int64) * self.bos_index\n",
    "\n",
    "    def _init_context_vectors(self, batch_size):\n",
    "        return torch.zeros(batch_size, self._rnn_hidden_size)\n",
    "\n",
    "    def forward(self, encoder_state, initial_hidden_state, target_sequence):\n",
    "        target_sequence = target_sequence.permute(1, 0)\n",
    "        output_sequence_size = target_sequence.size(0)\n",
    "\n",
    "        # Use the provided encoder hidden state as the initial hidden state\n",
    "        h_t = self.hidden_map(initial_hidden_state)\n",
    "\n",
    "        batch_size = encoder_state.size(0)\n",
    "\n",
    "        # Init context vectors to zeros\n",
    "        context_vectors = self._init_context_vectors(batch_size)\n",
    "        # Init first y_t word as BOS\n",
    "        y_t_index = self._init_indices(batch_size)\n",
    "\n",
    "        h_t = h_t.to(encoder_state.device)\n",
    "        y_t_index = y_t_index.to(encoder_state.device)\n",
    "        context_vectors = context_vectors.to(encoder_state.device)\n",
    "\n",
    "        output_vectors.self._cached_p_attn, self._cached_ht = [], [], []\n",
    "        self._cached_decoder_state = encoder_state.cpu().detach().numpy()\n",
    "\n",
    "        for i in range(output_sequence_size):\n",
    "            y_t_index = target_sequence[i]\n",
    "\n",
    "            # Step 1. Embed word and concat with previous context\n",
    "            y_input_vector = self.target_embedding(y_t_index)\n",
    "            rnn_input = torch.cat([y_input_vector, context_vectors], dim=1)\n",
    "\n",
    "            # Step 2: Make a GRU Step, getting a new hidden vector\n",
    "            h_t = self.gru_cell(rnn_input, h_t)\n",
    "            self._cached_ht.append(h_t.cpu().detach().numpy())\n",
    "\n",
    "            # Step 3: Use the current hidden to attend to the encoder state\n",
    "            context_vectors, p_attn, _ = verbose_attention(\n",
    "                encoder_state_vectors=encoder_state, query_vec=h_t\n",
    "            )\n",
    "            # Auxillary: Cache the attention probabilities for visualization\n",
    "            self._cached_p_attn.append(p_attn.cpu().detach().numpy())\n",
    "\n",
    "            # Step 4: Use the current hidden and context vectors to make a prediction to the next word\n",
    "            prediction_vector = torch.cat((context_vectors, h_t), dim=1)\n",
    "            score_for_y_t_index = self.classifier(F.dropout(prediction_vector, 0.3))\n",
    "            # Auxillary: Collect the prediction scores\n",
    "            output_vectors.append(score_for_y_t_index)\n",
    "        output_vectors = torch.stack(output_vectors).premute(1, 0, 2)\n",
    "        return output_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ac9d86",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 13;\n",
       "                var nbb_unformatted_code = \"class NMTModel(nn.Module):\\n    def __init__(\\n        self,\\n        source_vocab_size,\\n        source_embedding_size,\\n        target_vocab_size,\\n        target_embedding_size,\\n        encoding_size,\\n        target_bos_index\\n    ):\\n        super(NMTModel, self).__init__()\\n        self.encoder = NMTEncoder(\\n            num_embeddings=source_vocab_size,\\n            embedding_size=source_embedding_size,\\n            rnn_hidden_size=encoding_size\\n        )\\n        decoding_size = encoding_size * 2\\n        self.decoder = NMTDecoder(\\n            num_embeddings=target_vocab_size,\\n            embedding_size=target_embedding_size,\\n            rnn_hidden_size=decoding_size,\\n            bos_index=target_bos_index\\n        )\\n        \\n    def forward(self, x_source, x_source_lengths, target_sequence):\\n        encoder_state, final_hidden_states = self.encoder(\\n            x_source,\\n            x_source_lengths\\n        )\\n        decoded_states = self.decoder(\\n            encoder_state=encoder_state,\\n            initial_hidden_state=final_hidden_states,\\n            target_sequence=target_sequence\\n        )\\n        return decoded_states\";\n",
       "                var nbb_formatted_code = \"class NMTModel(nn.Module):\\n    def __init__(\\n        self,\\n        source_vocab_size,\\n        source_embedding_size,\\n        target_vocab_size,\\n        target_embedding_size,\\n        encoding_size,\\n        target_bos_index,\\n    ):\\n        super(NMTModel, self).__init__()\\n        self.encoder = NMTEncoder(\\n            num_embeddings=source_vocab_size,\\n            embedding_size=source_embedding_size,\\n            rnn_hidden_size=encoding_size,\\n        )\\n        decoding_size = encoding_size * 2\\n        self.decoder = NMTDecoder(\\n            num_embeddings=target_vocab_size,\\n            embedding_size=target_embedding_size,\\n            rnn_hidden_size=decoding_size,\\n            bos_index=target_bos_index,\\n        )\\n\\n    def forward(self, x_source, x_source_lengths, target_sequence):\\n        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\\n        decoded_states = self.decoder(\\n            encoder_state=encoder_state,\\n            initial_hidden_state=final_hidden_states,\\n            target_sequence=target_sequence,\\n        )\\n        return decoded_states\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class NMTModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        source_vocab_size,\n",
    "        source_embedding_size,\n",
    "        target_vocab_size,\n",
    "        target_embedding_size,\n",
    "        encoding_size,\n",
    "        target_bos_index,\n",
    "    ):\n",
    "        super(NMTModel, self).__init__()\n",
    "        self.encoder = NMTEncoder(\n",
    "            num_embeddings=source_vocab_size,\n",
    "            embedding_size=source_embedding_size,\n",
    "            rnn_hidden_size=encoding_size,\n",
    "        )\n",
    "        decoding_size = encoding_size * 2\n",
    "        self.decoder = NMTDecoder(\n",
    "            num_embeddings=target_vocab_size,\n",
    "            embedding_size=target_embedding_size,\n",
    "            rnn_hidden_size=decoding_size,\n",
    "            bos_index=target_bos_index,\n",
    "        )\n",
    "\n",
    "    def forward(self, x_source, x_source_lengths, target_sequence):\n",
    "        encoder_state, final_hidden_states = self.encoder(x_source, x_source_lengths)\n",
    "        decoded_states = self.decoder(\n",
    "            encoder_state=encoder_state,\n",
    "            initial_hidden_state=final_hidden_states,\n",
    "            target_sequence=target_sequence,\n",
    "        )\n",
    "        return decoded_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5860052",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Advanced Sequence Modeling for Natural Language Processing",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
