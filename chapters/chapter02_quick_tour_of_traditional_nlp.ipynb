{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc2a433",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#NLP-v/s-Computational-Linguistics\" data-toc-modified-id=\"NLP-v/s-Computational-Linguistics-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>NLP v/s Computational Linguistics</a></span></li><li><span><a href=\"#Corpora,-Tokens-&amp;-Types\" data-toc-modified-id=\"Corpora,-Tokens-&amp;-Types-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Corpora, Tokens &amp; Types</a></span></li><li><span><a href=\"#Unigrams,-Bigrams,-Trigrams,-...,-N-grams\" data-toc-modified-id=\"Unigrams,-Bigrams,-Trigrams,-...,-N-grams-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Unigrams, Bigrams, Trigrams, ..., N-grams</a></span></li><li><span><a href=\"#Lemmas-&amp;-Stems\" data-toc-modified-id=\"Lemmas-&amp;-Stems-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Lemmas &amp; Stems</a></span></li><li><span><a href=\"#Categorizing-Words:-POS-Tagging\" data-toc-modified-id=\"Categorizing-Words:-POS-Tagging-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Categorizing Words: POS Tagging</a></span></li><li><span><a href=\"#Categorizing-Spans:-Chunking-&amp;-Named-Entity-Recognition\" data-toc-modified-id=\"Categorizing-Spans:-Chunking-&amp;-Named-Entity-Recognition-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Categorizing Spans: Chunking &amp; Named Entity Recognition</a></span></li><li><span><a href=\"#Structure-of-Sentences\" data-toc-modified-id=\"Structure-of-Sentences-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Structure of Sentences</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfab5ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## NLP v/s Computational Linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769d349",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_NLP_ aims to develop methods for solving practical problems involving language such as information extraction, automatic speech recognition, machine translation, sentiment analysis, question answering and summarization. _Computational Linguistics(CL)_ on the other hand, employs computational methods to understand properties of human language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be365c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Corpora, Tokens & Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98936821",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- _Corpora_: Text Dataset used for NLP methods.\n",
    "- _Tokens_: Contiguous units of grouped characters.\n",
    "- _Instance or Data Point_: Text along with its metadata.\n",
    "- _Dataset_: Also known as corpora, a collection of instance.\n",
    "- _Tokenization_: Process of breaking a text down into tokens.\n",
    "- _Vocabulary/Lexicon_: Set of all types in a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c5e5b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"../images/figure_2_1.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ca4817",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 748 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.61.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: jinja2 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.5.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<4.0.0,>=2.2.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.0.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "311cfe4e",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['may', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Text using Spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"May, don't slap the green witch\"\n",
    "print(\n",
    "    [\n",
    "        str(token)\n",
    "        for token in nlp(text.lower())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc9f3d29",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['snow', 'white', 'and', 'the', 'seven', 'degrees', '#makeamoviecold', '@midnight', ':-)']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing Text using nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tweet = u\"Snow White and the Seven Degrees #MakeAMovieCold@midnight:-)\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(\n",
    "    tokenizer.tokenize(tweet.lower())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a23ab2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Unigrams, Bigrams, Trigrams, ..., N-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a144ec",
   "metadata": {
    "hidden": true
   },
   "source": [
    "_N-grams_ are fixed-length(n) consective token sequences occuring in the text. A bigram has two tokens, a unigram one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55e4b068",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', \"n't\"], [',', \"n't\", 'slap'], [\"n't\", 'slap', 'green'], ['slap', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "def n_grams(text, n):\n",
    "    return [\n",
    "        text[i:i+n] for i in range(len(text)-n+1)\n",
    "    ]\n",
    "\n",
    "cleaned = [\n",
    "    'mary', ',', \"n't\", 'slap', 'green', 'witch', '.'\n",
    "]\n",
    "print(n_grams(cleaned, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dffce0d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Lemmas & Stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c79a5a4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- _Lemmas_ are root forms of works. Process of reducing the dimensionality of vector representation by reducing tokens to their lemmas is called _lemmatization_.\n",
    "- _Stemming_ uses handcrafted rules to strip endings of words to reduce then to a common form called _stems_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6aa12cc",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he -----> he\n",
      "was -----> be\n",
      "running -----> run\n",
      "late -----> late\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"he was running late\")\n",
    "for token in doc:\n",
    "    print(f\"{token} -----> {token.lemma_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61d614c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Categorizing Words: POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e7fc974",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary ----> PROPN\n",
      "slapped ----> VERB\n",
      "the ----> DET\n",
      "green ----> ADJ\n",
      "witch ----> NOUN\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mary slapped the green witch\")\n",
    "for token in doc:\n",
    "    print(f\"{token} ----> {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b2c2c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Categorizing Spans: Chunking & Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659685f7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- _Shallow/Chuncking_ parsing aims to derive higher order units composed of the grammatical atoms, like nouns, verbs, adjectives and so on.\n",
    "- _Named Entity_ is a string mention of a real world concept like a person, location, organization, drug name etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6726c899",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary ----> NP\n",
      "the green witch ----> NP\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Mary slapped the green witch\")\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f\"{chunk} ----> {chunk.label_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfdc9ae",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Structure of Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a717dc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- _Parsing_ is identifying the relationship between phrasal units identified using Shallow Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8232da",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"../images/figure_2_2.png\" />\n",
    "<img src=\"../images/figure_2_3.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1869f0c8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
