{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b734a675",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vanilla Embedding based Tweet Classifier\n",
    "\n",
    "- `data/emb_results1.csv` -> 0.77076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168a1480",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import SequenceTweetDataset\\nfrom classifiers import TweetEmbeddingClassifier\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import SequenceTweetDataset\\nfrom classifiers import TweetEmbeddingClassifier\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import SequenceTweetDataset\n",
    "from classifiers import TweetEmbeddingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f97a1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/embeddings/document_classification/vectorizer.json\n",
      "\tmodels/embeddings/document_classification/model.pth\n",
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/document_classification\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/document_classification\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/embeddings/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=False,\n",
    "    embedding_size=100,\n",
    "    hidden_dim=100,\n",
    "    num_channels=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d8fac9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n",
      "TweetEmbeddingClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (convnet): Sequential(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/playground/NLP-with-PyTorch/projects/kaggle_nlp_with_disaster_tweets/utils.py:315: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(embedding_i)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"args.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\";\n",
       "                var nbb_formatted_code = \"args.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetEmbeddingClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    num_channels=args.num_channels,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1,\n",
    "    dropout_p=args.dropout_p,\n",
    "    pretrained_embeddings=embeddings,\n",
    "    padding_idx=0,\n",
    ")\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cff0a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee30d7e925942f1b32fd489e5b0ecc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182b4846c1c4588bd18c6d5e3acb491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a508319f2e44edf95294fda75b3cf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6367046709467724, Training Accuracy=61.985518292682926\n",
      "Validation Loss=0.4669395312666893, Validation Accuracy=79.296875.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.1959298821847613, Training Accuracy=92.98780487804879\n",
      "Validation Loss=0.6810704581439495, Validation Accuracy=77.1484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.17105375866337516, Training Accuracy=94.13109756097562\n",
      "Validation Loss=0.735496610403061, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.17115213322203335, Training Accuracy=94.16920731707317\n",
      "Validation Loss=0.7313713729381562, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.17088518200851072, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7399904876947403, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.17190431149267568, Training Accuracy=94.11204268292681\n",
      "Validation Loss=0.7698633819818497, Validation Accuracy=75.48828125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.17098327653437123, Training Accuracy=94.2454268292683\n",
      "Validation Loss=0.7172230966389179, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.17250303342574982, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7563951984047889, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.1699268078295196, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7499470338225365, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.1715263566229401, Training Accuracy=94.359756097561\n",
      "Validation Loss=0.6992455348372459, Validation Accuracy=77.9296875.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=78.02734375, Test Loss=0.7120643705129623.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"classifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_formatted_code = \"classifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c56b472d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 15;\n",
       "                var nbb_unformatted_code = \"def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\\n    vectorized_tweet = torch.tensor(\\n        vectorizer.vectorize(tweet, vector_length=max_length)\\n    )\\n    result = classifier(vectorized_tweet.unsqueeze(0))\\n    probability_value = F.sigmoid(result).item()\\n    predicted_index = 1 if probability_value >= decision_threshold else 0\\n    return vectorizer.target_vocab.lookup_index(predicted_index)\";\n",
       "                var nbb_formatted_code = \"def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\\n    vectorized_tweet = torch.tensor(\\n        vectorizer.vectorize(tweet, vector_length=max_length)\\n    )\\n    result = classifier(vectorized_tweet.unsqueeze(0))\\n    probability_value = F.sigmoid(result).item()\\n    predicted_index = 1 if probability_value >= decision_threshold else 0\\n    return vectorizer.target_vocab.lookup_index(predicted_index)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\n",
    "    vectorized_tweet = torch.tensor(\n",
    "        vectorizer.vectorize(tweet, vector_length=max_length)\n",
    "    )\n",
    "    result = classifier(vectorized_tweet.unsqueeze(0))\n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    predicted_index = 1 if probability_value >= decision_threshold else 0\n",
    "    return vectorizer.target_vocab.lookup_index(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f694499",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df6 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df6.to_csv(\\\"data/emb_results1.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df6 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df6.to_csv(\\\"data/emb_results1.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/emb_results1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa80c2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## With Full Dataset \n",
    "\n",
    "- `data/emb_results2.csv` -> 0.74256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4beb5996",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/embeddings/vectorizer.json\n",
      "\tmodels/embeddings/model.pth\n",
      "Using CUDA: False\n",
      "Using pre-trained embeddings\n",
      "TweetEmbeddingClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (convnet): Sequential(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23a9695dee948a19e8fe4438314f350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "069a3e39afab45b5bdaeb7551cf18036",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f38ea7456e4825b449ed9f8cb0a33e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=7613 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.5865282734571877, Training Accuracy=67.90254237288136\n",
      "Validation Loss=0.4292343817651272, Validation Accuracy=81.640625.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.19085996926335966, Training Accuracy=93.29978813559322\n",
      "Validation Loss=0.16570794209837916, Validation Accuracy=94.7265625.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.0916260357125331, Training Accuracy=96.82203389830508\n",
      "Validation Loss=0.09555885102599859, Validation Accuracy=96.77734375.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.052477165204236065, Training Accuracy=97.40466101694918\n",
      "Validation Loss=0.046623325906693935, Validation Accuracy=98.046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.04023859547293287, Training Accuracy=97.96080508474576\n",
      "Validation Loss=0.04067229013890028, Validation Accuracy=97.94921875.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.03670175115467382, Training Accuracy=98.11970338983048\n",
      "Validation Loss=0.03527743322774768, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.03689892452715312, Training Accuracy=98.14618644067798\n",
      "Validation Loss=0.03583157784305513, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.03733847165574968, Training Accuracy=98.04025423728814\n",
      "Validation Loss=0.030639207659987733, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.03680177028212002, Training Accuracy=98.11970338983049\n",
      "Validation Loss=0.03527746582403779, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.03688705330556732, Training Accuracy=98.07997881355931\n",
      "Validation Loss=0.03507911879569292, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=98.14453125, Test Loss=0.032497358624823384.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results2.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results2.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/embeddings/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=False,\n",
    "    embedding_size=100,\n",
    "    hidden_dim=100,\n",
    "    num_channels=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, use_full_dataset=True\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetEmbeddingClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    num_channels=args.num_channels,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1,\n",
    "    dropout_p=args.dropout_p,\n",
    "    pretrained_embeddings=embeddings,\n",
    "    padding_idx=0,\n",
    ")\n",
    "print(classifier)\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")\n",
    "\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/emb_results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0204131b",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## With 500 Epochs and Full Dataset\n",
    "\n",
    "- `data/emb_results3.csv` -> 0.74256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c87df60",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/embeddings/vectorizer.json\n",
      "\tmodels/embeddings/model.pth\n",
      "Using CUDA: False\n",
      "Using pre-trained embeddings\n",
      "TweetEmbeddingClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (convnet): Sequential(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a517250b5f4cc492e2503e2466b932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca403c635994807b4a696abf672d45b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddcca37ac744c75a984de094926a84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=7613 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.5865282734571877, Training Accuracy=67.90254237288136\n",
      "Validation Loss=0.4292343817651272, Validation Accuracy=81.640625.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.19085996926335966, Training Accuracy=93.29978813559322\n",
      "Validation Loss=0.16570794209837916, Validation Accuracy=94.7265625.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.0916260357125331, Training Accuracy=96.82203389830508\n",
      "Validation Loss=0.09555885102599859, Validation Accuracy=96.77734375.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.052477165204236065, Training Accuracy=97.40466101694918\n",
      "Validation Loss=0.046623325906693935, Validation Accuracy=98.046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.04023859547293287, Training Accuracy=97.96080508474576\n",
      "Validation Loss=0.04067229013890028, Validation Accuracy=97.94921875.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.03670175115467382, Training Accuracy=98.11970338983048\n",
      "Validation Loss=0.03527743322774768, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.03689892452715312, Training Accuracy=98.14618644067798\n",
      "Validation Loss=0.03583157784305513, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.03733847165574968, Training Accuracy=98.04025423728814\n",
      "Validation Loss=0.030639207659987733, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.03680177028212002, Training Accuracy=98.11970338983049\n",
      "Validation Loss=0.03527746582403779, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.03688705330556732, Training Accuracy=98.07997881355931\n",
      "Validation Loss=0.03507911879569292, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 100th Epoch Stats---------------\n",
      "Training Loss=0.03649966835470522, Training Accuracy=98.06673728813563\n",
      "Validation Loss=0.03502661339007318, Validation Accuracy=98.33984375000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 110th Epoch Stats---------------\n",
      "Training Loss=0.03614756018239057, Training Accuracy=98.14618644067795\n",
      "Validation Loss=0.03557096375152469, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 120th Epoch Stats---------------\n",
      "Training Loss=0.03657633360570013, Training Accuracy=98.10646186440678\n",
      "Validation Loss=0.03250232501886785, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 130th Epoch Stats---------------\n",
      "Training Loss=0.036116719561613206, Training Accuracy=98.17266949152544\n",
      "Validation Loss=0.03398288576863706, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 140th Epoch Stats---------------\n",
      "Training Loss=0.036119716636583955, Training Accuracy=98.1064618644068\n",
      "Validation Loss=0.03433646960183978, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 150th Epoch Stats---------------\n",
      "Training Loss=0.03677695194038294, Training Accuracy=98.14618644067798\n",
      "Validation Loss=0.0346192205324769, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 160th Epoch Stats---------------\n",
      "Training Loss=0.03663593590638395, Training Accuracy=98.19915254237287\n",
      "Validation Loss=0.03616444021463394, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 170th Epoch Stats---------------\n",
      "Training Loss=0.03657459348471741, Training Accuracy=98.13294491525424\n",
      "Validation Loss=0.037084251060150564, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 180th Epoch Stats---------------\n",
      "Training Loss=0.03646239738169489, Training Accuracy=98.18591101694913\n",
      "Validation Loss=0.03443435579538345, Validation Accuracy=98.33984375.\n",
      "------------------------------------------------------------\n",
      "--------------- 190th Epoch Stats---------------\n",
      "Training Loss=0.03664344071678944, Training Accuracy=98.07997881355934\n",
      "Validation Loss=0.03388117905706167, Validation Accuracy=98.73046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 200th Epoch Stats---------------\n",
      "Training Loss=0.03622318746648351, Training Accuracy=98.21239406779664\n",
      "Validation Loss=0.03422584384679794, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 210th Epoch Stats---------------\n",
      "Training Loss=0.03626089792509202, Training Accuracy=98.17266949152541\n",
      "Validation Loss=0.03571280220057815, Validation Accuracy=98.43749999999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 220th Epoch Stats---------------\n",
      "Training Loss=0.03639395618653399, Training Accuracy=98.17266949152541\n",
      "Validation Loss=0.034249551827088, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 230th Epoch Stats---------------\n",
      "Training Loss=0.037084403242600164, Training Accuracy=98.09322033898306\n",
      "Validation Loss=0.0315686478279531, Validation Accuracy=98.828125.\n",
      "------------------------------------------------------------\n",
      "--------------- 240th Epoch Stats---------------\n",
      "Training Loss=0.03670301100523291, Training Accuracy=98.1064618644068\n",
      "Validation Loss=0.03453801840078085, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 250th Epoch Stats---------------\n",
      "Training Loss=0.03728936593663894, Training Accuracy=98.04025423728812\n",
      "Validation Loss=0.03780386177822948, Validation Accuracy=97.94921875.\n",
      "------------------------------------------------------------\n",
      "--------------- 260th Epoch Stats---------------\n",
      "Training Loss=0.03620134665772823, Training Accuracy=98.09322033898303\n",
      "Validation Loss=0.03680333215743303, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 270th Epoch Stats---------------\n",
      "Training Loss=0.03682198852174364, Training Accuracy=98.10646186440675\n",
      "Validation Loss=0.03591960633639246, Validation Accuracy=98.14453125.\n",
      "------------------------------------------------------------\n",
      "--------------- 280th Epoch Stats---------------\n",
      "Training Loss=0.03629146273232113, Training Accuracy=98.15942796610172\n",
      "Validation Loss=0.03558932221494615, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 290th Epoch Stats---------------\n",
      "Training Loss=0.03617055250047628, Training Accuracy=98.1991525423729\n",
      "Validation Loss=0.034471016842871904, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 300th Epoch Stats---------------\n",
      "Training Loss=0.036353722928975866, Training Accuracy=98.18591101694916\n",
      "Validation Loss=0.03179912152700126, Validation Accuracy=98.43749999999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 310th Epoch Stats---------------\n",
      "Training Loss=0.036069530220228734, Training Accuracy=98.09322033898306\n",
      "Validation Loss=0.03556286601815373, Validation Accuracy=98.14453125.\n",
      "------------------------------------------------------------\n",
      "--------------- 320th Epoch Stats---------------\n",
      "Training Loss=0.03656582698478538, Training Accuracy=98.0799788135593\n",
      "Validation Loss=0.032511844765394926, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 330th Epoch Stats---------------\n",
      "Training Loss=0.037026325856351246, Training Accuracy=98.13294491525423\n",
      "Validation Loss=0.035025316290557384, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 340th Epoch Stats---------------\n",
      "Training Loss=0.03635038348640931, Training Accuracy=98.1329449152542\n",
      "Validation Loss=0.03445377014577389, Validation Accuracy=98.33984374999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 350th Epoch Stats---------------\n",
      "Training Loss=0.03532869914957023, Training Accuracy=98.3448093220339\n",
      "Validation Loss=0.03610576951177791, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 360th Epoch Stats---------------\n",
      "Training Loss=0.03592650601008939, Training Accuracy=98.23887711864406\n",
      "Validation Loss=0.03634186019189656, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 370th Epoch Stats---------------\n",
      "Training Loss=0.03608285858280071, Training Accuracy=98.23887711864407\n",
      "Validation Loss=0.03496144525706768, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 380th Epoch Stats---------------\n",
      "Training Loss=0.036718644036011695, Training Accuracy=98.06673728813558\n",
      "Validation Loss=0.035028314450755715, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 390th Epoch Stats---------------\n",
      "Training Loss=0.03660617669318188, Training Accuracy=98.11970338983052\n",
      "Validation Loss=0.036256383173167706, Validation Accuracy=98.43749999999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 400th Epoch Stats---------------\n",
      "Training Loss=0.03663740647262184, Training Accuracy=98.05349576271182\n",
      "Validation Loss=0.03397603752091527, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 410th Epoch Stats---------------\n",
      "Training Loss=0.036487357761963435, Training Accuracy=98.06673728813561\n",
      "Validation Loss=0.03303769673220813, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 420th Epoch Stats---------------\n",
      "Training Loss=0.036520671525622814, Training Accuracy=98.146186440678\n",
      "Validation Loss=0.033542979042977095, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 430th Epoch Stats---------------\n",
      "Training Loss=0.036630326132211136, Training Accuracy=98.14618644067798\n",
      "Validation Loss=0.034823542227968574, Validation Accuracy=98.14453125.\n",
      "------------------------------------------------------------\n",
      "--------------- 440th Epoch Stats---------------\n",
      "Training Loss=0.0365213858184673, Training Accuracy=98.13294491525424\n",
      "Validation Loss=0.03309651813469827, Validation Accuracy=98.53515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 450th Epoch Stats---------------\n",
      "Training Loss=0.03636520505898585, Training Accuracy=98.17266949152544\n",
      "Validation Loss=0.03336094331461936, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 460th Epoch Stats---------------\n",
      "Training Loss=0.036319727827873766, Training Accuracy=98.19915254237287\n",
      "Validation Loss=0.036207896890118725, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 470th Epoch Stats---------------\n",
      "Training Loss=0.036712145882706014, Training Accuracy=98.23887711864408\n",
      "Validation Loss=0.03402388049289584, Validation Accuracy=98.6328125.\n",
      "------------------------------------------------------------\n",
      "--------------- 480th Epoch Stats---------------\n",
      "Training Loss=0.03673572430260859, Training Accuracy=98.14618644067795\n",
      "Validation Loss=0.033617419889196754, Validation Accuracy=98.4375.\n",
      "------------------------------------------------------------\n",
      "--------------- 490th Epoch Stats---------------\n",
      "Training Loss=0.03630477568860783, Training Accuracy=98.21239406779661\n",
      "Validation Loss=0.03652320150285959, Validation Accuracy=98.2421875.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=98.046875, Test Loss=0.03182123409351334.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 21;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=500,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results3.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=500,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results3.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/embeddings/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=False,\n",
    "    embedding_size=100,\n",
    "    hidden_dim=100,\n",
    "    num_channels=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=500,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, use_full_dataset=True\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetEmbeddingClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    num_channels=args.num_channels,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1,\n",
    "    dropout_p=args.dropout_p,\n",
    "    pretrained_embeddings=embeddings,\n",
    "    padding_idx=0,\n",
    ")\n",
    "print(classifier)\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")\n",
    "\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/emb_results3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0b673",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## With 500 Epochs and Splitted Dataset\n",
    "\n",
    "- `data/emb_results4.csv` -> 0.76647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96908a01",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/embeddings/vectorizer.json\n",
      "\tmodels/embeddings/model.pth\n",
      "Using CUDA: False\n",
      "Using pre-trained embeddings\n",
      "TweetEmbeddingClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (convnet): Sequential(\n",
      "    (0): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (1): ELU(alpha=1.0)\n",
      "    (2): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (3): ELU(alpha=1.0)\n",
      "    (4): Conv1d(100, 100, kernel_size=(3,), stride=(2,))\n",
      "    (5): ELU(alpha=1.0)\n",
      "    (6): Conv1d(100, 100, kernel_size=(3,), stride=(1,))\n",
      "    (7): ELU(alpha=1.0)\n",
      "  )\n",
      "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9add2036ba4addbdfb83949d3d8617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24fd436252d42bbab2914a1b5e0e301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89ecc14dfcb4579885f59ed5301caab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6367046709467724, Training Accuracy=61.985518292682926\n",
      "Validation Loss=0.4669395312666893, Validation Accuracy=79.296875.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.1959298821847613, Training Accuracy=92.98780487804879\n",
      "Validation Loss=0.6810704581439495, Validation Accuracy=77.1484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.17105375866337516, Training Accuracy=94.13109756097562\n",
      "Validation Loss=0.735496610403061, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.17115213322203335, Training Accuracy=94.16920731707317\n",
      "Validation Loss=0.7313713729381562, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.17088518200851072, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7399904876947403, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.17190431149267568, Training Accuracy=94.11204268292681\n",
      "Validation Loss=0.7698633819818497, Validation Accuracy=75.48828125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.17098327653437123, Training Accuracy=94.2454268292683\n",
      "Validation Loss=0.7172230966389179, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.17250303342574982, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7563951984047889, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.1699268078295196, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7499470338225365, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.1715263566229401, Training Accuracy=94.359756097561\n",
      "Validation Loss=0.6992455348372459, Validation Accuracy=77.9296875.\n",
      "------------------------------------------------------------\n",
      "--------------- 100th Epoch Stats---------------\n",
      "Training Loss=0.16999141563002654, Training Accuracy=94.22637195121952\n",
      "Validation Loss=0.7462200224399567, Validation Accuracy=75.5859375.\n",
      "------------------------------------------------------------\n",
      "--------------- 110th Epoch Stats---------------\n",
      "Training Loss=0.17142729679258858, Training Accuracy=94.26448170731707\n",
      "Validation Loss=0.7439382672309874, Validation Accuracy=75.390625.\n",
      "------------------------------------------------------------\n",
      "--------------- 120th Epoch Stats---------------\n",
      "Training Loss=0.17209502436765806, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7300053685903549, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 130th Epoch Stats---------------\n",
      "Training Loss=0.17302574617106742, Training Accuracy=94.24542682926827\n",
      "Validation Loss=0.7479410022497177, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 140th Epoch Stats---------------\n",
      "Training Loss=0.17071426187346622, Training Accuracy=94.24542682926828\n",
      "Validation Loss=0.7427035868167877, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 150th Epoch Stats---------------\n",
      "Training Loss=0.16998902399365493, Training Accuracy=94.28353658536585\n",
      "Validation Loss=0.7509345784783363, Validation Accuracy=75.5859375.\n",
      "------------------------------------------------------------\n",
      "--------------- 160th Epoch Stats---------------\n",
      "Training Loss=0.17070322919909545, Training Accuracy=94.1310975609756\n",
      "Validation Loss=0.7584594190120697, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 170th Epoch Stats---------------\n",
      "Training Loss=0.17099920151437198, Training Accuracy=94.20731707317074\n",
      "Validation Loss=0.7599552050232887, Validation Accuracy=76.3671875.\n",
      "------------------------------------------------------------\n",
      "--------------- 180th Epoch Stats---------------\n",
      "Training Loss=0.17312143397767368, Training Accuracy=94.1501524390244\n",
      "Validation Loss=0.7294768169522285, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 190th Epoch Stats---------------\n",
      "Training Loss=0.17084464866940569, Training Accuracy=94.24542682926833\n",
      "Validation Loss=0.7476035431027412, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 200th Epoch Stats---------------\n",
      "Training Loss=0.17040690416243018, Training Accuracy=94.24542682926831\n",
      "Validation Loss=0.7207959368824959, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 210th Epoch Stats---------------\n",
      "Training Loss=0.17148651636955217, Training Accuracy=94.24542682926828\n",
      "Validation Loss=0.7458164393901825, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 220th Epoch Stats---------------\n",
      "Training Loss=0.1717224213771704, Training Accuracy=94.22637195121953\n",
      "Validation Loss=0.7760886251926422, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 230th Epoch Stats---------------\n",
      "Training Loss=0.17118022027539045, Training Accuracy=94.18826219512194\n",
      "Validation Loss=0.7308670282363892, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 240th Epoch Stats---------------\n",
      "Training Loss=0.16931591600906554, Training Accuracy=94.26448170731706\n",
      "Validation Loss=0.7172991447150708, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 250th Epoch Stats---------------\n",
      "Training Loss=0.1708874222708911, Training Accuracy=94.30259146341464\n",
      "Validation Loss=0.7666587606072426, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 260th Epoch Stats---------------\n",
      "Training Loss=0.17152319630471674, Training Accuracy=94.1310975609756\n",
      "Validation Loss=0.7556052505970001, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 270th Epoch Stats---------------\n",
      "Training Loss=0.17237811099465303, Training Accuracy=94.24542682926831\n",
      "Validation Loss=0.7264513894915581, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 280th Epoch Stats---------------\n",
      "Training Loss=0.17082880455546265, Training Accuracy=94.18826219512194\n",
      "Validation Loss=0.7118139155209064, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 290th Epoch Stats---------------\n",
      "Training Loss=0.16895928797198506, Training Accuracy=94.34070121951221\n",
      "Validation Loss=0.7517141997814178, Validation Accuracy=75.390625.\n",
      "------------------------------------------------------------\n",
      "--------------- 300th Epoch Stats---------------\n",
      "Training Loss=0.16830532052894917, Training Accuracy=94.37881097560971\n",
      "Validation Loss=0.7303918674588203, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 310th Epoch Stats---------------\n",
      "Training Loss=0.17186083263013413, Training Accuracy=94.20731707317071\n",
      "Validation Loss=0.7515888810157776, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 320th Epoch Stats---------------\n",
      "Training Loss=0.16972983319584917, Training Accuracy=94.28353658536587\n",
      "Validation Loss=0.7373735085129738, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 330th Epoch Stats---------------\n",
      "Training Loss=0.17132065118086046, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.7075831219553947, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 340th Epoch Stats---------------\n",
      "Training Loss=0.17175152861490478, Training Accuracy=94.26448170731709\n",
      "Validation Loss=0.7472766637802124, Validation Accuracy=75.29296875.\n",
      "------------------------------------------------------------\n",
      "--------------- 350th Epoch Stats---------------\n",
      "Training Loss=0.17294556483989812, Training Accuracy=94.43597560975611\n",
      "Validation Loss=0.7627811431884766, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 360th Epoch Stats---------------\n",
      "Training Loss=0.17077579785410954, Training Accuracy=94.2263719512195\n",
      "Validation Loss=0.7461343631148339, Validation Accuracy=76.56250000000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 370th Epoch Stats---------------\n",
      "Training Loss=0.17173901127605903, Training Accuracy=94.24542682926833\n",
      "Validation Loss=0.7451383545994759, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 380th Epoch Stats---------------\n",
      "Training Loss=0.1707779339900831, Training Accuracy=94.18826219512198\n",
      "Validation Loss=0.7417226284742355, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 390th Epoch Stats---------------\n",
      "Training Loss=0.17009561163623163, Training Accuracy=94.28353658536585\n",
      "Validation Loss=0.7440472766757011, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 400th Epoch Stats---------------\n",
      "Training Loss=0.16924647005592905, Training Accuracy=94.2454268292683\n",
      "Validation Loss=0.73076082020998, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 410th Epoch Stats---------------\n",
      "Training Loss=0.16909455726059472, Training Accuracy=94.22637195121952\n",
      "Validation Loss=0.7589662149548531, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 420th Epoch Stats---------------\n",
      "Training Loss=0.17071459842164344, Training Accuracy=94.35975609756096\n",
      "Validation Loss=0.7395319864153863, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 430th Epoch Stats---------------\n",
      "Training Loss=0.1721938893925853, Training Accuracy=94.22637195121952\n",
      "Validation Loss=0.7413509711623192, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 440th Epoch Stats---------------\n",
      "Training Loss=0.17168660679968395, Training Accuracy=94.24542682926827\n",
      "Validation Loss=0.7469621449708937, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 450th Epoch Stats---------------\n",
      "Training Loss=0.17119698044730394, Training Accuracy=94.24542682926833\n",
      "Validation Loss=0.7532069571316242, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 460th Epoch Stats---------------\n",
      "Training Loss=0.17202533854217064, Training Accuracy=94.20731707317073\n",
      "Validation Loss=0.7622229307889938, Validation Accuracy=75.78125.\n",
      "------------------------------------------------------------\n",
      "--------------- 470th Epoch Stats---------------\n",
      "Training Loss=0.1714823543116814, Training Accuracy=94.09298780487805\n",
      "Validation Loss=0.7686159387230873, Validation Accuracy=75.09765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 480th Epoch Stats---------------\n",
      "Training Loss=0.1722471330587457, Training Accuracy=94.20731707317077\n",
      "Validation Loss=0.7489457130432129, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 490th Epoch Stats---------------\n",
      "Training Loss=0.17255600450969322, Training Accuracy=94.09298780487805\n",
      "Validation Loss=0.7548147588968277, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=78.7109375, Test Loss=0.6690439954400063.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 23;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=500,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results4.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/embeddings/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=False,\\n    embedding_size=100,\\n    hidden_dim=100,\\n    num_channels=100,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=500,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\nargs.use_glove = True\\n\\nif args.reload_from_files:\\n    # training from a checkpoint\\n    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\nelse:\\n    # create dataset and vectorizer\\n    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\n# Use GloVe or randomly initialized embeddings\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetEmbeddingClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    num_channels=args.num_channels,\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1,\\n    dropout_p=args.dropout_p,\\n    pretrained_embeddings=embeddings,\\n    padding_idx=0,\\n)\\nprint(classifier)\\n\\nclassifier = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\\n\\ntest_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/emb_results4.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/embeddings/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=False,\n",
    "    embedding_size=100,\n",
    "    hidden_dim=100,\n",
    "    num_channels=100,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=500,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "args.use_glove = True\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, use_full_dataset=False\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "# Use GloVe or randomly initialized embeddings\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetEmbeddingClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    num_channels=args.num_channels,\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1,\n",
    "    dropout_p=args.dropout_p,\n",
    "    pretrained_embeddings=embeddings,\n",
    "    padding_idx=0,\n",
    ")\n",
    "print(classifier)\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")\n",
    "\n",
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/emb_results4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f820fe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607b36c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
