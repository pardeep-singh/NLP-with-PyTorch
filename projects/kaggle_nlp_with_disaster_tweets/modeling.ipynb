{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2ca0c8",
   "metadata": {},
   "source": [
    "## Classification using Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b53899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/perceptron/vectorizer.json\n",
      "\tmodels/perceptron/model.pth\n",
      "Using Cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetPerceptronClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetPerceptronClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetPerceptronClassifier\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/perceptron\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea52550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset & Creating vectorizer\n",
      "TweetPerceptronClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_formatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236835a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c115affb0b59411a9b51ff29bfb39e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a612ec01be4b4c759ce168809ed4d73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1778e745d584163a423a4023f1742c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.681884057638122, Training Accuracy=66.88262195121949\n",
      "Validation Loss=0.6708768680691719, Validation Accuracy=71.19140625.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.520560126479079, Training Accuracy=81.85975609756099\n",
      "Validation Loss=0.5529165044426918, Validation Accuracy=78.515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.447150328537313, Training Accuracy=84.71798780487804\n",
      "Validation Loss=0.5127294063568115, Validation Accuracy=79.58984374999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.4019845971247045, Training Accuracy=85.78506097560975\n",
      "Validation Loss=0.48284977674484253, Validation Accuracy=80.17578125.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.3730987455786729, Training Accuracy=86.85213414634147\n",
      "Validation Loss=0.4681103564798832, Validation Accuracy=81.25.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.3690585196018219, Training Accuracy=86.96646341463413\n",
      "Validation Loss=0.4662978574633598, Validation Accuracy=81.15234375.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.36998234870957164, Training Accuracy=86.94740853658537\n",
      "Validation Loss=0.47008950263261795, Validation Accuracy=80.76171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.36953318991312156, Training Accuracy=86.9474085365854\n",
      "Validation Loss=0.4734106995165348, Validation Accuracy=80.17578125000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.368781757063982, Training Accuracy=87.04268292682929\n",
      "Validation Loss=0.4650215320289135, Validation Accuracy=81.34765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.369591574116451, Training Accuracy=86.96646341463416\n",
      "Validation Loss=0.47686124965548515, Validation Accuracy=80.56640625.\n",
      "------------------------------------------------------------\n",
      "-------- Test Accuracy=81.0546875, Test Loss=0.45349790900945663.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"train_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"train_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff97517",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-74689a57acca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/playground/NLP-with-PyTorch/projects/kaggle_nlp_with_disaster_tweets/utils.py\u001b[0m in \u001b[0;36mpredict_class\u001b[0;34m(classifier, vectorizer, tweet, decision_threshold)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecision_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mtokenized_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mvectorized_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_tweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0mprobability_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/playground/NLP-with-PyTorch/projects/kaggle_nlp_with_disaster_tweets/vectorizer.py\u001b[0m in \u001b[0;36mvectorize\u001b[0;34m(self, tweet)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         \u001b[0mone_hot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTweetVectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_length_cutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mtoken_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlookup_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/playground/NLP-with-PyTorch/projects/kaggle_nlp_with_disaster_tweets/vectorizer.py\u001b[0m in \u001b[0;36mtokenizer\u001b[0;34m(tweet, token_length_cutoff)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \"\"\"\n\u001b[1;32m     40\u001b[0m         clean_tweet = p.clean(\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mtweet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"&amp;\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m         \u001b[0mtweet_without_stop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import utils\\n\\ntweet = \\\"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\\\"\\nutils.predict_class(classifier, dataset.get_vectorizer(), tweet)\";\n",
       "                var nbb_formatted_code = \"import utils\\n\\ntweet = \\\"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\\\"\\nutils.predict_class(classifier, dataset.get_vectorizer(), tweet)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "tweet = \"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\"\n",
    "utils.predict_class(classifier, dataset.get_vectorizer(), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b279d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
