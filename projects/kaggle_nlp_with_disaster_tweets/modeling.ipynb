{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2ca0c8",
   "metadata": {},
   "source": [
    "## Classification using Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91b53899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/perceptron/vectorizer.json\n",
      "\tmodels/perceptron/model.pth\n",
      "Using Cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetPerceptronClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetPerceptronClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetPerceptronClassifier\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/perceptron\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aea52550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset & Creating vectorizer\n",
      "TweetPerceptronClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_formatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236835a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363056bdafaf4c4d8699e606a7842c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc81b0c4825446a69cb7bc4765e69ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0629fb4f45ee44b58aa0ac76aaa1afad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.681884057638122, Training Accuracy=66.88262195121949\n",
      "Validation Loss=0.6708768680691719, Validation Accuracy=71.19140625.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.520560126479079, Training Accuracy=81.85975609756099\n",
      "Validation Loss=0.5529165044426918, Validation Accuracy=78.515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.447150328537313, Training Accuracy=84.71798780487804\n",
      "Validation Loss=0.5127294063568115, Validation Accuracy=79.58984374999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.4019845971247045, Training Accuracy=85.78506097560975\n",
      "Validation Loss=0.48284977674484253, Validation Accuracy=80.17578125.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.3730987455786729, Training Accuracy=86.85213414634147\n",
      "Validation Loss=0.4681103564798832, Validation Accuracy=81.25.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.3690585196018219, Training Accuracy=86.96646341463413\n",
      "Validation Loss=0.4662978574633598, Validation Accuracy=81.15234375.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.36998234870957164, Training Accuracy=86.94740853658537\n",
      "Validation Loss=0.47008950263261795, Validation Accuracy=80.76171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.36953318991312156, Training Accuracy=86.9474085365854\n",
      "Validation Loss=0.4734106995165348, Validation Accuracy=80.17578125000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.368781757063982, Training Accuracy=87.04268292682929\n",
      "Validation Loss=0.4650215320289135, Validation Accuracy=81.34765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.369591574116451, Training Accuracy=86.96646341463416\n",
      "Validation Loss=0.47686124965548515, Validation Accuracy=80.56640625.\n",
      "------------------------------------------------------------\n",
      "-------- Test Accuracy=81.0546875, Test Loss=0.45349790900945663.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"train_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"train_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff97517",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"import utils\\n\\ntweet = \\\"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\\\"\\nutils.predict_class(classifier, dataset.get_vectorizer(), tweet)\";\n",
       "                var nbb_formatted_code = \"import utils\\n\\ntweet = \\\"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\\\"\\nutils.predict_class(classifier, dataset.get_vectorizer(), tweet)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "tweet = \"The Campaign: Will Ferrell and Zach Galifianakis commit comic mayhem in this hilarious political farce. 4* http://t.co/tQ3j2qGtZQ'\"\n",
    "utils.predict_class(classifier, dataset.get_vectorizer(), tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f3c06c",
   "metadata": {},
   "source": [
    "### Prepare test dataset results\n",
    "\n",
    "- Got 0.78823 for perceptron_results.csv submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e848c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e043145d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        3263 non-null   int64 \n",
      " 1   keyword   3237 non-null   object\n",
      " 2   location  2158 non-null   object\n",
      " 3   text      3263 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 102.1+ KB\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"test_dataset.info()\";\n",
       "                var nbb_formatted_code = \"test_dataset.info()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e844b4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1], [2, 1], [3, 1], [9, 0], [11, 1], [12, 1], [21, 0], [22, 0], [27, 0], [29, 0]]\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 16;\n",
       "                var nbb_unformatted_code = \"results = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nprint(results[:10])\";\n",
       "                var nbb_formatted_code = \"results = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nprint(results[:10])\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "770912a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       0\n",
       "4  11       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 18;\n",
       "                var nbb_unformatted_code = \"submission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.head()\";\n",
       "                var nbb_formatted_code = \"submission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "698938c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 19;\n",
       "                var nbb_unformatted_code = \"submission_df.to_csv(\\\"data/perceptron_results.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"submission_df.to_csv(\\\"data/perceptron_results.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df.to_csv(\"data/perceptron_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f83219",
   "metadata": {},
   "source": [
    "### Train using full test dataset\n",
    "\n",
    "- 0.73184\n",
    "- 0.78363 (with scheduler fix)\n",
    "- 0.77658 (500 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d38006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/perceptron/vectorizer.json\n",
      "\tmodels/perceptron/model.pth\n",
      "Using Cuda: False\n",
      "Loading dataset & Creating vectorizer\n",
      "TweetPerceptronClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00f6e8f5412497ca8d9c389fbdfd81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bc7a327a8e40299e95a2bf4ebb4691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d0f33d13d4ab2a6e977beaefaa7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.681884057638122, Training Accuracy=66.88262195121949\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.5218462318908879, Training Accuracy=81.87881097560977\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.44711853236686894, Training Accuracy=84.56554878048779\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.4016107960445125, Training Accuracy=85.82317073170732\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.37020789341228766, Training Accuracy=86.7949695121951\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.3462637902759925, Training Accuracy=87.57621951219514\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.3272667854297451, Training Accuracy=88.24314024390246\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.31088968893376795, Training Accuracy=88.89100609756098\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.2984569763265005, Training Accuracy=89.40548780487805\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.2850437386006844, Training Accuracy=89.76753048780489\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 100th Epoch Stats---------------\n",
      "Training Loss=0.2744513637408978, Training Accuracy=90.09146341463416\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 110th Epoch Stats---------------\n",
      "Training Loss=0.26526290505397615, Training Accuracy=90.5297256097561\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 120th Epoch Stats---------------\n",
      "Training Loss=0.2562841562236227, Training Accuracy=90.77743902439026\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 130th Epoch Stats---------------\n",
      "Training Loss=0.2483042772950196, Training Accuracy=91.00609756097563\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 140th Epoch Stats---------------\n",
      "Training Loss=0.24194428942552423, Training Accuracy=91.33003048780489\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 150th Epoch Stats---------------\n",
      "Training Loss=0.23698138354755033, Training Accuracy=91.55868902439025\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 160th Epoch Stats---------------\n",
      "Training Loss=0.23564511937339133, Training Accuracy=91.63490853658537\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 170th Epoch Stats---------------\n",
      "Training Loss=0.23604706184166233, Training Accuracy=91.63490853658537\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 180th Epoch Stats---------------\n",
      "Training Loss=0.23580557871155622, Training Accuracy=91.65396341463415\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 190th Epoch Stats---------------\n",
      "Training Loss=0.2358438481644886, Training Accuracy=91.65396341463412\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 200th Epoch Stats---------------\n",
      "Training Loss=0.2357789009082608, Training Accuracy=91.65396341463416\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 210th Epoch Stats---------------\n",
      "Training Loss=0.2366318709966613, Training Accuracy=91.61585365853654\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 220th Epoch Stats---------------\n",
      "Training Loss=0.23560883468244137, Training Accuracy=91.74923780487805\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 230th Epoch Stats---------------\n",
      "Training Loss=0.23488248666612113, Training Accuracy=91.71112804878048\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 240th Epoch Stats---------------\n",
      "Training Loss=0.23624584006100166, Training Accuracy=91.6349085365854\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 250th Epoch Stats---------------\n",
      "Training Loss=0.23632586801924355, Training Accuracy=91.55868902439026\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 260th Epoch Stats---------------\n",
      "Training Loss=0.23535871905524552, Training Accuracy=91.65396341463418\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 270th Epoch Stats---------------\n",
      "Training Loss=0.2357648087710869, Training Accuracy=91.61585365853657\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 280th Epoch Stats---------------\n",
      "Training Loss=0.23543391794693172, Training Accuracy=91.69207317073169\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 290th Epoch Stats---------------\n",
      "Training Loss=0.2364151005337878, Training Accuracy=91.59679878048782\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 300th Epoch Stats---------------\n",
      "Training Loss=0.23615504401486093, Training Accuracy=91.59679878048779\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 310th Epoch Stats---------------\n",
      "Training Loss=0.2360448099491073, Training Accuracy=91.61585365853661\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 320th Epoch Stats---------------\n",
      "Training Loss=0.23681899505417522, Training Accuracy=91.57774390243905\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 330th Epoch Stats---------------\n",
      "Training Loss=0.2353984142948941, Training Accuracy=91.6920731707317\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 340th Epoch Stats---------------\n",
      "Training Loss=0.23487563976427406, Training Accuracy=91.6730182926829\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 350th Epoch Stats---------------\n",
      "Training Loss=0.23494378241097053, Training Accuracy=91.71112804878051\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 360th Epoch Stats---------------\n",
      "Training Loss=0.23613478134318097, Training Accuracy=91.59679878048779\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 370th Epoch Stats---------------\n",
      "Training Loss=0.23578875457368245, Training Accuracy=91.65396341463413\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 380th Epoch Stats---------------\n",
      "Training Loss=0.23535394414169034, Training Accuracy=91.67301829268295\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 390th Epoch Stats---------------\n",
      "Training Loss=0.23633681419419084, Training Accuracy=91.63490853658539\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 400th Epoch Stats---------------\n",
      "Training Loss=0.2357822969192412, Training Accuracy=91.71112804878052\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 410th Epoch Stats---------------\n",
      "Training Loss=0.23670431353696963, Training Accuracy=91.63490853658534\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 420th Epoch Stats---------------\n",
      "Training Loss=0.23565743173040993, Training Accuracy=91.63490853658537\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 430th Epoch Stats---------------\n",
      "Training Loss=0.23505403283165724, Training Accuracy=91.73018292682927\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 440th Epoch Stats---------------\n",
      "Training Loss=0.2357501274928814, Training Accuracy=91.65396341463418\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 450th Epoch Stats---------------\n",
      "Training Loss=0.2362863087072605, Training Accuracy=91.67301829268293\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 460th Epoch Stats---------------\n",
      "Training Loss=0.23562525975994952, Training Accuracy=91.73018292682927\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 470th Epoch Stats---------------\n",
      "Training Loss=0.23701223658352363, Training Accuracy=91.59679878048782\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 480th Epoch Stats---------------\n",
      "Training Loss=0.2356203743597356, Training Accuracy=91.6539634146341\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n",
      "--------------- 490th Epoch Stats---------------\n",
      "Training Loss=0.2363098594473629, Training Accuracy=91.61585365853657\n",
      "Validation Loss=0.0, Validation Accuracy=0.0.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=500,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/perceptron\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=500,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/perceptron\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=500,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetPerceptronClassifier(num_features=len(vectorizer.tweet_vocab))\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3270af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.head()\\nsubmission_df.to_csv(\\\"data/perceptron_results3.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.head()\\nsubmission_df.to_csv(\\\"data/perceptron_results3.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.head()\n",
    "submission_df.to_csv(\"data/perceptron_results3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdcc4e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"submission_df1 = pd.read_csv(\\\"data/perceptron_results.csv\\\")\\nsubmission_df2 = pd.read_csv(\\\"data/perceptron_results1.csv\\\")\\nsubmission_df3 = pd.read_csv(\\\"data/perceptron_results2.csv\\\")\\nsubmission_df4 = pd.read_csv(\\\"data/perceptron_results3.csv\\\")\";\n",
       "                var nbb_formatted_code = \"submission_df1 = pd.read_csv(\\\"data/perceptron_results.csv\\\")\\nsubmission_df2 = pd.read_csv(\\\"data/perceptron_results1.csv\\\")\\nsubmission_df3 = pd.read_csv(\\\"data/perceptron_results2.csv\\\")\\nsubmission_df4 = pd.read_csv(\\\"data/perceptron_results3.csv\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df1 = pd.read_csv(\"data/perceptron_results.csv\")\n",
    "submission_df2 = pd.read_csv(\"data/perceptron_results1.csv\")\n",
    "submission_df3 = pd.read_csv(\"data/perceptron_results2.csv\")\n",
    "submission_df4 = pd.read_csv(\"data/perceptron_results3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94137530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3263, 2), (2795, 2), (3130, 2), (2692, 2), (3065, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"submission_df1.shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df2[\\\"target\\\"]\\n].shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df3[\\\"target\\\"]\\n].shape, submission_df2[\\n    submission_df3[\\\"target\\\"] == submission_df2[\\\"target\\\"]\\n].shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df4[\\\"target\\\"]\\n].shape\";\n",
       "                var nbb_formatted_code = \"submission_df1.shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df2[\\\"target\\\"]\\n].shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df3[\\\"target\\\"]\\n].shape, submission_df2[\\n    submission_df3[\\\"target\\\"] == submission_df2[\\\"target\\\"]\\n].shape, submission_df1[\\n    submission_df1[\\\"target\\\"] == submission_df4[\\\"target\\\"]\\n].shape\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df1.shape, submission_df1[\n",
    "    submission_df1[\"target\"] == submission_df2[\"target\"]\n",
    "].shape, submission_df1[\n",
    "    submission_df1[\"target\"] == submission_df3[\"target\"]\n",
    "].shape, submission_df2[\n",
    "    submission_df3[\"target\"] == submission_df2[\"target\"]\n",
    "].shape, submission_df1[\n",
    "    submission_df1[\"target\"] == submission_df4[\"target\"]\n",
    "].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd553ba",
   "metadata": {},
   "source": [
    "## Classification using MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9329fb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/mlp/vectorizer.json\n",
      "\tmodels/mlp/model.pth\n",
      "Using Cuda: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetMLPClassifier\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/mlp\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    hidden_dim=300,\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "940ddaf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset & Creating vectorizer\n",
      "TweetMLPClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f808edc9674dcca92d3b87e29f5772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8081735b40491aa111dcec1f124dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7147d75f839046c8ad1245f4c38b07aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6277405648696712, Training Accuracy=69.68368902439026\n",
      "Validation Loss=0.5224693901836872, Validation Accuracy=79.98046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.16384193791848853, Training Accuracy=93.95960365853658\n",
      "Validation Loss=0.6141280904412268, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.158365460612425, Training Accuracy=94.28353658536585\n",
      "Validation Loss=0.6175562217831612, Validation Accuracy=77.05078125.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.15800704098329313, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6482495218515396, Validation Accuracy=75.78125.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.1579828738439374, Training Accuracy=94.3216463414634\n",
      "Validation Loss=0.6256633102893829, Validation Accuracy=76.85546875.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.15821252799615623, Training Accuracy=94.28353658536584\n",
      "Validation Loss=0.6164956092834473, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.15720444917678833, Training Accuracy=94.32164634146345\n",
      "Validation Loss=0.6420382857322694, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.15751778425239935, Training Accuracy=94.37881097560974\n",
      "Validation Loss=0.6267729736864567, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.1577825700727905, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6447095349431038, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.1572771870144984, Training Accuracy=94.39786585365854\n",
      "Validation Loss=0.6150166466832162, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "-------- Test Accuracy=78.125, Test Loss=0.5628506280481815.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab),\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"if args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab), hidden_dim=args.hidden_dim, output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetMLPClassifier(\n",
    "    input_dim=len(vectorizer.tweet_vocab),\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1\n",
    ")\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be11cf3b",
   "metadata": {},
   "source": [
    "### Prepare Submission CSV\n",
    "\n",
    "- `data/mlp_results1.csv` -> 0.77045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6908d61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df1 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df1.head()\\nsubmission_df1.to_csv(\\\"data/mlp_results1.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df1 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df1.head()\\nsubmission_df1.to_csv(\\\"data/mlp_results1.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "submission_df1 = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df1.to_csv(\"data/mlp_results1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab30f192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"submission_df1.head()\";\n",
       "                var nbb_formatted_code = \"submission_df1.head()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "faaf2a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2018\n",
       "1    1245\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"submission_df1[\\\"target\\\"].value_counts()\";\n",
       "                var nbb_formatted_code = \"submission_df1[\\\"target\\\"].value_counts()\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "submission_df1[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64e1de1",
   "metadata": {},
   "source": [
    "### Train Using Full Dataset\n",
    "\n",
    "- `data/mlp_results2.csv` -> 0.72939\n",
    "- `data/mlp_results3.csv` -> 0.74195 (Correct Full Train Dataset)\n",
    "- `mlp_results4.csv` -> 0.74992 (Full Dataset & Validation Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0825d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/mlp/vectorizer.json\n",
      "\tmodels/mlp/model.pth\n",
      "Using Cuda: False\n",
      "Loading dataset & Creating vectorizer\n",
      "TweetMLPClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf3885f2bf7f42f6ba0ecc14c774d8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a4d028e1314a4bac7907360eb8f7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "762a7edfc86e4293904aaa3e7f3b979b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=7613 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.5873162120075551, Training Accuracy=73.31832627118645\n",
      "Validation Loss=0.41285038366913795, Validation Accuracy=84.765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.15083907633009602, Training Accuracy=94.27966101694918\n",
      "Validation Loss=0.12037540879100561, Validation Accuracy=96.09375.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.08511996269226076, Training Accuracy=96.61016949152544\n",
      "Validation Loss=0.07143650902435182, Validation Accuracy=97.75390625.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.069224481486668, Training Accuracy=97.2457627118644\n",
      "Validation Loss=0.0609111855737865, Validation Accuracy=98.046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.067380387620148, Training Accuracy=97.45762711864406\n",
      "Validation Loss=0.06919081462547183, Validation Accuracy=97.55859375.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.06718702292290783, Training Accuracy=97.4708686440678\n",
      "Validation Loss=0.06366599537432194, Validation Accuracy=97.94921875.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.0667971701250743, Training Accuracy=97.45762711864401\n",
      "Validation Loss=0.06727890251204371, Validation Accuracy=97.65625.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.06673900555756132, Training Accuracy=97.4708686440678\n",
      "Validation Loss=0.06649489654228091, Validation Accuracy=97.8515625.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.06702217465992703, Training Accuracy=97.48411016949152\n",
      "Validation Loss=0.06476322188973427, Validation Accuracy=97.65625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.06748250347830483, Training Accuracy=97.4576271186441\n",
      "Validation Loss=0.0716186398640275, Validation Accuracy=97.4609375.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=97.94921875, Test Loss=0.06054385006427765.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab),\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab), hidden_dim=args.hidden_dim, output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetMLPClassifier\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/mlp\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    hidden_dim=300,\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file, use_full_dataset=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file, use_full_dataset=True\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetMLPClassifier(\n",
    "    input_dim=len(vectorizer.tweet_vocab),\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1\n",
    ")\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24d47130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df4 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df4.to_csv(\\\"data/mlp_results4.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df4 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df4.to_csv(\\\"data/mlp_results4.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "submission_df4 = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df4.to_csv(\"data/mlp_results4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc30d71b",
   "metadata": {},
   "source": [
    "### 500 Epochs with splitted dataset\n",
    "\n",
    "- `data/mlp_results5.csv` -> 0.77015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef733866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The nb_black extension is already loaded. To reload it, use:\n",
      "  %reload_ext nb_black\n",
      "Expanded filepaths: \n",
      "\tmodels/mlp/vectorizer.json\n",
      "\tmodels/mlp/model.pth\n",
      "Using Cuda: False\n",
      "Loading dataset & Creating vectorizer\n",
      "TweetMLPClassifier(\n",
      "  (fc1): Linear(in_features=3108, out_features=300, bias=True)\n",
      "  (fc2): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb03732bd0f4523970bfe074dc34ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b4f2e0841e4173b32444e00805daf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc424d7b24134e0bbbf1e33d8c2a1ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6277405648696712, Training Accuracy=69.68368902439026\n",
      "Validation Loss=0.5224693901836872, Validation Accuracy=79.98046875.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.16384193791848853, Training Accuracy=93.95960365853658\n",
      "Validation Loss=0.6141280904412268, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.158365460612425, Training Accuracy=94.28353658536585\n",
      "Validation Loss=0.6175562217831612, Validation Accuracy=77.05078125.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.15800704098329313, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6482495218515396, Validation Accuracy=75.78125.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.1579828738439374, Training Accuracy=94.3216463414634\n",
      "Validation Loss=0.6256633102893829, Validation Accuracy=76.85546875.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.15821252799615623, Training Accuracy=94.28353658536584\n",
      "Validation Loss=0.6164956092834473, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.15720444917678833, Training Accuracy=94.32164634146345\n",
      "Validation Loss=0.6420382857322694, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.15751778425239935, Training Accuracy=94.37881097560974\n",
      "Validation Loss=0.6267729736864567, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.1577825700727905, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6447095349431038, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.1572771870144984, Training Accuracy=94.39786585365854\n",
      "Validation Loss=0.6150166466832162, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 100th Epoch Stats---------------\n",
      "Training Loss=0.15910635107174156, Training Accuracy=94.28353658536587\n",
      "Validation Loss=0.6169542744755745, Validation Accuracy=77.1484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 110th Epoch Stats---------------\n",
      "Training Loss=0.1579540257410305, Training Accuracy=94.35975609756096\n",
      "Validation Loss=0.6194419972598553, Validation Accuracy=77.24609375.\n",
      "------------------------------------------------------------\n",
      "--------------- 120th Epoch Stats---------------\n",
      "Training Loss=0.1577549449554304, Training Accuracy=94.32164634146343\n",
      "Validation Loss=0.6326090544462204, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 130th Epoch Stats---------------\n",
      "Training Loss=0.1575067713856697, Training Accuracy=94.32164634146343\n",
      "Validation Loss=0.6328704580664635, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 140th Epoch Stats---------------\n",
      "Training Loss=0.15862968909304317, Training Accuracy=94.26448170731706\n",
      "Validation Loss=0.6180557906627655, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 150th Epoch Stats---------------\n",
      "Training Loss=0.15710175382654845, Training Accuracy=94.3407012195122\n",
      "Validation Loss=0.62770189717412, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 160th Epoch Stats---------------\n",
      "Training Loss=0.1585391114761189, Training Accuracy=94.28353658536587\n",
      "Validation Loss=0.6247134879231453, Validation Accuracy=76.85546875.\n",
      "------------------------------------------------------------\n",
      "--------------- 170th Epoch Stats---------------\n",
      "Training Loss=0.1572732222152919, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6395358592271805, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 180th Epoch Stats---------------\n",
      "Training Loss=0.1587455714257752, Training Accuracy=94.26448170731707\n",
      "Validation Loss=0.6414211615920067, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 190th Epoch Stats---------------\n",
      "Training Loss=0.15705751227896392, Training Accuracy=94.37881097560974\n",
      "Validation Loss=0.6353804022073746, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 200th Epoch Stats---------------\n",
      "Training Loss=0.15801615413369208, Training Accuracy=94.30259146341463\n",
      "Validation Loss=0.6363375559449196, Validation Accuracy=76.3671875.\n",
      "------------------------------------------------------------\n",
      "--------------- 210th Epoch Stats---------------\n",
      "Training Loss=0.1584381232174431, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6201266758143902, Validation Accuracy=76.85546875.\n",
      "------------------------------------------------------------\n",
      "--------------- 220th Epoch Stats---------------\n",
      "Training Loss=0.1584845788595153, Training Accuracy=94.28353658536581\n",
      "Validation Loss=0.6287946254014969, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 230th Epoch Stats---------------\n",
      "Training Loss=0.15788072747428242, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6281082890927792, Validation Accuracy=76.7578125.\n",
      "------------------------------------------------------------\n",
      "--------------- 240th Epoch Stats---------------\n",
      "Training Loss=0.15676340769703792, Training Accuracy=94.37881097560975\n",
      "Validation Loss=0.630128052085638, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 250th Epoch Stats---------------\n",
      "Training Loss=0.15720097393524354, Training Accuracy=94.35975609756096\n",
      "Validation Loss=0.6305380538105965, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 260th Epoch Stats---------------\n",
      "Training Loss=0.15816833951124332, Training Accuracy=94.3216463414634\n",
      "Validation Loss=0.6411700919270515, Validation Accuracy=75.68359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 270th Epoch Stats---------------\n",
      "Training Loss=0.1589787931340497, Training Accuracy=94.24542682926833\n",
      "Validation Loss=0.6302601099014282, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 280th Epoch Stats---------------\n",
      "Training Loss=0.15851147436514137, Training Accuracy=94.28353658536584\n",
      "Validation Loss=0.6495504975318909, Validation Accuracy=75.78125.\n",
      "------------------------------------------------------------\n",
      "--------------- 290th Epoch Stats---------------\n",
      "Training Loss=0.15888064990683298, Training Accuracy=94.26448170731705\n",
      "Validation Loss=0.624580018222332, Validation Accuracy=76.66015625000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 300th Epoch Stats---------------\n",
      "Training Loss=0.1578992221777032, Training Accuracy=94.34070121951218\n",
      "Validation Loss=0.6306543312966824, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 310th Epoch Stats---------------\n",
      "Training Loss=0.1585595527800118, Training Accuracy=94.28353658536588\n",
      "Validation Loss=0.6343980729579926, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 320th Epoch Stats---------------\n",
      "Training Loss=0.15816489716128604, Training Accuracy=94.30259146341461\n",
      "Validation Loss=0.6382190808653831, Validation Accuracy=75.78125.\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- 330th Epoch Stats---------------\n",
      "Training Loss=0.15779515755612675, Training Accuracy=94.34070121951218\n",
      "Validation Loss=0.6351474225521088, Validation Accuracy=75.87890625000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 340th Epoch Stats---------------\n",
      "Training Loss=0.1573032227958121, Training Accuracy=94.34070121951221\n",
      "Validation Loss=0.6388144046068192, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 350th Epoch Stats---------------\n",
      "Training Loss=0.15790210537067279, Training Accuracy=94.32164634146342\n",
      "Validation Loss=0.6140845566987991, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 360th Epoch Stats---------------\n",
      "Training Loss=0.15820107227418487, Training Accuracy=94.30259146341461\n",
      "Validation Loss=0.6285732388496399, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 370th Epoch Stats---------------\n",
      "Training Loss=0.15791712337877695, Training Accuracy=94.34070121951221\n",
      "Validation Loss=0.623885951936245, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 380th Epoch Stats---------------\n",
      "Training Loss=0.15733574785110427, Training Accuracy=94.35975609756102\n",
      "Validation Loss=0.6418654397130013, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 390th Epoch Stats---------------\n",
      "Training Loss=0.15711474146057922, Training Accuracy=94.32164634146343\n",
      "Validation Loss=0.6196785494685173, Validation Accuracy=76.953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 400th Epoch Stats---------------\n",
      "Training Loss=0.15844650730127238, Training Accuracy=94.26448170731706\n",
      "Validation Loss=0.6419351100921631, Validation Accuracy=76.07421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 410th Epoch Stats---------------\n",
      "Training Loss=0.15780405046009435, Training Accuracy=94.30259146341461\n",
      "Validation Loss=0.6266186125576496, Validation Accuracy=77.5390625.\n",
      "------------------------------------------------------------\n",
      "--------------- 420th Epoch Stats---------------\n",
      "Training Loss=0.1579184335906331, Training Accuracy=94.28353658536587\n",
      "Validation Loss=0.6148569509387016, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 430th Epoch Stats---------------\n",
      "Training Loss=0.15725788237845026, Training Accuracy=94.35975609756096\n",
      "Validation Loss=0.6407494656741619, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 440th Epoch Stats---------------\n",
      "Training Loss=0.15796183903769753, Training Accuracy=94.30259146341464\n",
      "Validation Loss=0.6143836453557014, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 450th Epoch Stats---------------\n",
      "Training Loss=0.15856255772637157, Training Accuracy=94.26448170731707\n",
      "Validation Loss=0.6257409118115902, Validation Accuracy=76.26953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 460th Epoch Stats---------------\n",
      "Training Loss=0.15683965076033668, Training Accuracy=94.37881097560978\n",
      "Validation Loss=0.6420887373387812, Validation Accuracy=75.9765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 470th Epoch Stats---------------\n",
      "Training Loss=0.1562770512409327, Training Accuracy=94.39786585365852\n",
      "Validation Loss=0.6335971280932425, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 480th Epoch Stats---------------\n",
      "Training Loss=0.15759893270527442, Training Accuracy=94.34070121951217\n",
      "Validation Loss=0.6163884475827217, Validation Accuracy=77.14843750000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 490th Epoch Stats---------------\n",
      "Training Loss=0.15675617154778504, Training Accuracy=94.39786585365853\n",
      "Validation Loss=0.6429769247770311, Validation Accuracy=75.78125000000001.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=78.515625, Test Loss=0.5530637390911578.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=500,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab),\\n    hidden_dim=args.hidden_dim,\\n    output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=500,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier(\\n    input_dim=len(vectorizer.tweet_vocab), hidden_dim=args.hidden_dim, output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetMLPClassifier\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/mlp\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    hidden_dim=300,\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=500,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetMLPClassifier(\n",
    "    input_dim=len(vectorizer.tweet_vocab),\n",
    "    hidden_dim=args.hidden_dim,\n",
    "    output_dim=1\n",
    ")\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08559c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df5 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df5.to_csv(\\\"data/mlp_results5.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df5 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df5.to_csv(\\\"data/mlp_results5.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "submission_df5 = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df5.to_csv(\"data/mlp_results5.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d733c",
   "metadata": {},
   "source": [
    "### 3 Layer MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e07292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/mlp/vectorizer.json\n",
      "\tmodels/mlp/model.pth\n",
      "Using Cuda: False\n",
      "Loading dataset & Creating vectorizer\n",
      "TweetMLPClassifier1(\n",
      "  (fc1): Linear(in_features=3108, out_features=1500, bias=True)\n",
      "  (fc2): Linear(in_features=1500, out_features=300, bias=True)\n",
      "  (fc3): Linear(in_features=300, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7a89a248b048ca8c7b79f4e6c05a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a41ba009a54ddebe366b373a76571a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c897c67f3d7f43c69dff9d36d4f75f60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.5541431540396154, Training Accuracy=72.25609756097563\n",
      "Validation Loss=0.4648152366280556, Validation Accuracy=79.58984375.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.0504790757124017, Training Accuracy=97.73246951219511\n",
      "Validation Loss=1.0860364064574244, Validation Accuracy=76.171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.0471876059863262, Training Accuracy=97.90396341463415\n",
      "Validation Loss=1.1171079427003863, Validation Accuracy=76.5625.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.047114420773052584, Training Accuracy=97.9420731707317\n",
      "Validation Loss=1.1491985395550728, Validation Accuracy=75.87890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.04723053025763209, Training Accuracy=97.90396341463413\n",
      "Validation Loss=1.0770857408642769, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.047316012199877217, Training Accuracy=97.9230182926829\n",
      "Validation Loss=1.1060209050774574, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.04743633505592986, Training Accuracy=97.90396341463412\n",
      "Validation Loss=1.144953869283199, Validation Accuracy=76.56250000000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.04710244750831186, Training Accuracy=97.92301829268294\n",
      "Validation Loss=1.1114583984017372, Validation Accuracy=76.66015625.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.047128911130130284, Training Accuracy=97.92301829268291\n",
      "Validation Loss=1.1224831268191338, Validation Accuracy=76.46484375.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.04673082694956442, Training Accuracy=97.94207317073172\n",
      "Validation Loss=1.096881479024887, Validation Accuracy=77.05078125.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=78.22265625, Test Loss=1.0008033737540245.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier, TweetMLPClassifier1\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim1=1500,\\n    hidden_dim2=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier1(\\n    input_dim=len(vectorizer.tweet_vocab),\\n    hidden_dim1=args.hidden_dim1,\\n    hidden_dim2=args.hidden_dim2,\\n    output_dim=1\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import TweetDataset\\nfrom classifiers import TweetMLPClassifier, TweetMLPClassifier1\\n\\nargs = Namespace(\\n    frequency_cutoff=25,\\n    model_state_file=\\\"model.pth\\\",\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    save_dir=\\\"models/mlp\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    hidden_dim1=1500,\\n    hidden_dim2=300,\\n    batch_size=128,\\n    early_stopping_criteria=5,\\n    learning_rate=0.001,\\n    num_epochs=100,\\n    seed=1337,\\n    catch_keyboard_interrupt=True,\\n    cuda=True,\\n    expand_filepaths_to_save_dir=True,\\n    reload_from_files=False,\\n)\\n\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n    print(\\\"Expanded filepaths: \\\")\\n    print(f\\\"\\\\t{args.vectorizer_file}\\\")\\n    print(f\\\"\\\\t{args.model_state_file}\\\")\\n\\n\\nif not torch.cuda.is_available():\\n    args.cuda = False\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\n\\nprint(f\\\"Using Cuda: {args.cuda}\\\")\\n\\nutils.set_seed_everywhere(args.seed, args.cuda)\\nutils.handle_dirs(args.save_dir)\\n\\nif args.reload_from_files:\\n    print(\\\"Loading Dataset & Vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_load_vectorizer(\\n        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\\n    )\\nelse:\\n    print(\\\"Loading dataset & Creating vectorizer\\\")\\n    dataset = TweetDataset.load_dataset_and_make_vectorizer(\\n        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\\n    )\\n    dataset.save_vectorizer(args.vectorizer_file)\\n    vectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetMLPClassifier1(\\n    input_dim=len(vectorizer.tweet_vocab),\\n    hidden_dim1=args.hidden_dim1,\\n    hidden_dim2=args.hidden_dim2,\\n    output_dim=1,\\n)\\nprint(classifier)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\ntrain_state = utils.train_model(\\n    classifier, loss_func, optimizer, scheduler, dataset, args\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier, dataset, loss_func, train_state, args\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import TweetDataset\n",
    "from classifiers import TweetMLPClassifier, TweetMLPClassifier1\n",
    "\n",
    "args = Namespace(\n",
    "    frequency_cutoff=25,\n",
    "    model_state_file=\"model.pth\",\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    save_dir=\"models/mlp\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    hidden_dim1=1500,\n",
    "    hidden_dim2=300,\n",
    "    batch_size=128,\n",
    "    early_stopping_criteria=5,\n",
    "    learning_rate=0.001,\n",
    "    num_epochs=100,\n",
    "    seed=1337,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(f\"\\t{args.vectorizer_file}\")\n",
    "    print(f\"\\t{args.model_state_file}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(f\"Using Cuda: {args.cuda}\")\n",
    "\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading Dataset & Vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_load_vectorizer(\n",
    "        args.tweets_csv, args.vectrozier_file, use_full_dataset=False\n",
    "    )\n",
    "else:\n",
    "    print(\"Loading dataset & Creating vectorizer\")\n",
    "    dataset = TweetDataset.load_dataset_and_make_vectorizer(\n",
    "        args.tweets_csv, args.vectorizer_file, use_full_dataset=False\n",
    "    )\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "    vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetMLPClassifier1(\n",
    "    input_dim=len(vectorizer.tweet_vocab),\n",
    "    hidden_dim1=args.hidden_dim1,\n",
    "    hidden_dim2=args.hidden_dim2,\n",
    "    output_dim=1\n",
    ")\n",
    "print(classifier)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "train_state = utils.train_model(\n",
    "    classifier, loss_func, optimizer, scheduler, dataset, args\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier, dataset, loss_func, train_state, args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839fd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df6 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df6.to_csv(\\\"data/mlp_results6.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\\n    results.append([id, prediction])\\nsubmission_df6 = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df6.to_csv(\\\"data/mlp_results6.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = utils.predict_class(classifier, dataset.get_vectorizer(), tweet)\n",
    "    results.append([id, prediction])\n",
    "submission_df6 = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df6.to_csv(\"data/mlp_results6.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e5957f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
