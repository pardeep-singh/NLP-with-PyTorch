{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b734a675",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Vanilla RNN based Tweet Classifier\n",
    "\n",
    "- `data/simple_rnn1_results1.csv` -> 0.57033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "168a1480",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import SequenceTweetDataset\\nfrom classifiers import TweetSimpleRNNClassifier\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n\\nimport collections\\nimport numpy as np\\nimport pandas as pd\\nimport re\\nfrom argparse import Namespace\\nimport os\\n\\nimport torch.optim as optim\\nimport torch\\nimport torch.nn as nn\\nimport torch.nn.functional as F\\n\\nimport utils\\nfrom dataset import SequenceTweetDataset\\nfrom classifiers import TweetSimpleRNNClassifier\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from argparse import Namespace\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import utils\n",
    "from dataset import SequenceTweetDataset\n",
    "from classifiers import TweetSimpleRNNClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f97a1d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/simple_rnn/vectorizer.json\n",
      "\tmodels/simple_rnn/model.pth\n",
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/simple_rnn/\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=100,\n",
    "    rnn_hidden_size=64,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d8fac9",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TweetSimpleRNNClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (rnn): ElmanRNN(\n",
      "    (rnn_cell): RNNCell(100, 64)\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_formatted_code = \"dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = TweetSimpleRNNClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    output_dim=1,\n",
    "    rnn_hidden_size=args.rnn_hidden_size,\n",
    "    padding_idx=vectorizer.tweet_vocab.mask_index,\n",
    ")\n",
    "print(classifier)\n",
    "classifer = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cff0a1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34b47ba8a0d4132827376f1ea34da67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bfdd65b53c64c34855e99f0bbe20764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eb39427528a440bb60b7aeb3ad2c02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6882142002989605, Training Accuracy=55.64024390243902\n",
      "Validation Loss=0.6843034774065018, Validation Accuracy=57.51953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.6846244989371886, Training Accuracy=56.97408536585366\n",
      "Validation Loss=0.6844219416379929, Validation Accuracy=56.93359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.68484344424271, Training Accuracy=56.99314024390243\n",
      "Validation Loss=0.6834568679332733, Validation Accuracy=57.51953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.6843078936018597, Training Accuracy=56.99314024390243\n",
      "Validation Loss=0.6858321875333786, Validation Accuracy=56.83593750000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.6845289817670496, Training Accuracy=56.87881097560975\n",
      "Validation Loss=0.6820941716432571, Validation Accuracy=56.73828125.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.6844771126421488, Training Accuracy=57.14557926829268\n",
      "Validation Loss=0.6825616210699081, Validation Accuracy=57.51953125.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.6837638122279468, Training Accuracy=57.06935975609757\n",
      "Validation Loss=0.6820853352546692, Validation Accuracy=57.6171875.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.6837822518697597, Training Accuracy=57.08841463414633\n",
      "Validation Loss=0.6851489394903183, Validation Accuracy=56.93359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.6843774420459097, Training Accuracy=57.06935975609756\n",
      "Validation Loss=0.6843120753765106, Validation Accuracy=56.73828124999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.6839916051887885, Training Accuracy=57.05030487804878\n",
      "Validation Loss=0.681877039372921, Validation Accuracy=57.51953125.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=56.73828125000001, Test Loss=0.6842761337757111.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"train_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_formatted_code = \"train_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56b472d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\\n    vectorized_tweet = torch.tensor(\\n        vectorizer.vectorize(tweet, vector_length=max_length)\\n    )\\n    result = classifier(vectorized_tweet.unsqueeze(0))\\n    probability_value = F.sigmoid(result).item()\\n    predicted_index = 1 if probability_value >= decision_threshold else 0\\n    return vectorizer.target_vocab.lookup_index(predicted_index)\";\n",
       "                var nbb_formatted_code = \"def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\\n    vectorized_tweet = torch.tensor(\\n        vectorizer.vectorize(tweet, vector_length=max_length)\\n    )\\n    result = classifier(vectorized_tweet.unsqueeze(0))\\n    probability_value = F.sigmoid(result).item()\\n    predicted_index = 1 if probability_value >= decision_threshold else 0\\n    return vectorizer.target_vocab.lookup_index(predicted_index)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def predict_class(classifier, vectorizer, tweet, max_length, decision_threshold=0.5):\n",
    "    vectorized_tweet = torch.tensor(\n",
    "        vectorizer.vectorize(tweet, vector_length=max_length)\n",
    "    )\n",
    "    result = classifier(vectorized_tweet.unsqueeze(0))\n",
    "    probability_value = F.sigmoid(result).item()\n",
    "    predicted_index = 1 if probability_value >= decision_threshold else 0\n",
    "    return vectorizer.target_vocab.lookup_index(predicted_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f694499",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/anaconda3/envs/nlp-with-pytorch/lib/python3.8/site-packages/torch/nn/functional.py:1709: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/simple_rnn1_results1.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/simple_rnn1_results1.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/simple_rnn1_results1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec6331",
   "metadata": {},
   "source": [
    "## RNN Classifier with Pretrained Embeddings\n",
    "\n",
    "- `data/simple_rnn1_results2.csv` -> 0.57033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0607b36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/simple_rnn/vectorizer.json\n",
      "\tmodels/simple_rnn/model.pth\n",
      "Using CUDA: False\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=True,\\n    embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=True,\\n    embedding_size=100,\\n    rnn_hidden_size=64,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/simple_rnn/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=True,\n",
    "    embedding_size=100,\n",
    "    rnn_hidden_size=64,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e0e8c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using pre-trained embeddings\n",
      "TweetSimpleRNNClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (rnn): ElmanRNN(\n",
      "    (rnn_cell): RNNCell(100, 64)\n",
      "  )\n",
      "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardeep/playground/NLP-with-PyTorch/projects/kaggle_nlp_with_disaster_tweets/utils.py:330: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  torch.nn.init.xavier_uniform(embedding_i)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n    pretrained_embeddings=embeddings,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_formatted_code = \"dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n    pretrained_embeddings=embeddings,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetSimpleRNNClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    output_dim=1,\n",
    "    rnn_hidden_size=args.rnn_hidden_size,\n",
    "    padding_idx=vectorizer.tweet_vocab.mask_index,\n",
    "    pretrained_embeddings=embeddings,\n",
    ")\n",
    "print(classifier)\n",
    "classifer = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebc2e5d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50134c5c3a5b4251909e8947af736032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4623e99a887641ea9728f1ef2a696562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1424cd376f46be9836cba4d11582e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6868613347774598, Training Accuracy=55.94512195121953\n",
      "Validation Loss=0.684223160147667, Validation Accuracy=56.73828125.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.6841114933897811, Training Accuracy=57.05030487804878\n",
      "Validation Loss=0.6875626444816589, Validation Accuracy=57.32421875000001.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.6846795401922088, Training Accuracy=56.9169207317073\n",
      "Validation Loss=0.6852826178073883, Validation Accuracy=56.4453125.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.684196668427165, Training Accuracy=57.01219512195122\n",
      "Validation Loss=0.6853861138224602, Validation Accuracy=56.93359374999999.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.6848697255297405, Training Accuracy=57.03124999999999\n",
      "Validation Loss=0.6838872507214545, Validation Accuracy=57.12890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.6843616729829369, Training Accuracy=56.9359756097561\n",
      "Validation Loss=0.6860746219754219, Validation Accuracy=56.15234375.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.6849194154506778, Training Accuracy=57.050304878048784\n",
      "Validation Loss=0.6813258230686189, Validation Accuracy=57.8125.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.683900617971653, Training Accuracy=56.993140243902445\n",
      "Validation Loss=0.6844173446297646, Validation Accuracy=56.8359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.6835934010947624, Training Accuracy=57.107469512195124\n",
      "Validation Loss=0.6839258447289467, Validation Accuracy=57.2265625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.6846255834509687, Training Accuracy=56.87881097560976\n",
      "Validation Loss=0.6831338182091713, Validation Accuracy=56.93359375.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=57.8125, Test Loss=0.6807447448372841.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"train_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_formatted_code = \"train_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f2840d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/simple_rnn1_results2.csv\\\", index=False)\";\n",
       "                var nbb_formatted_code = \"test_dataset = pd.read_csv(\\\"data/test.csv\\\")\\nresults = []\\nfor id, _, _, tweet in test_dataset.values:\\n    prediction = predict_class(\\n        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\\n    )\\n    results.append([id, prediction])\\nsubmission_df = pd.DataFrame(results, columns=[\\\"id\\\", \\\"target\\\"])\\nsubmission_df.to_csv(\\\"data/simple_rnn1_results2.csv\\\", index=False)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv(\"data/test.csv\")\n",
    "results = []\n",
    "for id, _, _, tweet in test_dataset.values:\n",
    "    prediction = predict_class(\n",
    "        classifier, dataset.get_vectorizer(), tweet, dataset._max_seq_length + 1\n",
    "    )\n",
    "    results.append([id, prediction])\n",
    "submission_df = pd.DataFrame(results, columns=[\"id\", \"target\"])\n",
    "submission_df.to_csv(\"data/simple_rnn1_results2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a5b4e1",
   "metadata": {},
   "source": [
    "## RNN with Pre Trained Classifier and 128 hidden unit size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a228a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodels/simple_rnn/vectorizer.json\n",
      "\tmodels/simple_rnn/model.pth\n",
      "Using CUDA: False\n",
      "Using pre-trained embeddings\n",
      "TweetSimpleRNNClassifier(\n",
      "  (emb): Embedding(3111, 100, padding_idx=0)\n",
      "  (rnn): ElmanRNN(\n",
      "    (rnn_cell): RNNCell(100, 128)\n",
      "  )\n",
      "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a259d0bcf745868e2be56e721f9e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Routine:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7479b9ba40854048afb49d0554f93cbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa9177a3da94ee9a9540b5ca8d33e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Split=train, Size=5331 ============\n",
      "============ Split=val, Size=1141 ============\n",
      "--------------- 0th Epoch Stats---------------\n",
      "Training Loss=0.6868588691804466, Training Accuracy=56.15472560975609\n",
      "Validation Loss=0.6846114024519919, Validation Accuracy=57.32421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 10th Epoch Stats---------------\n",
      "Training Loss=0.684220289311758, Training Accuracy=56.97408536585365\n",
      "Validation Loss=0.6833800822496414, Validation Accuracy=57.32421875.\n",
      "------------------------------------------------------------\n",
      "--------------- 20th Epoch Stats---------------\n",
      "Training Loss=0.6830715551608947, Training Accuracy=57.10746951219512\n",
      "Validation Loss=0.686154693365097, Validation Accuracy=56.8359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 30th Epoch Stats---------------\n",
      "Training Loss=0.6831843634931053, Training Accuracy=57.1455792682927\n",
      "Validation Loss=0.6875371411442757, Validation Accuracy=56.34765625.\n",
      "------------------------------------------------------------\n",
      "--------------- 40th Epoch Stats---------------\n",
      "Training Loss=0.6837428735523691, Training Accuracy=57.10746951219514\n",
      "Validation Loss=0.6827304884791374, Validation Accuracy=57.8125.\n",
      "------------------------------------------------------------\n",
      "--------------- 50th Epoch Stats---------------\n",
      "Training Loss=0.683761987744308, Training Accuracy=57.01219512195122\n",
      "Validation Loss=0.6856632232666016, Validation Accuracy=56.640625.\n",
      "------------------------------------------------------------\n",
      "--------------- 60th Epoch Stats---------------\n",
      "Training Loss=0.6843634538534211, Training Accuracy=57.03125\n",
      "Validation Loss=0.6822871193289757, Validation Accuracy=58.0078125.\n",
      "------------------------------------------------------------\n",
      "--------------- 70th Epoch Stats---------------\n",
      "Training Loss=0.6847556102566604, Training Accuracy=56.97408536585366\n",
      "Validation Loss=0.6858493909239768, Validation Accuracy=56.93359375.\n",
      "------------------------------------------------------------\n",
      "--------------- 80th Epoch Stats---------------\n",
      "Training Loss=0.6845949946380244, Training Accuracy=56.993140243902445\n",
      "Validation Loss=0.6833294034004211, Validation Accuracy=57.12890625.\n",
      "------------------------------------------------------------\n",
      "--------------- 90th Epoch Stats---------------\n",
      "Training Loss=0.684783234828856, Training Accuracy=57.0503048780488\n",
      "Validation Loss=0.6842780858278276, Validation Accuracy=56.0546875.\n",
      "------------------------------------------------------------\n",
      "============ Split=test, Size=1141 ============\n",
      "-------- Test Accuracy=57.6171875, Test Loss=0.6816476285457611.--------\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=True,\\n    embedding_size=100,\\n    rnn_hidden_size=128,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\ndataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n    pretrained_embeddings=embeddings,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\n\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_formatted_code = \"args = Namespace(\\n    # Data and Path hyper parameters\\n    tweets_csv=\\\"data/train_with_splits.csv\\\",\\n    vectorizer_file=\\\"vectorizer.json\\\",\\n    model_state_file=\\\"model.pth\\\",\\n    save_dir=\\\"models/simple_rnn/\\\",\\n    # Model hyper parameters\\n    glove_filepath=\\\"../../data/glove.6B.100d.txt\\\",\\n    use_glove=True,\\n    embedding_size=100,\\n    rnn_hidden_size=128,\\n    # Training hyper parameter\\n    seed=1337,\\n    learning_rate=0.001,\\n    dropout_p=0.1,\\n    batch_size=128,\\n    num_epochs=100,\\n    early_stopping_criteria=5,\\n    # Runtime option\\n    cuda=True,\\n    catch_keyboard_interrupt=True,\\n    reload_from_files=False,\\n    expand_filepaths_to_save_dir=True,\\n)\\n\\nif args.expand_filepaths_to_save_dir:\\n    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\\n\\n    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\\n\\n    print(\\\"Expanded filepaths: \\\")\\n    print(\\\"\\\\t{}\\\".format(args.vectorizer_file))\\n    print(\\\"\\\\t{}\\\".format(args.model_state_file))\\n\\n# Check CUDA\\nif not torch.cuda.is_available():\\n    args.cuda = False\\n\\nargs.device = torch.device(\\\"cuda\\\" if args.cuda else \\\"cpu\\\")\\nprint(\\\"Using CUDA: {}\\\".format(args.cuda))\\n\\n# Set seed for reproducibility\\nutils.set_seed_everywhere(args.seed, args.cuda)\\n\\n# handle dirs\\nutils.handle_dirs(args.save_dir)\\n\\ndataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\\ndataset.save_vectorizer(args.vectorizer_file)\\nvectorizer = dataset.get_vectorizer()\\n\\nif args.use_glove:\\n    words = vectorizer.tweet_vocab._token_to_idx.keys()\\n    embeddings = utils.make_embedding_matrix(\\n        glove_filepath=args.glove_filepath, words=words\\n    )\\n    print(\\\"Using pre-trained embeddings\\\")\\nelse:\\n    print(\\\"Not using pre-trained embeddings\\\")\\n    embeddings = None\\n\\nclassifier = TweetSimpleRNNClassifier(\\n    embedding_size=args.embedding_size,\\n    num_embeddings=len(vectorizer.tweet_vocab),\\n    output_dim=1,\\n    rnn_hidden_size=args.rnn_hidden_size,\\n    padding_idx=vectorizer.tweet_vocab.mask_index,\\n    pretrained_embeddings=embeddings,\\n)\\nprint(classifier)\\nclassifer = classifier.to(args.device)\\nloss_func = nn.BCEWithLogitsLoss()\\noptimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(\\n    optimizer=optimizer, mode=\\\"min\\\", factor=0.5, patience=1\\n)\\n\\ntrain_state = utils.train_model(\\n    classifier=classifier,\\n    loss_func=loss_func,\\n    optimizer=optimizer,\\n    scheduler=scheduler,\\n    dataset=dataset,\\n    args=args,\\n)\\ntrain_state = utils.evaluate_test_split(\\n    classifier=classifier,\\n    dataset=dataset,\\n    loss_func=loss_func,\\n    args=args,\\n    train_state=train_state,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path hyper parameters\n",
    "    tweets_csv=\"data/train_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"models/simple_rnn/\",\n",
    "    # Model hyper parameters\n",
    "    glove_filepath=\"../../data/glove.6B.100d.txt\",\n",
    "    use_glove=True,\n",
    "    embedding_size=100,\n",
    "    rnn_hidden_size=128,\n",
    "    # Training hyper parameter\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    dropout_p=0.1,\n",
    "    batch_size=128,\n",
    "    num_epochs=100,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime option\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "utils.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "utils.handle_dirs(args.save_dir)\n",
    "\n",
    "dataset = SequenceTweetDataset.load_dataset_and_make_vectorizer(args.tweets_csv)\n",
    "dataset.save_vectorizer(args.vectorizer_file)\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "if args.use_glove:\n",
    "    words = vectorizer.tweet_vocab._token_to_idx.keys()\n",
    "    embeddings = utils.make_embedding_matrix(\n",
    "        glove_filepath=args.glove_filepath, words=words\n",
    "    )\n",
    "    print(\"Using pre-trained embeddings\")\n",
    "else:\n",
    "    print(\"Not using pre-trained embeddings\")\n",
    "    embeddings = None\n",
    "\n",
    "classifier = TweetSimpleRNNClassifier(\n",
    "    embedding_size=args.embedding_size,\n",
    "    num_embeddings=len(vectorizer.tweet_vocab),\n",
    "    output_dim=1,\n",
    "    rnn_hidden_size=args.rnn_hidden_size,\n",
    "    padding_idx=vectorizer.tweet_vocab.mask_index,\n",
    "    pretrained_embeddings=embeddings,\n",
    ")\n",
    "print(classifier)\n",
    "classifer = classifier.to(args.device)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer=optimizer, mode=\"min\", factor=0.5, patience=1\n",
    ")\n",
    "\n",
    "train_state = utils.train_model(\n",
    "    classifier=classifier,\n",
    "    loss_func=loss_func,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    dataset=dataset,\n",
    "    args=args,\n",
    ")\n",
    "train_state = utils.evaluate_test_split(\n",
    "    classifier=classifier,\n",
    "    dataset=dataset,\n",
    "    loss_func=loss_func,\n",
    "    args=args,\n",
    "    train_state=train_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182514c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
